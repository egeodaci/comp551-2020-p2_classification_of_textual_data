```
90, Avg. loss: 0.026612
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 35.99, NNZs: 41832, Bias: -1.018514, T: 87950, Avg. loss: 0.003685
Total training time: 0.25 seconds.
-- Epoch 11
Norm: 36.25, NNZs: 32031, Bias: -0.987405, T: 96745, Avg. loss: 0.002159
Total training time: 0.26 seconds.
Convergence after 11 epochs took 0.26 seconds
Convergence after 10 epochs took 0.23 seconds
Norm: 31.25, NNZs: 40668, Bias: -0.936199, T: 17594, Avg. loss: 0.028996
Total training time: 0.02 seconds.
Norm: 33.16, NNZs: 56282, Bias: -1.089818, T: 70312, Avg. loss: 0.003460
-- Epoch 10
Norm: 37.47, NNZs: 50975, Bias: -1.077834, T: 61523, Avg. loss: 0.006192
-- Epoch 3
Total training time: 0.23 seconds.
Total training time: 0.25 seconds.
-- Epoch 8
-- Epoch 9
Norm: 34.72, NNZs: 47516, Bias: -0.915033, T: 17594, Avg. loss: 0.047403
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 11
Norm: 40.18, NNZs: 47710, Bias: -1.004387, T: 79155, Avg. loss: 0.005892
Total training time: 0.27 seconds.
-- Epoch 10
Norm: 35.86, NNZs: 38215, Bias: -1.050359, T: 87970, Avg. loss: 0.003308
Total training time: 0.26 seconds.
Norm: 36.45, NNZs: 32475, Bias: -0.982497, T: 96679, Avg. loss: 0.002729
Convergence after 10 epochs took 0.26 seconds
Norm: 39.53, NNZs: 44492, Bias: -1.013767, T: 96767, Avg. loss: 0.004434
-- Epoch 9
-- Epoch 1
Total training time: 0.23 seconds.
Norm: 35.83, NNZs: 36322, Bias: -0.869586, T: 79173, Avg. loss: 0.005852
Total training time: 0.27 seconds.
-- Epoch 10
Convergence after 11 epochs took 0.23 seconds
-- Epoch 1
-- Epoch 10
Norm: 37.13, NNZs: 37202, Bias: -1.031617, T: 79173, Avg. loss: 0.005343
Total training time: 0.26 seconds.
Norm: 32.92, NNZs: 48223, Bias: -1.070568, T: 79155, Avg. loss: 0.002508
Total training time: 0.25 seconds.
Norm: 38.04, NNZs: 47632, Bias: -1.109369, T: 96745, Avg. loss: 0.006412
Norm: 34.76, NNZs: 45616, Bias: -0.986604, T: 87950, Avg. loss: 0.001570
Norm: 31.22, NNZs: 34710, Bias: -0.946696, T: 87950, Avg. loss: 0.004685
Total training time: 0.26 seconds.
-- Epoch 10
Total training time: 0.26 seconds.
Norm: 40.47, NNZs: 41708, Bias: -0.986706, T: 87890, Avg. loss: 0.004614
Total training time: 0.28 seconds.
Norm: 37.91, NNZs: 44829, Bias: -1.021921, T: 79101, Avg. loss: 0.006689
Total training time: 0.25 seconds.
Norm: 37.45, NNZs: 51596, Bias: -0.963420, T: 26367, Avg. loss: 0.019092
Norm: 26.32, NNZs: 39374, Bias: -0.849698, T: 8789, Avg. loss: 0.108549
-- Epoch 11
-- Epoch 10
Total training time: 0.02 seconds.
Norm: 41.67, NNZs: 49604, Bias: -1.099511, T: 79101, Avg. loss: 0.003357
Total training time: 0.26 seconds.
-- Epoch 2
Norm: 31.93, NNZs: 45842, Bias: -0.939240, T: 26367, Avg. loss: 0.010318
Total training time: 0.05 seconds.
-- Epoch 10
Norm: 37.59, NNZs: 50975, Bias: -1.082172, T: 70312, Avg. loss: 0.005912
Total training time: 0.24 seconds.
-- Epoch 4
Norm: 31.52, NNZs: 40474, Bias: -0.940442, T: 79101, Avg. loss: 0.005364
Total training time: 0.21 seconds.
-- Epoch 9
-- Epoch 10
Norm: 32.95, NNZs: 43041, Bias: -1.091812, T: 70376, Avg. loss: 0.002805
-- Epoch 10
Norm: 37.75, NNZs: 47718, Bias: -0.938113, T: 26391, Avg. loss: 0.019704
Total training time: 0.24 seconds.
-- Epoch 9
Norm: 38.01, NNZs: 44830, Bias: -1.016437, T: 87890, Avg. loss: 0.006490
Total training time: 0.07 seconds.
Total training time: 0.26 seconds.
Total training time: 0.28 seconds.
Convergence after 10 epochs took 0.27 seconds
Total training time: 0.26 seconds.
Norm: 32.20, NNZs: 40731, Bias: -0.899378, T: 17590, Avg. loss: 0.031940
Total training time: 0.04 seconds.
-- Epoch 3
-- Epoch 4
Norm: 40.52, NNZs: 41710, Bias: -1.018460, T: 96679, Avg. loss: 0.004405
Total training time: 0.06 seconds.
Norm: 32.74, NNZs: 40874, Bias: -0.930737, T: 26391, Avg. loss: 0.011126
Total training time: 0.04 seconds.
-- Epoch 4
Convergence after 11 epochs took 0.27 seconds
-- Epoch 1
Norm: 38.26, NNZs: 48872, Bias: -1.071034, T: 87970, Avg. loss: 0.005339
Total training time: 0.28 seconds.
-- Epoch 1
-- Epoch 4
-- Epoch 11
Norm: 32.99, NNZs: 43043, Bias: -1.095976, T: 79173, Avg. loss: 0.002525
Total training time: 0.25 seconds.
Convergence after 11 epochs took 0.27 seconds
Total training time: 0.29 seconds.
-- Epoch 12
-- Epoch 1
Norm: 41.75, NNZs: 49604, Bias: -1.103878, T: 87890, Avg. loss: 0.003219
Total training time: 0.27 seconds.
Norm: 31.62, NNZs: 40478, Bias: -0.934981, T: 87890, Avg. loss: 0.005070
Total training time: 0.22 seconds.
-- Epoch 11
-- Epoch 1
Convergence after 10 epochs took 0.22 seconds
Norm: 25.13, NNZs: 34817, Bias: -0.994075, T: 8789, Avg. loss: 0.111578
Norm: 33.25, NNZs: 56289, Bias: -1.090871, T: 79101, Avg. loss: 0.003119
-- Epoch 1
Norm: 38.44, NNZs: 51608, Bias: -0.991596, T: 35156, Avg. loss: 0.009568
-- Epoch 11
Norm: 41.68, NNZs: 38680, Bias: -1.084796, T: 96767, Avg. loss: 0.002884
Norm: 26.59, NNZs: 42293, Bias: -0.866413, T: 8797, Avg. loss: 0.111414
Total training time: 0.02 seconds.
Norm: 31.62, NNZs: 38958, Bias: -0.946914, T: 79173, Avg. loss: 0.004372
Total training time: 0.27 seconds.
-- Epoch 2
Norm: 32.95, NNZs: 48223, Bias: -1.073066, T: 87950, Avg. loss: 0.002449
Total training time: 0.27 seconds.
Norm: 37.54, NNZs: 35440, Bias: -0.983191, T: 96745, Avg. loss: 0.007137
Norm: 34.21, NNZs: 40920, Bias: -0.934281, T: 26385, Avg. loss: 0.014113
Total training time: 0.05 seconds.
Norm: 40.33, NNZs: 47712, Bias: -0.989733, T: 87950, Avg. loss: 0.005185
Total training time: 0.29 seconds.
Total training time: 0.02 seconds.
Convergence after 11 epochs took 0.28 seconds
Convergence after 10 epochs took 0.29 seconds
Norm: 31.98, NNZs: 40537, Bias: -0.858318, T: 17578, Avg. loss: 0.033798
Total training time: 0.04 seconds.
Total training time: 0.28 seconds.
-- Epoch 4
Norm: 32.56, NNZs: 39345, Bias: -0.948659, T: 26385, Avg. loss: 0.009974
Total training time: 0.11 seconds.
Norm: 36.07, NNZs: 41832, Bias: -1.027991, T: 96745, Avg. loss: 0.003300
Total training time: 0.28 seconds.
-- Epoch 3
Total training time: 0.28 seconds.
Total training time: 0.28 seconds.
-- Epoch 10
Norm: 38.49, NNZs: 46705, Bias: -0.986085, T: 35180, Avg. loss: 0.009667
-- Epoch 1
-- Epoch 1
Total training time: 0.09 seconds.
-- Epoch 11
-- Epoch 4
-- Epoch 5
Convergence after 11 epochs took 0.28 seconds
Norm: 25.48, NNZs: 31509, Bias: -0.973458, T: 8795, Avg. loss: 0.107662
Convergence after 11 epochs took 0.28 seconds
-- Epoch 1
-- Epoch 11
-- Epoch 10
Norm: 35.84, NNZs: 36322, Bias: -0.869331, T: 87970, Avg. loss: 0.005800
Total training time: 0.30 seconds.
Convergence after 10 epochs took 0.30 seconds
Norm: 25.80, NNZs: 40276, Bias: -0.951743, T: 8797, Avg. loss: 0.109391
Norm: 37.26, NNZs: 37209, Bias: -1.035448, T: 87970, Avg. loss: 0.005267
Norm: 32.42, NNZs: 43324, Bias: -0.862420, T: 17594, Avg. loss: 0.035155
Total training time: 0.29 seconds.
Total training time: 0.03 seconds.
-- Epoch 11
-- Epoch 3
Norm: 25.21, NNZs: 37099, Bias: -0.977940, T: 8797, Avg. loss: 0.111952
Total training time: 0.02 seconds.
Norm: 38.88, NNZs: 47769, Bias: -0.981507, T: 35188, Avg. loss: 0.009911
Total training time: 0.08 seconds.
-- Epoch 5
-- Epoch 2
Norm: 38.32, NNZs: 48872, Bias: -1.079043, T: 96767, Avg. loss: 0.005370
Total training time: 0.30 seconds.
Convergence after 11 epochs took 0.30 seconds
Norm: 32.38, NNZs: 45877, Bias: -0.946983, T: 35156, Avg. loss: 0.006551
Norm: 25.71, NNZs: 31186, Bias: -0.942113, T: 8795, Avg. loss: 0.113011
Total training time: 0.10 seconds.
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 35.34, NNZs: 36411, Bias: -0.940571, T: 87890, Avg. loss: 0.002476
Total training time: 0.27 seconds.
-- Epoch 2
Norm: 20.59, NNZs: 28882, Bias: -0.953346, T: 8795, Avg. loss: 0.101626
Total training time: 0.01 seconds.
Norm: 32.99, NNZs: 39426, Bias: -0.956856, T: 35180, Avg. loss: 0.005811
Total training time: 0.12 seconds.
-- Epoch 2
Total training time: 0.02 seconds.
-- Epoch 5
Norm: 38.95, NNZs: 46717, Bias: -0.988769, T: 43975, Avg. loss: 0.005953
Total training time: 0.10 seconds.
-- Epoch 2
Total training time: 0.02 seconds.
-- Epoch 6
Norm: 33.13, NNZs: 40946, Bias: -0.945795, T: 35188, Avg. loss: 0.006530
Norm: 32.98, NNZs: 48223, Bias: -1.073132, T: 96745, Avg. loss: 0.002410
Total training time: 0.29 seconds.
Total training time: 0.07 seconds.
Convergence after 11 epochs took 0.29 seconds
-- Epoch 2
Norm: 31.78, NNZs: 38968, Bias: -0.942954, T: 87970, Avg. loss: 0.004157
Total training time: 0.30 seconds.
-- Epoch 12
Convergence after 10 epochs took 0.30 seconds
-- Epoch 11
Norm: 23.94, NNZs: 39417, Bias: -0.949981, T: 8795, Avg. loss: 0.091778
Total training time: 0.01 seconds.
-- Epoch 1
Norm: 41.83, NNZs: 49617, Bias: -1.101008, T: 96679, Avg. loss: 0.003057
Total training time: 0.29 seconds.
-- Epoch 2
Convergence after 11 epochs took 0.29 seconds
-- Epoch 10
Norm: 24.49, NNZs: 31434, Bias: -0.944646, T: 8795, Avg. loss: 0.109041
Total training time: 0.02 seconds.
Norm: 29.88, NNZs: 32002, Bias: -0.971161, T: 17590, Avg. loss: 0.046461
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 33.20, NNZs: 39436, Bias: -0.960519, T: 43975, Avg. loss: 0.004697
-- Epoch 3
Total training time: 0.12 seconds.
-- Epoch 6
Norm: 31.85, NNZs: 32650, Bias: -0.913976, T: 17590, Avg. loss: 0.033920
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 37.39, NNZs: 37215, Bias: -1.038997, T: 96767, Avg. loss: 0.005223
Norm: 34.23, NNZs: 43543, Bias: -0.946555, T: 26391, Avg. loss: 0.015239
-- Epoch 1
Norm: 28.97, NNZs: 40739, Bias: -0.866548, T: 17590, Avg. loss: 0.026583
Total training time: 0.02 seconds.
Norm: 32.42, NNZs: 32797, Bias: -0.961853, T: 17590, Avg. loss: 0.036901
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 1
-- Epoch 3
Norm: 33.40, NNZs: 32372, Bias: -1.010507, T: 26385, Avg. loss: 0.019237
Total training time: 0.02 seconds.
-- Epoch 4
Norm: 31.57, NNZs: 32857, Bias: -1.012105, T: 17590, Avg. loss: 0.036700
Total training time: 0.02 seconds.
Norm: 34.00, NNZs: 32805, Bias: -0.955314, T: 26385, Avg. loss: 0.013641
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 33.35, NNZs: 39450, Bias: -0.967231, T: 52770, Avg. loss: 0.004295
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 39.15, NNZs: 46724, Bias: -0.995463, T: 52770, Avg. loss: 0.004688
Total training time: 0.11 seconds.
-- Epoch 7
-- Epoch 7
Total training time: 0.09 seconds.
-- Epoch 5
-- Epoch 5
-- Epoch 1
Norm: 34.86, NNZs: 40995, Bias: -0.952966, T: 35180, Avg. loss: 0.007077
Total training time: 0.08 seconds.
-- Epoch 5
Total training time: 0.31 seconds.
Norm: 35.11, NNZs: 33119, Bias: -1.017581, T: 26385, Avg. loss: 0.015198
Total training time: 0.03 seconds.
Convergence after 11 epochs took 0.31 seconds
-- Epoch 4
Norm: 34.61, NNZs: 32437, Bias: -1.036243, T: 35180, Avg. loss: 0.008760
Total training time: 0.02 seconds.
-- Epoch 5
Norm: 33.91, NNZs: 33081, Bias: -1.033047, T: 26385, Avg. loss: 0.012632
Total training time: 0.03 seconds.
Norm: 40.50, NNZs: 47712, Bias: -0.977746, T: 96745, Avg. loss: 0.005299
Total training time: 0.32 seconds.
-- Epoch 4
-- Epoch 12
Norm: 24.29, NNZs: 37799, Bias: -0.903863, T: 8789, Avg. loss: 0.089120
Norm: 37.66, NNZs: 50975, Bias: -1.083089, T: 79101, Avg. loss: 0.005662
Total training time: 0.29 seconds.
Norm: 40.66, NNZs: 41715, Bias: -1.016789, T: 105468, Avg. loss: 0.004602
Total training time: 0.32 seconds.
Norm: 33.70, NNZs: 41553, Bias: -0.910179, T: 26367, Avg. loss: 0.014027
Total training time: 0.07 seconds.
Norm: 39.26, NNZs: 46724, Bias: -0.997095, T: 61565, Avg. loss: 0.004255
Total training time: 0.12 seconds.
-- Epoch 8
Norm: 30.61, NNZs: 40965, Bias: -0.905021, T: 26385, Avg. loss: 0.012004
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 35.16, NNZs: 41016, Bias: -0.954463, T: 43975, Avg. loss: 0.005227
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 40.59, NNZs: 47721, Bias: -1.001535, T: 105540, Avg. loss: 0.005482
Total training time: 0.33 seconds.
Norm: 34.60, NNZs: 33099, Bias: -1.046134, T: 35180, Avg. loss: 0.005527
Total training time: 0.03 seconds.
Convergence after 12 epochs took 0.33 seconds
-- Epoch 5
Norm: 33.42, NNZs: 39452, Bias: -0.967167, T: 61565, Avg. loss: 0.003874
Total training time: 0.14 seconds.
Norm: 34.71, NNZs: 32845, Bias: -0.964087, T: 35180, Avg. loss: 0.006541
Total training time: 0.04 seconds.
Norm: 35.14, NNZs: 32444, Bias: -1.035764, T: 43975, Avg. loss: 0.005615
Total training time: 0.03 seconds.
-- Epoch 8
-- Epoch 5
-- Epoch 6
Norm: 39.38, NNZs: 46724, Bias: -0.990796, T: 70360, Avg. loss: 0.004165
Total training time: 0.12 seconds.
Norm: 39.40, NNZs: 47782, Bias: -0.983029, T: 43985, Avg. loss: 0.006353
Total training time: 0.11 seconds.
-- Epoch 9
Norm: 31.11, NNZs: 41008, Bias: -0.919413, T: 35180, Avg. loss: 0.006934
Total training time: 0.04 seconds.
-- Epoch 6
-- Epoch 5
Norm: 36.15, NNZs: 33168, Bias: -1.021819, T: 35180, Avg. loss: 0.006575
Total training time: 0.04 seconds.
-- Epoch 5
Norm: 33.03, NNZs: 43043, Bias: -1.094419, T: 87970, Avg. loss: 0.002446
Total training time: 0.30 seconds.
-- Epoch 1
-- Epoch 11
Norm: 34.93, NNZs: 33105, Bias: -1.053069, T: 43975, Avg. loss: 0.003875
Total training time: 0.04 seconds.
Norm: 35.40, NNZs: 32445, Bias: -1.038998, T: 52770, Avg. loss: 0.004457
Total training time: 0.04 seconds.
-- Epoch 6
-- Epoch 7
Norm: 34.94, NNZs: 32854, Bias: -0.964029, T: 43975, Avg. loss: 0.004350
Total training time: 0.05 seconds.
-- Epoch 6
Norm: 39.44, NNZs: 46724, Bias: -0.992805, T: 79155, Avg. loss: 0.003999
Total training time: 0.13 seconds.
-- Epoch 10
Total training time: 0.06 seconds.
Norm: 31.29, NNZs: 41010, Bias: -0.924025, T: 43975, Avg. loss: 0.005562
Total training time: 0.04 seconds.
-- Epoch 4
-- Epoch 6
Norm: 36.54, NNZs: 33183, Bias: -1.030633, T: 43975, Avg. loss: 0.003798
Total training time: 0.05 seconds.
-- Epoch 6
Norm: 35.07, NNZs: 33114, Bias: -1.055867, T: 52770, Avg. loss: 0.003117
Total training time: 0.05 seconds.
Norm: 35.52, NNZs: 32445, Bias: -1.037319, T: 61565, Avg. loss: 0.003857
Total training time: 0.04 seconds.
-- Epoch 7
Norm: 35.02, NNZs: 32854, Bias: -0.962637, T: 52770, Avg. loss: 0.003712
Total training time: 0.05 seconds.
-- Epoch 8
-- Epoch 7
Norm: 24.52, NNZs: 35126, Bias: -0.983047, T: 8789, Avg. loss: 0.111335
Total training time: 0.02 seconds.
Norm: 35.31, NNZs: 41026, Bias: -0.957027, T: 52770, Avg. loss: 0.004444
Total training time: 0.10 seconds.
-- Epoch 2
-- Epoch 7
Norm: 38.05, NNZs: 44830, Bias: -1.022366, T: 96679, Avg. loss: 0.006366
Total training time: 0.32 seconds.
Norm: 39.49, NNZs: 46724, Bias: -0.988260, T: 87950, Avg. loss: 0.003816
Total training time: 0.14 seconds.
Convergence after 11 epochs took 0.32 seconds
-- Epoch 10
Total training time: 0.03 seconds.
-- Epoch 5
-- Epoch 11
Convergence after 12 epochs took 0.34 seconds
Norm: 31.37, NNZs: 41016, Bias: -0.922037, T: 52770, Avg. loss: 0.005080
Total training time: 0.05 seconds.
-- Epoch 7
Norm: 32.00, NNZs: 51690, Bias: -0.930680, T: 17594, Avg. loss: 0.033888
Norm: 20.48, NNZs: 32806, Bias: -0.925532, T: 8797, Avg. loss: 0.101788
Total training time: 0.03 seconds.
Norm: 41.73, NNZs: 38680, Bias: -1.083467, T: 105564, Avg. loss: 0.002838
Total training time: 0.33 seconds.
Convergence after 12 epochs took 0.33 seconds
Norm: 39.66, NNZs: 47787, Bias: -0.974209, T: 52782, Avg. loss: 0.004849
Norm: 32.59, NNZs: 38707, Bias: -0.974796, T: 17594, Avg. loss: 0.041050
Norm: 36.74, NNZs: 33184, Bias: -1.035210, T: 52770, Avg. loss: 0.002940
Total training time: 0.06 seconds.
Total training time: 0.06 seconds.
Total training time: 0.13 seconds.
-- Epoch 7
-- Epoch 3
Norm: 33.30, NNZs: 40946, Bias: -0.944899, T: 43985, Avg. loss: 0.005280
Norm: 35.14, NNZs: 33116, Bias: -1.056107, T: 61565, Avg. loss: 0.002780
Total training time: 0.05 seconds.
-- Epoch 8
Norm: 35.05, NNZs: 32861, Bias: -0.963354, T: 61565, Avg. loss: 0.003495
Total training time: 0.06 seconds.
-- Epoch 8
Norm: 35.40, NNZs: 41026, Bias: -0.957814, T: 61565, Avg. loss: 0.004109
Total training time: 0.11 seconds.
-- Epoch 8
-- Epoch 2
Norm: 31.39, NNZs: 41016, Bias: -0.924491, T: 61565, Avg. loss: 0.004929
Total training time: 0.05 seconds.
Norm: 32.30, NNZs: 36264, Bias: -0.994595, T: 17578, Avg. loss: 0.039025
Total training time: 0.08 seconds.
-- Epoch 8
Norm: 33.54, NNZs: 39456, Bias: -0.975206, T: 70360, Avg. loss: 0.003989
Total training time: 0.16 seconds.
-- Epoch 9
Norm: 36.87, NNZs: 33190, Bias: -1.041555, T: 61565, Avg. loss: 0.002660
Total training time: 0.06 seconds.
Total training time: 0.11 seconds.
-- Epoch 8
-- Epoch 6
Norm: 35.17, NNZs: 33116, Bias: -1.057166, T: 70360, Avg. loss: 0.002603
Total training time: 0.06 seconds.
-- Epoch 9
Norm: 24.18, NNZs: 34793, Bias: -0.914504, T: 8797, Avg. loss: 0.088767
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 25.31, NNZs: 32514, Bias: -0.977104, T: 8789, Avg. loss: 0.106688
Total training time: 0.06 seconds.
Norm: 35.05, NNZs: 32861, Bias: -0.963744, T: 70360, Avg. loss: 0.003415
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 1
-- Epoch 2
Norm: 39.53, NNZs: 46738, Bias: -0.997400, T: 96745, Avg. loss: 0.003990
Total training time: 0.15 seconds.
Convergence after 11 epochs took 0.15 seconds
Norm: 38.85, NNZs: 51617, Bias: -0.994861, T: 43945, Avg. loss: 0.006466
Total training time: 0.15 seconds.
Norm: 31.41, NNZs: 41016, Bias: -0.925225, T: 70360, Avg. loss: 0.004874
Total training time: 0.06 seconds.
-- Epoch 9
Norm: 33.59, NNZs: 39460, Bias: -0.971784, T: 79155, Avg. loss: 0.003516
Total training time: 0.17 seconds.
-- Epoch 10
-- Epoch 7
Norm: 24.43, NNZs: 31795, Bias: -0.993927, T: 8797, Avg. loss: 0.109261
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 36.93, NNZs: 33195, Bias: -1.046145, T: 70360, Avg. loss: 0.002348
Total training time: 0.07 seconds.
-- Epoch 9
Norm: 35.18, NNZs: 33117, Bias: -1.058273, T: 79155, Avg. loss: 0.002587
Total training time: 0.06 seconds.
Norm: 33.37, NNZs: 40946, Bias: -0.946864, T: 52782, Avg. loss: 0.004917
Total training time: 0.12 seconds.
-- Epoch 10
-- Epoch 7
-- Epoch 6
-- Epoch 3
Norm: 37.76, NNZs: 50986, Bias: -1.086317, T: 87890, Avg. loss: 0.005718
Total training time: 0.33 seconds.
-- Epoch 11
Norm: 32.59, NNZs: 45895, Bias: -0.956467, T: 43945, Avg. loss: 0.005453
Total training time: 0.13 seconds.
-- Epoch 6
Norm: 31.50, NNZs: 54139, Bias: -0.947747, T: 17578, Avg. loss: 0.033458
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 33.04, NNZs: 43043, Bias: -1.095677, T: 96767, Avg. loss: 0.002376
Total training time: 0.32 seconds.
Convergence after 11 epochs took 0.33 seconds
Norm: 31.92, NNZs: 37033, Bias: -0.988563, T: 17578, Avg. loss: 0.038181
Total training time: 0.04 seconds.
Norm: 31.42, NNZs: 41016, Bias: -0.923998, T: 79155, Avg. loss: 0.004816
Total training time: 0.07 seconds.
Norm: 28.79, NNZs: 38874, Bias: -0.932770, T: 17578, Avg. loss: 0.025462
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 10
-- Epoch 3
Norm: 33.31, NNZs: 56289, Bias: -1.091536, T: 87890, Avg. loss: 0.003040
Total training time: 0.35 seconds.
Total training time: 0.08 seconds.
Norm: 35.35, NNZs: 36411, Bias: -0.940453, T: 96679, Avg. loss: 0.002411
Total training time: 0.33 seconds.
Norm: 36.98, NNZs: 33196, Bias: -1.047654, T: 79155, Avg. loss: 0.002281
Total training time: 0.08 seconds.
-- Epoch 11
Convergence after 11 epochs took 0.33 seconds
-- Epoch 3
-- Epoch 10
Norm: 28.73, NNZs: 35939, Bias: -0.907607, T: 17594, Avg. loss: 0.025033
Total training time: 0.05 seconds.
-- Epoch 3
Norm: 35.22, NNZs: 36537, Bias: -1.027837, T: 26367, Avg. loss: 0.016032
Total training time: 0.10 seconds.
-- Epoch 4
Norm: 20.79, NNZs: 37042, Bias: -0.904499, T: 8789, Avg. loss: 0.101915
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 34.34, NNZs: 37233, Bias: -0.999244, T: 26367, Avg. loss: 0.013223
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 34.44, NNZs: 41576, Bias: -0.930725, T: 35156, Avg. loss: 0.008684
Total training time: 0.12 seconds.
Norm: 39.08, NNZs: 51618, Bias: -0.982123, T: 52734, Avg. loss: 0.005292
Total training time: 0.16 seconds.
Norm: 31.42, NNZs: 41016, Bias: -0.924711, T: 87950, Avg. loss: 0.004813
Total training time: 0.08 seconds.
-- Epoch 5
-- Epoch 7
Convergence after 10 epochs took 0.08 seconds
Norm: 39.78, NNZs: 47787, Bias: -0.989955, T: 61579, Avg. loss: 0.004594
Total training time: 0.15 seconds.
Norm: 33.34, NNZs: 56289, Bias: -1.091377, T: 96679, Avg. loss: 0.002882
Total training time: 0.36 seconds.
Convergence after 11 epochs took 0.36 seconds
Norm: 37.02, NNZs: 33196, Bias: -1.049368, T: 87950, Avg. loss: 0.002221
Total training time: 0.08 seconds.
Convergence after 10 epochs took 0.09 seconds
Norm: 34.24, NNZs: 52220, Bias: -0.967067, T: 26391, Avg. loss: 0.013088
Total training time: 0.09 seconds.
-- Epoch 2
-- Epoch 4
Norm: 35.10, NNZs: 43621, Bias: -0.944790, T: 35188, Avg. loss: 0.009272
Total training time: 0.10 seconds.
Norm: 30.31, NNZs: 39540, Bias: -0.964998, T: 17578, Avg. loss: 0.048092
Total training time: 0.03 seconds.
Norm: 32.77, NNZs: 45913, Bias: -0.966147, T: 52734, Avg. loss: 0.005083
Total training time: 0.15 seconds.
Norm: 30.31, NNZs: 36175, Bias: -0.922974, T: 26391, Avg. loss: 0.011423
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 37.80, NNZs: 50988, Bias: -1.091291, T: 96679, Avg. loss: 0.005592
Total training time: 0.34 seconds.
Norm: 31.70, NNZs: 33639, Bias: -0.998619, T: 17594, Avg. loss: 0.036965
Total training time: 0.06 seconds.
-- Epoch 7
-- Epoch 4
-- Epoch 5
Convergence after 11 epochs took 0.34 seconds
Norm: 35.57, NNZs: 32445, Bias: -1.041072, T: 70360, Avg. loss: 0.003652
Total training time: 0.08 seconds.
Norm: 34.81, NNZs: 41587, Bias: -0.925368, T: 43945, Avg. loss: 0.006589
Total training time: 0.12 seconds.
-- Epoch 9
-- Epoch 6
-- Epoch 3
Norm: 33.74, NNZs: 54687, Bias: -0.961751, T: 26367, Avg. loss: 0.013195
Total training time: 0.09 seconds.
Norm: 35.20, NNZs: 33117, Bias: -1.058216, T: 87950, Avg. loss: 0.002546
Total training time: 0.09 seconds.
Norm: 33.40, NNZs: 40946, Bias: -0.947792, T: 61579, Avg. loss: 0.004730
Total training time: 0.14 seconds.
Norm: 30.23, NNZs: 35986, Bias: -0.963425, T: 17594, Avg. loss: 0.049393
Total training time: 0.07 seconds.
Norm: 34.94, NNZs: 52274, Bias: -0.978260, T: 35188, Avg. loss: 0.005810
Total training time: 0.09 seconds.
Convergence after 10 epochs took 0.09 seconds
-- Epoch 8
-- Epoch 3
-- Epoch 5
-- Epoch 8
Norm: 35.18, NNZs: 37269, Bias: -1.011040, T: 35156, Avg. loss: 0.005818
Total training time: 0.06 seconds.
Norm: 39.17, NNZs: 51637, Bias: -0.999595, T: 61523, Avg. loss: 0.005160
Total training time: 0.18 seconds.
-- Epoch 5
-- Epoch 8
Norm: 33.69, NNZs: 39795, Bias: -0.950452, T: 26367, Avg. loss: 0.017786
Total training time: 0.03 seconds.
Norm: 30.80, NNZs: 36177, Bias: -0.929930, T: 35188, Avg. loss: 0.006740
Total training time: 0.06 seconds.
Norm: 32.88, NNZs: 45922, Bias: -0.978160, T: 61523, Avg. loss: 0.004736
Total training time: 0.15 seconds.
-- Epoch 4
-- Epoch 5
-- Epoch 8
-- Epoch 4
Norm: 36.33, NNZs: 36582, Bias: -1.048638, T: 35156, Avg. loss: 0.006953
Total training time: 0.11 seconds.
-- Epoch 5
Norm: 30.32, NNZs: 39051, Bias: -0.939805, T: 26367, Avg. loss: 0.011626
Total training time: 0.08 seconds.
Norm: 35.69, NNZs: 39045, Bias: -1.017152, T: 26391, Avg. loss: 0.016793
Total training time: 0.10 seconds.
-- Epoch 4
Norm: 35.43, NNZs: 37281, Bias: -1.015453, T: 43945, Avg. loss: 0.003274
Total training time: 0.06 seconds.
Norm: 39.28, NNZs: 51637, Bias: -0.990700, T: 70312, Avg. loss: 0.004807
Total training time: 0.18 seconds.
-- Epoch 6
-- Epoch 4
-- Epoch 9
Norm: 35.09, NNZs: 39839, Bias: -1.023877, T: 35156, Avg. loss: 0.009550
Total training time: 0.04 seconds.
Norm: 33.79, NNZs: 36216, Bias: -0.976092, T: 26391, Avg. loss: 0.019502
Total training time: 0.07 seconds.
Norm: 32.95, NNZs: 45923, Bias: -0.980659, T: 70312, Avg. loss: 0.004341
Total training time: 0.16 seconds.
-- Epoch 5
-- Epoch 4
-- Epoch 9
Norm: 34.44, NNZs: 54753, Bias: -0.987218, T: 35156, Avg. loss: 0.006445
Total training time: 0.10 seconds.
Norm: 35.44, NNZs: 41029, Bias: -0.959633, T: 70360, Avg. loss: 0.003905
-- Epoch 9
Total training time: 0.15 seconds.
-- Epoch 5
-- Epoch 9
Norm: 36.83, NNZs: 39088, Bias: -1.029925, T: 35188, Avg. loss: 0.006734
Total training time: 0.11 seconds.
-- Epoch 5
Norm: 30.99, NNZs: 36199, Bias: -0.933477, T: 43985, Avg. loss: 0.005484
Total training time: 0.08 seconds.
Norm: 35.53, NNZs: 37283, Bias: -1.017193, T: 52734, Avg. loss: 0.002648
Total training time: 0.07 seconds.
-- Epoch 6
Norm: 33.60, NNZs: 39460, Bias: -0.972994, T: 87950, Avg. loss: 0.003512
Total training time: 0.21 seconds.
-- Epoch 7
Convergence after 10 epochs took 0.21 seconds
Norm: 35.73, NNZs: 39841, Bias: -1.015739, T: 43945, Avg. loss: 0.005262
Total training time: 0.05 seconds.
Norm: 39.88, NNZs: 47788, Bias: -0.982791, T: 70376, Avg. loss: 0.004180
Total training time: 0.18 seconds.
Norm: 33.04, NNZs: 45946, Bias: -0.983613, T: 79101, Avg. loss: 0.004203
Total training time: 0.17 seconds.
-- Epoch 6
-- Epoch 9
-- Epoch 10
Norm: 34.65, NNZs: 54761, Bias: -0.991280, T: 43945, Avg. loss: 0.004046
Total training time: 0.11 seconds.
-- Epoch 6
Norm: 35.47, NNZs: 41031, Bias: -0.960163, T: 79155, Avg. loss: 0.003826
Total training time: 0.16 seconds.
Norm: 35.06, NNZs: 32861, Bias: -0.963520, T: 79155, Avg. loss: 0.003394
Total training time: 0.11 seconds.
-- Epoch 10
Norm: 39.32, NNZs: 51637, Bias: -0.995811, T: 79101, Avg. loss: 0.004693
Total training time: 0.19 seconds.
-- Epoch 10
-- Epoch 10
Norm: 35.62, NNZs: 32445, Bias: -1.042947, T: 79155, Avg. loss: 0.003601
Total training time: 0.11 seconds.
-- Epoch 10
Norm: 35.58, NNZs: 37286, Bias: -1.015467, T: 61523, Avg. loss: 0.002380
Total training time: 0.08 seconds.
-- Epoch 8
Norm: 31.06, NNZs: 36200, Bias: -0.936589, T: 52782, Avg. loss: 0.004974
Total training time: 0.09 seconds.
-- Epoch 7
Norm: 35.97, NNZs: 39866, Bias: -1.025643, T: 52734, Avg. loss: 0.003797
Total training time: 0.05 seconds.
Norm: 39.92, NNZs: 47788, Bias: -0.986815, T: 79173, Avg. loss: 0.004023
Total training time: 0.19 seconds.
Norm: 33.10, NNZs: 45946, Bias: -0.982463, T: 87890, Avg. loss: 0.003999
Total training time: 0.18 seconds.
Norm: 36.88, NNZs: 36606, Bias: -1.059638, T: 43945, Avg. loss: 0.004318
Total training time: 0.13 seconds.
-- Epoch 7
Convergence after 10 epochs took 0.18 seconds
-- Epoch 6
Norm: 39.38, NNZs: 51637, Bias: -0.992198, T: 87890, Avg. loss: 0.004667
Total training time: 0.20 seconds.
Norm: 34.01, NNZs: 33830, Bias: -1.028698, T: 26391, Avg. loss: 0.012794
-- Epoch 11
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 35.47, NNZs: 43681, Bias: -0.942020, T: 43985, Avg. loss: 0.006685
Total training time: 0.13 seconds.
-- Epoch 6
Norm: 35.49, NNZs: 41066, Bias: -0.960633, T: 87950, Avg. loss: 0.003733
Total training time: 0.17 seconds.
-- Epoch 10
Norm: 35.59, NNZs: 37286, Bias: -1.016230, T: 70312, Avg. loss: 0.002285
Total training time: 0.09 seconds.
Convergence after 10 epochs took 0.17 seconds
Norm: 35.65, NNZs: 32448, Bias: -1.043138, T: 87950, Avg. loss: 0.003504
Total training time: 0.11 seconds.
-- Epoch 9
-- Epoch 11
Norm: 36.10, NNZs: 39871, Bias: -1.029251, T: 61523, Avg. loss: 0.003276
Total training time: 0.06 seconds.
-- Epoch 8
Norm: 31.09, NNZs: 36200, Bias: -0.934551, T: 61579, Avg. loss: 0.004798
Total training time: 0.09 seconds.
Norm: 30.85, NNZs: 39075, Bias: -0.942062, T: 35156, Avg. loss: 0.007072
Norm: 37.10, NNZs: 36608, Bias: -1.064673, T: 52734, Avg. loss: 0.002829
Total training time: 0.14 seconds.
Norm: 35.06, NNZs: 32861, Bias: -0.963589, T: 87950, Avg. loss: 0.003383
Total training time: 0.12 seconds.
-- Epoch 8
-- Epoch 7
Convergence after 10 epochs took 0.12 seconds
Norm: 35.01, NNZs: 41605, Bias: -0.935357, T: 52734, Avg. loss: 0.005845
Total training time: 0.16 seconds.
-- Epoch 7
Norm: 35.28, NNZs: 36280, Bias: -1.012012, T: 35188, Avg. loss: 0.009800
Total training time: 0.10 seconds.
-- Epoch 5
Total training time: 0.10 seconds.
Norm: 35.75, NNZs: 43697, Bias: -0.947836, T: 52782, Avg. loss: 0.006345
Total training time: 0.14 seconds.
-- Epoch 5
-- Epoch 7
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.4s remaining:    0.0s
Norm: 35.14, NNZs: 52281, Bias: -0.981929, T: 43985, Avg. loss: 0.003510
Total training time: 0.13 seconds.
-- Epoch 6
Norm: 39.99, NNZs: 47788, Bias: -0.982100, T: 87970, Avg. loss: 0.004003
Total training time: 0.20 seconds.
-- Epoch 11
Norm: 35.60, NNZs: 37286, Bias: -1.016185, T: 79101, Avg. loss: 0.002251
Total training time: 0.09 seconds.
-- Epoch 10
Norm: 35.66, NNZs: 32448, Bias: -1.042895, T: 96745, Avg. loss: 0.003457
Total training time: 0.12 seconds.
Convergence after 11 epochs took 0.12 seconds
Norm: 36.20, NNZs: 39871, Bias: -1.024918, T: 70312, Avg. loss: 0.003017
Total training time: 0.07 seconds.
Norm: 31.11, NNZs: 36200, Bias: -0.936549, T: 70376, Avg. loss: 0.004772
Total training time: 0.10 seconds.
-- Epoch 9
Norm: 37.24, NNZs: 36610, Bias: -1.067734, T: 61523, Avg. loss: 0.002447
Total training time: 0.15 seconds.
-- Epoch 9
-- Epoch 8
Norm: 34.83, NNZs: 33846, Bias: -1.035707, T: 35188, Avg. loss: 0.005918
Total training time: 0.10 seconds.
Norm: 35.15, NNZs: 41621, Bias: -0.934488, T: 61523, Avg. loss: 0.005363
Total training time: 0.17 seconds.
-- Epoch 5
-- Epoch 8
Norm: 35.89, NNZs: 36282, Bias: -1.015505, T: 43985, Avg. loss: 0.005575
Total training time: 0.11 seconds.
-- Epoch 6
Norm: 33.41, NNZs: 40946, Bias: -0.947989, T: 70376, Avg. loss: 0.004661
Total training time: 0.18 seconds.
-- Epoch 9
Norm: 35.99, NNZs: 43736, Bias: -0.940268, T: 61579, Avg. loss: 0.005718
Norm: 37.29, NNZs: 39103, Bias: -1.028410, T: 43985, Avg. loss: 0.003492
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 35.22, NNZs: 52281, Bias: -0.982379, T: 52782, Avg. loss: 0.002991
Norm: 34.73, NNZs: 54761, Bias: -0.991197, T: 52734, Avg. loss: 0.003418
Total training time: 0.14 seconds.
Total training time: 0.13 seconds.
-- Epoch 7
-- Epoch 7
Norm: 35.60, NNZs: 37286, Bias: -1.016261, T: 87890, Avg. loss: 0.002237
Total training time: 0.10 seconds.
Convergence after 10 epochs took 0.10 seconds
Norm: 36.24, NNZs: 39871, Bias: -1.027088, T: 79101, Avg. loss: 0.002885
Total training time: 0.08 seconds.
Norm: 31.12, NNZs: 36200, Bias: -0.936573, T: 79173, Avg. loss: 0.004709
Total training time: 0.11 seconds.
-- Epoch 10
Norm: 37.32, NNZs: 36610, Bias: -1.070395, T: 70312, Avg. loss: 0.002202
Total training time: 0.15 seconds.
-- Epoch 10
-- Epoch 9
Norm: 35.25, NNZs: 41632, Bias: -0.938092, T: 70312, Avg. loss: 0.005152
Total training time: 0.17 seconds.
-- Epoch 9
Norm: 39.41, NNZs: 51637, Bias: -0.999219, T: 96679, Avg. loss: 0.004625
Total training time: 0.22 seconds.
Norm: 31.05, NNZs: 39115, Bias: -0.944728, T: 43945, Avg. loss: 0.005737
Total training time: 0.12 seconds.
Norm: 36.16, NNZs: 36283, Bias: -1.014980, T: 52782, Avg. loss: 0.003999
Total training time: 0.11 seconds.
Convergence after 11 epochs took 0.22 seconds
-- Epoch 7
Norm: 37.48, NNZs: 39103, Bias: -1.030813, T: 52782, Avg. loss: 0.002391
Total training time: 0.15 seconds.
Total training time: 0.14 seconds.
-- Epoch 6
-- Epoch 8
Norm: 34.76, NNZs: 54761, Bias: -0.991033, T: 61523, Avg. loss: 0.003192
Total training time: 0.14 seconds.
-- Epoch 8
Norm: 35.24, NNZs: 52281, Bias: -0.981918, T: 61579, Avg. loss: 0.002767
Total training time: 0.15 seconds.
-- Epoch 8
Norm: 40.02, NNZs: 47792, Bias: -0.990176, T: 96767, Avg. loss: 0.003982
Total training time: 0.21 seconds.
Norm: 36.26, NNZs: 39871, Bias: -1.026685, T: 87890, Avg. loss: 0.002773
Norm: 31.12, NNZs: 36200, Bias: -0.937063, T: 87970, Avg. loss: 0.004686
Total training time: 0.08 seconds.
Total training time: 0.11 seconds.
Convergence after 11 epochs took 0.21 seconds
-- Epoch 11
Convergence after 10 epochs took 0.11 seconds
Norm: 37.36, NNZs: 36610, Bias: -1.071720, T: 79101, Avg. loss: 0.002005
Total training time: 0.16 seconds.
-- Epoch 7
-- Epoch 10
Norm: 33.42, NNZs: 40946, Bias: -0.948259, T: 79173, Avg. loss: 0.004622
Total training time: 0.19 seconds.
Norm: 35.29, NNZs: 41632, Bias: -0.937612, T: 79101, Avg. loss: 0.004852
Total training time: 0.18 seconds.
-- Epoch 10
Norm: 35.10, NNZs: 33849, Bias: -1.039113, T: 43985, Avg. loss: 0.003602
Total training time: 0.12 seconds.
-- Epoch 10
-- Epoch 6
Norm: 36.29, NNZs: 36283, Bias: -1.019624, T: 61579, Avg. loss: 0.003504
Total training time: 0.12 seconds.
-- Epoch 8
Norm: 36.20, NNZs: 43760, Bias: -0.949005, T: 70376, Avg. loss: 0.005638
Total training time: 0.16 seconds.
-- Epoch 9
Norm: 35.25, NNZs: 52281, Bias: -0.982268, T: 70376, Avg. loss: 0.002703
Total training time: 0.15 seconds.
Norm: 34.76, NNZs: 54761, Bias: -0.991314, T: 70312, Avg. loss: 0.003129
Total training time: 0.15 seconds.
-- Epoch 9
-- Epoch 9
Norm: 31.11, NNZs: 39115, Bias: -0.946117, T: 52734, Avg. loss: 0.005148
Total training time: 0.13 seconds.
-- Epoch 7
Norm: 36.27, NNZs: 39871, Bias: -1.027253, T: 96679, Avg. loss: 0.002737
Total training time: 0.09 seconds.
Norm: 33.42, NNZs: 40946, Bias: -0.948230, T: 87970, Avg. loss: 0.004611
Total training time: 0.20 seconds.
Convergence after 10 epochs took 0.20 seconds
Convergence after 11 epochs took 0.09 seconds
Norm: 37.38, NNZs: 36610, Bias: -1.072883, T: 87890, Avg. loss: 0.001937
Total training time: 0.17 seconds.
Norm: 37.55, NNZs: 39103, Bias: -1.030738, T: 61579, Avg. loss: 0.001885
Total training time: 0.15 seconds.
-- Epoch 11
-- Epoch 8
Norm: 35.21, NNZs: 33851, Bias: -1.047486, T: 52782, Avg. loss: 0.003044
Total training time: 0.13 seconds.
-- Epoch 7
Norm: 36.40, NNZs: 36283, Bias: -1.016566, T: 70376, Avg. loss: 0.003328
Total training time: 0.13 seconds.
Norm: 35.34, NNZs: 41632, Bias: -0.940056, T: 87890, Avg. loss: 0.004850
Total training time: 0.19 seconds.
-- Epoch 9
Convergence after 10 epochs took 0.19 seconds
Norm: 36.29, NNZs: 43760, Bias: -0.947659, T: 79173, Avg. loss: 0.004959
Total training time: 0.17 seconds.
-- Epoch 10
Norm: 35.25, NNZs: 52281, Bias: -0.982198, T: 79173, Avg. loss: 0.002680
Total training time: 0.16 seconds.
Norm: 34.76, NNZs: 54761, Bias: -0.991417, T: 79101, Avg. loss: 0.003108
Total training time: 0.15 seconds.
-- Epoch 10
-- Epoch 10
Norm: 31.14, NNZs: 39116, Bias: -0.946272, T: 61523, Avg. loss: 0.004980
Total training time: 0.13 seconds.
-- Epoch 8
-- Epoch 1
Norm: 37.58, NNZs: 39103, Bias: -1.031020, T: 70376, Avg. loss: 0.001708
Total training time: 0.16 seconds.
Norm: 37.40, NNZs: 36610, Bias: -1.073021, T: 96679, Avg. loss: 0.001905
Total training time: 0.17 seconds.
-- Epoch 9
Norm: 35.26, NNZs: 33851, Bias: -1.045850, T: 61579, Avg. loss: 0.002655
Total training time: 0.13 seconds.
Convergence after 11 epochs took 0.17 seconds
-- Epoch 1
-- Epoch 8
Norm: 36.46, NNZs: 36285, Bias: -1.016029, T: 79173, Avg. loss: 0.003146
Total training time: 0.13 seconds.
-- Epoch 10
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.4s finished
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
Norm: 36.41, NNZs: 43773, Bias: -0.954015, T: 87970, Avg. loss: 0.005065
Total training time: 0.17 seconds.
Convergence after 10 epochs took 0.17 seconds
Norm: 35.25, NNZs: 52281, Bias: -0.982220, T: 87970, Avg. loss: 0.002673
Total training time: 0.16 seconds.
-- Epoch 1
Convergence after 10 epochs took 0.16 seconds
Norm: 34.77, NNZs: 54761, Bias: -0.991378, T: 87890, Avg. loss: 0.003101
Total training time: 0.16 seconds.
-- Epoch 1
Convergence after 10 epochs took 0.16 seconds
-- Epoch 1
-- Epoch 1
Norm: 31.15, NNZs: 39116, Bias: -0.946646, T: 70312, Avg. loss: 0.004930
Total training time: 0.14 seconds.
-- Epoch 1
-- Epoch 9
Norm: 35.29, NNZs: 33851, Bias: -1.048004, T: 70376, Avg. loss: 0.002614
Total training time: 0.13 seconds.
Norm: 37.60, NNZs: 39103, Bias: -1.031158, T: 79173, Avg. loss: 0.001637
Total training time: 0.16 seconds.
-- Epoch 9
-- Epoch 10
Norm: 36.50, NNZs: 36285, Bias: -1.017852, T: 87970, Avg. loss: 0.003073
Total training time: 0.14 seconds.
-- Epoch 11
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 31.15, NNZs: 39116, Bias: -0.946758, T: 79101, Avg. loss: 0.004904
Total training time: 0.15 seconds.
-- Epoch 10
Norm: 35.30, NNZs: 33851, Bias: -1.047830, T: 79173, Avg. loss: 0.002546
Total training time: 0.14 seconds.
Norm: 37.61, NNZs: 39103, Bias: -1.031307, T: 87970, Avg. loss: 0.001602
Total training time: 0.17 seconds.
-- Epoch 10
-- Epoch 11
Norm: 36.52, NNZs: 36285, Bias: -1.019028, T: 96767, Avg. loss: 0.003022
Total training time: 0.14 seconds.
Convergence after 11 epochs took 0.14 seconds
-- Epoch 1
Norm: 25.13, NNZs: 45057, Bias: -0.911087, T: 8797, Avg. loss: 0.102183
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 26.18, NNZs: 32284, Bias: -0.908371, T: 8797, Avg. loss: 0.122353
Total training time: 0.01 seconds.
Norm: 25.17, NNZs: 31211, Bias: -0.938500, T: 8797, Avg. loss: 0.122738
Total training time: 0.01 seconds.
-- Epoch 2
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.5s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.5s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished
Norm: 31.15, NNZs: 39116, Bias: -0.946749, T: 87890, Avg. loss: 0.004894
Total training time: 0.15 seconds.
Convergence after 10 epochs took 0.15 seconds
Norm: 37.61, NNZs: 39103, Bias: -1.031220, T: 96767, Avg. loss: 0.001592
Total training time: 0.18 seconds.
Convergence after 11 epochs took 0.18 seconds
Norm: 26.62, NNZs: 37437, Bias: -0.876732, T: 8797, Avg. loss: 0.102895
Norm: 25.70, NNZs: 39857, Bias: -0.895787, T: 8797, Avg. loss: 0.100678
-- Epoch 2
Norm: 25.09, NNZs: 44770, Bias: -0.999790, T: 8797, Avg. loss: 0.116468
Total training time: 0.02 seconds.
Total training time: 0.01 seconds.
-- Epoch 2
Total training time: 0.01 seconds.
Norm: 35.31, NNZs: 33851, Bias: -1.048023, T: 87970, Avg. loss: 0.002526
Total training time: 0.15 seconds.
Convergence after 10 epochs took 0.15 seconds
-- Epoch 1
Norm: 26.74, NNZs: 31052, Bias: -0.898963, T: 8797, Avg. loss: 0.114619
-- Epoch 1
-- Epoch 2
Total training time: 0.02 seconds.
Norm: 23.85, NNZs: 32037, Bias: -1.006154, T: 8797, Avg. loss: 0.091561
-- Epoch 2
-- Epoch 2
Total training time: 0.01 seconds.
Norm: 30.63, NNZs: 43425, Bias: -0.915271, T: 17594, Avg. loss: 0.027712
Total training time: 0.02 seconds.
Norm: 30.81, NNZs: 46594, Bias: -0.942401, T: 17594, Avg. loss: 0.033257
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 3
Norm: 24.23, NNZs: 28483, Bias: -0.914113, T: 8797, Avg. loss: 0.109763
Total training time: 0.01 seconds.
Norm: 33.55, NNZs: 34197, Bias: -0.938890, T: 17594, Avg. loss: 0.046516
Total training time: 0.02 seconds.
Norm: 24.55, NNZs: 33596, Bias: -0.839929, T: 8797, Avg. loss: 0.087545
Total training time: 0.02 seconds.
Norm: 26.18, NNZs: 34887, Bias: -0.899445, T: 8797, Avg. loss: 0.117095
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 2
-- Epoch 2
Norm: 31.07, NNZs: 40108, Bias: -0.857869, T: 17594, Avg. loss: 0.027303
Total training time: 0.02 seconds.
Norm: 32.97, NNZs: 32702, Bias: -0.851397, T: 17594, Avg. loss: 0.037762
Total training time: 0.02 seconds.
-- Epoch 3
-- Epoch 3
Norm: 29.39, NNZs: 42594, Bias: -0.998095, T: 17594, Avg. loss: 0.027728
Total training time: 0.02 seconds.
Norm: 32.13, NNZs: 45996, Bias: -0.985943, T: 17594, Avg. loss: 0.043276
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 3
Norm: 33.43, NNZs: 46953, Bias: -0.941578, T: 26391, Avg. loss: 0.014800
Total training time: 0.02 seconds.
Norm: 32.89, NNZs: 44267, Bias: -0.980356, T: 26391, Avg. loss: 0.012613
Total training time: 0.02 seconds.
Norm: 31.71, NNZs: 30285, Bias: -0.915487, T: 17594, Avg. loss: 0.040461
Total training time: 0.02 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 4
Norm: 36.78, NNZs: 34585, Bias: -0.973064, T: 26391, Avg. loss: 0.021383
Total training time: 0.03 seconds.
Norm: 28.49, NNZs: 34951, Bias: -0.837748, T: 17594, Avg. loss: 0.023553
Total training time: 0.03 seconds.
Norm: 32.36, NNZs: 36175, Bias: -0.946149, T: 17594, Avg. loss: 0.041022
Total training time: 0.02 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 3
Norm: 25.24, NNZs: 24330, Bias: -0.992169, T: 8797, Avg. loss: 0.107159
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 1
Norm: 32.49, NNZs: 40322, Bias: -0.887314, T: 26391, Avg. loss: 0.013286
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 35.25, NNZs: 32893, Bias: -0.850928, T: 26391, Avg. loss: 0.017312
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 31.45, NNZs: 49141, Bias: -1.016260, T: 26391, Avg. loss: 0.011309
Total training time: 0.02 seconds.
-- Epoch 4
Norm: 34.37, NNZs: 30525, Bias: -0.935286, T: 26391, Avg. loss: 0.016190
Total training time: 0.02 seconds.
Norm: 35.42, NNZs: 46465, Bias: -1.040999, T: 26391, Avg. loss: 0.021827
Norm: 33.54, NNZs: 38047, Bias: -1.014540, T: 17594, Avg. loss: 0.051945
Total training time: 0.03 seconds.
Total training time: 0.03 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 4
Norm: 33.91, NNZs: 44333, Bias: -0.969181, T: 35188, Avg. loss: 0.005513
Total training time: 0.03 seconds.
Norm: 34.29, NNZs: 47070, Bias: -0.971213, T: 35188, Avg. loss: 0.007036
Total training time: 0.03 seconds.
-- Epoch 5
-- Epoch 5
Norm: 38.22, NNZs: 34639, Bias: -0.984679, T: 35188, Avg. loss: 0.011300
Total training time: 0.03 seconds.
Norm: 29.62, NNZs: 35058, Bias: -0.882253, T: 26391, Avg. loss: 0.011528
Total training time: 0.03 seconds.
-- Epoch 5
-- Epoch 4
-- Epoch 1
Norm: 25.24, NNZs: 43328, Bias: -0.893751, T: 8806, Avg. loss: 0.102663
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 33.05, NNZs: 40341, Bias: -0.910850, T: 35188, Avg. loss: 0.009320
Total training time: 0.03 seconds.
-- Epoch 5
Norm: 36.05, NNZs: 32933, Bias: -0.854182, T: 35188, Avg. loss: 0.010095
Total training time: 0.03 seconds.
Norm: 32.14, NNZs: 49231, Bias: -1.038398, T: 35188, Avg. loss: 0.005373
Total training time: 0.03 seconds.
-- Epoch 5
-- Epoch 5
Norm: 35.30, NNZs: 30572, Bias: -0.974218, T: 35188, Avg. loss: 0.008078
Total training time: 0.03 seconds.
-- Epoch 5
Norm: 37.69, NNZs: 38503, Bias: -1.059984, T: 26391, Avg. loss: 0.024971
Total training time: 0.04 seconds.
-- Epoch 4
Norm: 34.38, NNZs: 44364, Bias: -0.976367, T: 43985, Avg. loss: 0.003037
Total training time: 0.04 seconds.
Norm: 34.63, NNZs: 47088, Bias: -0.969969, T: 43985, Avg. loss: 0.004070
Total training time: 0.04 seconds.
-- Epoch 6
-- Epoch 6
Norm: 39.00, NNZs: 34667, Bias: -0.997447, T: 43985, Avg. loss: 0.007653
Total training time: 0.04 seconds.
Norm: 30.29, NNZs: 35140, Bias: -0.893602, T: 35188, Avg. loss: 0.008769
Total training time: 0.04 seconds.
-- Epoch 6
-- Epoch 5
-- Epoch 1
Norm: 33.29, NNZs: 40349, Bias: -0.909590, T: 43985, Avg. loss: 0.007494
Total training time: 0.04 seconds.
-- Epoch 6
-- Epoch 1
-- Epoch 1
Norm: 35.74, NNZs: 30591, Bias: -0.968821, T: 43985, Avg. loss: 0.004961
Norm: 32.74, NNZs: 49268, Bias: -1.045104, T: 43985, Avg. loss: 0.004568
Total training time: 0.03 seconds.
Total training time: 0.03 seconds.
-- Epoch 6
-- Epoch 6
Norm: 39.72, NNZs: 38580, Bias: -1.091587, T: 35188, Avg. loss: 0.013113
-- Epoch 1
Norm: 34.81, NNZs: 47102, Bias: -0.966532, T: 52782, Avg. loss: 0.003373
Total training time: 0.04 seconds.
Norm: 30.53, NNZs: 35145, Bias: -0.911471, T: 43985, Avg. loss: 0.007013
Total training time: 0.05 seconds.
Norm: 39.40, NNZs: 34695, Bias: -1.011498, T: 52782, Avg. loss: 0.005810
Norm: 35.18, NNZs: 37800, Bias: -0.966474, T: 26391, Avg. loss: 0.020021
Norm: 33.40, NNZs: 40349, Bias: -0.915750, T: 52782, Avg. loss: 0.006986
Total training time: 0.04 seconds.
Norm: 34.57, NNZs: 44365, Bias: -0.982176, T: 52782, Avg. loss: 0.001877
Total training time: 0.04 seconds.
Total training time: 0.04 seconds.
Total training time: 0.04 seconds.
-- Epoch 6
Total training time: 0.05 seconds.
Norm: 36.35, NNZs: 32978, Bias: -0.854373, T: 43985, Avg. loss: 0.007656
Total training time: 0.05 seconds.
-- Epoch 7
-- Epoch 4
-- Epoch 7
-- Epoch 6
Norm: 31.86, NNZs: 26420, Bias: -0.954376, T: 17594, Avg. loss: 0.037374
Total training time: 0.04 seconds.
Norm: 33.06, NNZs: 49292, Bias: -1.060592, T: 52782, Avg. loss: 0.003539
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 35.89, NNZs: 30591, Bias: -0.973704, T: 52782, Avg. loss: 0.003843
Total training time: 0.04 seconds.
Norm: 37.01, NNZs: 46541, Bias: -1.051963, T: 35188, Avg. loss: 0.012572
Total training time: 0.05 seconds.
-- Epoch 7
-- Epoch 7
-- Epoch 5
Norm: 25.06, NNZs: 45643, Bias: -0.959627, T: 8806, Avg. loss: 0.114645
Norm: 24.40, NNZs: 28610, Bias: -0.906776, T: 8806, Avg. loss: 0.110822
-- Epoch 1
-- Epoch 1
Norm: 26.37, NNZs: 35041, Bias: -0.886402, T: 8806, Avg. loss: 0.112376
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 36.35, NNZs: 37829, Bias: -0.993037, T: 35188, Avg. loss: 0.011471
Norm: 39.63, NNZs: 34701, Bias: -1.034364, T: 61579, Avg. loss: 0.005066
-- Epoch 5
Total training time: 0.06 seconds.
-- Epoch 8
Norm: 36.49, NNZs: 32983, Bias: -0.878861, T: 52782, Avg. loss: 0.007299
Norm: 23.97, NNZs: 32732, Bias: -1.023811, T: 8806, Avg. loss: 0.091094
Total training time: 0.01 seconds.
Total training time: 0.05 seconds.
Norm: 33.29, NNZs: 49298, Bias: -1.061429, T: 61579, Avg. loss: 0.002986
Total training time: 0.05 seconds.
Norm: 35.97, NNZs: 30591, Bias: -0.975983, T: 61579, Avg. loss: 0.003470
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 7
-- Epoch 8
-- Epoch 8
Norm: 24.98, NNZs: 35344, Bias: -1.002979, T: 8806, Avg. loss: 0.121323
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 25.31, NNZs: 24390, Bias: -0.988805, T: 8806, Avg. loss: 0.107997
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 7
Norm: 25.91, NNZs: 46485, Bias: -0.916192, T: 8806, Avg. loss: 0.102329
Total training time: 0.02 seconds.
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 2
Norm: 34.44, NNZs: 26654, Bias: -0.960585, T: 26391, Avg. loss: 0.016629
Total training time: 0.05 seconds.
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 39.82, NNZs: 34701, Bias: -1.020123, T: 70376, Avg. loss: 0.004467
Total training time: 0.06 seconds.
Norm: 36.01, NNZs: 30591, Bias: -0.974780, T: 70376, Avg. loss: 0.003207
Total training time: 0.05 seconds.
-- Epoch 9
Norm: 36.60, NNZs: 32993, Bias: -0.864693, T: 61579, Avg. loss: 0.006514
Total training time: 0.06 seconds.
-- Epoch 9
Norm: 29.16, NNZs: 34980, Bias: -1.016705, T: 17612, Avg. loss: 0.025624
Total training time: 0.02 seconds.
-- Epoch 8
-- Epoch 7
Norm: 34.70, NNZs: 44371, Bias: -0.982041, T: 61579, Avg. loss: 0.001468
Total training time: 0.07 seconds.
-- Epoch 8
Norm: 36.65, NNZs: 32993, Bias: -0.876378, T: 70376, Avg. loss: 0.006560
Total training time: 0.07 seconds.
Norm: 37.77, NNZs: 46544, Bias: -1.052005, T: 43985, Avg. loss: 0.008526
Total training time: 0.07 seconds.
Norm: 30.79, NNZs: 35159, Bias: -0.907946, T: 52782, Avg. loss: 0.006630
Total training time: 0.07 seconds.
-- Epoch 9
-- Epoch 6
Norm: 35.58, NNZs: 26792, Bias: -0.981865, T: 35188, Avg. loss: 0.009370
Total training time: 0.06 seconds.
-- Epoch 5
Norm: 39.89, NNZs: 34702, Bias: -1.029163, T: 79173, Avg. loss: 0.004152
Total training time: 0.07 seconds.
Norm: 36.03, NNZs: 30591, Bias: -0.976066, T: 79173, Avg. loss: 0.003164
Total training time: 0.06 seconds.
-- Epoch 10
-- Epoch 7
-- Epoch 10
Norm: 34.87, NNZs: 47102, Bias: -0.972595, T: 61579, Avg. loss: 0.002965
Total training time: 0.07 seconds.
-- Epoch 8
Norm: 34.82, NNZs: 44371, Bias: -0.981578, T: 70376, Avg. loss: 0.001378
Total training time: 0.07 seconds.
Norm: 33.45, NNZs: 49302, Bias: -1.063554, T: 70376, Avg. loss: 0.002680
Total training time: 0.07 seconds.
-- Epoch 9
-- Epoch 9
Norm: 36.70, NNZs: 32993, Bias: -0.861565, T: 79173, Avg. loss: 0.006145
Total training time: 0.07 seconds.
-- Epoch 10
-- Epoch 1
Norm: 33.21, NNZs: 46957, Bias: -1.018568, T: 17612, Avg. loss: 0.049368
Total training time: 0.04 seconds.
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 38.13, NNZs: 46544, Bias: -1.058794, T: 52782, Avg. loss: 0.006792
Total training time: 0.08 seconds.
Norm: 36.05, NNZs: 26801, Bias: -0.997927, T: 43985, Avg. loss: 0.006124
Total training time: 0.07 seconds.
-- Epoch 2
-- Epoch 7
-- Epoch 6
Norm: 39.98, NNZs: 34704, Bias: -1.019217, T: 87970, Avg. loss: 0.003958
Norm: 36.05, NNZs: 30591, Bias: -0.976029, T: 87970, Avg. loss: 0.003103
Total training time: 0.07 seconds.
Total training time: 0.08 seconds.
-- Epoch 11
-- Epoch 11
Norm: 34.87, NNZs: 44374, Bias: -0.990088, T: 79173, Avg. loss: 0.001180
Total training time: 0.08 seconds.
-- Epoch 10
Norm: 26.45, NNZs: 46941, Bias: -0.939930, T: 8806, Avg. loss: 0.120359
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 3
Norm: 30.92, NNZs: 35164, Bias: -0.908467, T: 61579, Avg. loss: 0.006117
Total training time: 0.08 seconds.
-- Epoch 5
Norm: 31.81, NNZs: 28462, Bias: -0.986679, T: 17612, Avg. loss: 0.038363
Norm: 31.95, NNZs: 48186, Bias: -0.995984, T: 17612, Avg. loss: 0.042922
Total training time: 0.06 seconds.
Total training time: 0.05 seconds.
Norm: 36.72, NNZs: 32994, Bias: -0.884623, T: 87970, Avg. loss: 0.006724
Total training time: 0.08 seconds.
Convergence after 10 epochs took 0.08 seconds
Norm: 38.31, NNZs: 46544, Bias: -1.062240, T: 61579, Avg. loss: 0.005859
Norm: 26.44, NNZs: 36701, Bias: -0.873149, T: 8806, Avg. loss: 0.103174
Total training time: 0.03 seconds.
Norm: 32.74, NNZs: 37009, Bias: -0.839083, T: 17612, Avg. loss: 0.037493
Total training time: 0.07 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 3
-- Epoch 3
Norm: 32.01, NNZs: 30693, Bias: -0.924421, T: 17612, Avg. loss: 0.040679
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 1
Norm: 31.09, NNZs: 35192, Bias: -1.051435, T: 26418, Avg. loss: 0.010944
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 35.01, NNZs: 48748, Bias: -1.020047, T: 26418, Avg. loss: 0.021017
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 34.41, NNZs: 28761, Bias: -0.980430, T: 26418, Avg. loss: 0.017073
Total training time: 0.06 seconds.
Norm: 34.71, NNZs: 37155, Bias: -0.830529, T: 26418, Avg. loss: 0.016284
Total training time: 0.07 seconds.
Norm: 34.91, NNZs: 31083, Bias: -0.961009, T: 26418, Avg. loss: 0.016458
-- Epoch 4
Total training time: 0.05 seconds.
Norm: 31.02, NNZs: 37616, Bias: -0.849411, T: 17612, Avg. loss: 0.028418
Total training time: 0.04 seconds.
-- Epoch 4
Norm: 37.42, NNZs: 47936, Bias: -1.020002, T: 26418, Avg. loss: 0.023714
-- Epoch 3
Total training time: 0.05 seconds.
Norm: 24.22, NNZs: 32950, Bias: -0.861243, T: 8806, Avg. loss: 0.085396
Total training time: 0.02 seconds.
-- Epoch 4
-- Epoch 2
Norm: 25.65, NNZs: 43932, Bias: -0.878728, T: 8806, Avg. loss: 0.116878
Total training time: 0.01 seconds.
Norm: 30.90, NNZs: 48168, Bias: -0.911213, T: 17612, Avg. loss: 0.028281
Total training time: 0.05 seconds.
-- Epoch 2
-- Epoch 3
Norm: 36.50, NNZs: 48786, Bias: -1.049233, T: 35224, Avg. loss: 0.012976
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 36.29, NNZs: 26839, Bias: -0.998552, T: 52782, Avg. loss: 0.004925
Total training time: 0.10 seconds.
Norm: 40.64, NNZs: 38612, Bias: -1.091192, T: 43985, Avg. loss: 0.007116
Total training time: 0.10 seconds.
Norm: 35.79, NNZs: 28836, Bias: -0.986155, T: 35224, Avg. loss: 0.010336
Total training time: 0.06 seconds.
Norm: 30.67, NNZs: 45060, Bias: -0.960887, T: 17612, Avg. loss: 0.033153
Total training time: 0.08 seconds.
-- Epoch 5
-- Epoch 3
Norm: 32.62, NNZs: 37732, Bias: -0.883944, T: 26418, Avg. loss: 0.014315
Total training time: 0.05 seconds.
-- Epoch 8
-- Epoch 4
Norm: 36.05, NNZs: 30591, Bias: -0.975992, T: 96767, Avg. loss: 0.003068
Total training time: 0.09 seconds.
Norm: 31.83, NNZs: 35289, Bias: -1.054027, T: 35224, Avg. loss: 0.005644
Total training time: 0.06 seconds.
Convergence after 11 epochs took 0.09 seconds
-- Epoch 5
Norm: 33.47, NNZs: 40349, Bias: -0.915433, T: 61579, Avg. loss: 0.006757
Total training time: 0.10 seconds.
-- Epoch 8
-- Epoch 8
Norm: 37.02, NNZs: 37862, Bias: -1.002821, T: 43985, Avg. loss: 0.008611
Total training time: 0.09 seconds.
Norm: 37.23, NNZs: 48819, Bias: -1.049960, T: 44030, Avg. loss: 0.009076
Total training time: 0.07 seconds.
Norm: 33.15, NNZs: 48801, Bias: -0.945040, T: 26418, Avg. loss: 0.013113
Total training time: 0.06 seconds.
-- Epoch 6
-- Epoch 4
Norm: 39.33, NNZs: 47985, Bias: -1.065548, T: 35224, Avg. loss: 0.012133
Norm: 36.48, NNZs: 28875, Bias: -1.015258, T: 44030, Avg. loss: 0.007475
Total training time: 0.07 seconds.
Total training time: 0.06 seconds.
Norm: 35.98, NNZs: 31134, Bias: -0.997642, T: 35224, Avg. loss: 0.007167
Total training time: 0.06 seconds.
-- Epoch 6
-- Epoch 5
-- Epoch 5
Norm: 33.22, NNZs: 37779, Bias: -0.908523, T: 35224, Avg. loss: 0.009758
Total training time: 0.05 seconds.
Norm: 28.23, NNZs: 34466, Bias: -0.860743, T: 17612, Avg. loss: 0.023200
Total training time: 0.03 seconds.
Norm: 33.61, NNZs: 48809, Bias: -0.905605, T: 17612, Avg. loss: 0.042019
Total training time: 0.05 seconds.
-- Epoch 5
-- Epoch 3
-- Epoch 3
-- Epoch 4
-- Epoch 1
-- Epoch 6
Norm: 34.98, NNZs: 47114, Bias: -0.975717, T: 70376, Avg. loss: 0.002942
Total training time: 0.11 seconds.
Norm: 37.58, NNZs: 48832, Bias: -1.053933, T: 52836, Avg. loss: 0.007504
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 36.84, NNZs: 28894, Bias: -1.020471, T: 52836, Avg. loss: 0.005741
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 40.28, NNZs: 48022, Bias: -1.077162, T: 44030, Avg. loss: 0.006787
Total training time: 0.07 seconds.
-- Epoch 6
Norm: 32.28, NNZs: 35342, Bias: -1.061847, T: 44030, Avg. loss: 0.004407
Total training time: 0.07 seconds.
Norm: 33.52, NNZs: 37806, Bias: -0.903143, T: 44030, Avg. loss: 0.007949
-- Epoch 6
Norm: 36.45, NNZs: 49034, Bias: -0.981287, T: 26418, Avg. loss: 0.019561
Total training time: 0.06 seconds.
Norm: 35.37, NNZs: 37170, Bias: -0.842246, T: 35224, Avg. loss: 0.010202
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 36.55, NNZs: 31144, Bias: -1.000524, T: 44030, Avg. loss: 0.004086
Total training time: 0.07 seconds.
-- Epoch 5
-- Epoch 6
Norm: 33.08, NNZs: 45244, Bias: -0.961127, T: 26418, Avg. loss: 0.014446
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 34.21, NNZs: 48861, Bias: -0.946569, T: 35224, Avg. loss: 0.006818
Total training time: 0.07 seconds.
-- Epoch 5
-- Epoch 6
-- Epoch 9
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 37.03, NNZs: 28900, Bias: -1.028743, T: 61642, Avg. loss: 0.005033
Total training time: 0.08 seconds.
Norm: 36.76, NNZs: 31161, Bias: -1.004453, T: 52836, Avg. loss: 0.002496
Total training time: 0.08 seconds.
Norm: 29.58, NNZs: 34573, Bias: -0.877824, T: 26418, Avg. loss: 0.011309
Total training time: 0.04 seconds.
-- Epoch 8
-- Epoch 7
-- Epoch 4
Norm: 32.55, NNZs: 35345, Bias: -1.082169, T: 52836, Avg. loss: 0.003902
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 31.99, NNZs: 45476, Bias: -0.950377, T: 17612, Avg. loss: 0.043298
Total training time: 0.03 seconds.
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 34.16, NNZs: 45344, Bias: -0.995628, T: 35224, Avg. loss: 0.007898
-- Epoch 6
Norm: 37.78, NNZs: 49079, Bias: -0.974196, T: 35224, Avg. loss: 0.009881
Total training time: 0.07 seconds.
Norm: 31.04, NNZs: 35174, Bias: -0.914790, T: 70376, Avg. loss: 0.006008
Norm: 34.96, NNZs: 44374, Bias: -0.989378, T: 87970, Avg. loss: 0.001178
Total training time: 0.12 seconds.
Total training time: 0.12 seconds.
-- Epoch 5
-- Epoch 1
Norm: 41.07, NNZs: 38613, Bias: -1.097130, T: 52782, Avg. loss: 0.004841
-- Epoch 9
Norm: 33.53, NNZs: 49302, Bias: -1.071166, T: 79173, Avg. loss: 0.002467
Total training time: 0.11 seconds.
Norm: 35.58, NNZs: 37172, Bias: -0.830669, T: 44030, Avg. loss: 0.007942
Total training time: 0.11 seconds.
Total training time: 0.11 seconds.
Total training time: 0.13 seconds.
Norm: 37.25, NNZs: 28904, Bias: -1.022747, T: 70448, Avg. loss: 0.004776
Total training time: 0.09 seconds.
-- Epoch 6
-- Epoch 10
-- Epoch 5
Norm: 32.74, NNZs: 35348, Bias: -1.076941, T: 61642, Avg. loss: 0.003219
Total training time: 0.09 seconds.
-- Epoch 11
Norm: 40.04, NNZs: 34704, Bias: -1.021272, T: 96767, Avg. loss: 0.004038
Total training time: 0.13 seconds.
Norm: 38.42, NNZs: 46544, Bias: -1.064378, T: 70376, Avg. loss: 0.005570
Total training time: 0.13 seconds.
-- Epoch 7
-- Epoch 9
-- Epoch 8
-- Epoch 9
Norm: 40.70, NNZs: 48031, Bias: -1.082717, T: 52836, Avg. loss: 0.004140
Total training time: 0.09 seconds.
Norm: 34.63, NNZs: 45367, Bias: -0.999315, T: 44030, Avg. loss: 0.004467
Total training time: 0.12 seconds.
Norm: 36.88, NNZs: 31161, Bias: -1.007938, T: 61642, Avg. loss: 0.002084
Total training time: 0.09 seconds.
-- Epoch 7
-- Epoch 6
-- Epoch 8
Norm: 34.72, NNZs: 48881, Bias: -0.950428, T: 44030, Avg. loss: 0.004208
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 38.45, NNZs: 49120, Bias: -1.003640, T: 44030, Avg. loss: 0.006929
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 31.10, NNZs: 35180, Bias: -0.920833, T: 79173, Avg. loss: 0.005706
Norm: 35.00, NNZs: 47114, Bias: -0.975424, T: 79173, Avg. loss: 0.002618
Total training time: 0.14 seconds.
Norm: 33.52, NNZs: 40349, Bias: -0.915741, T: 70376, Avg. loss: 0.006601
Total training time: 0.14 seconds.
Total training time: 0.14 seconds.
Norm: 26.12, NNZs: 44104, Bias: -0.965864, T: 8797, Avg. loss: 0.106311
Total training time: 0.03 seconds.
Norm: 33.66, NNZs: 37813, Bias: -0.910787, T: 52836, Avg. loss: 0.007340
Total training time: 0.09 seconds.
Norm: 37.38, NNZs: 28907, Bias: -1.026894, T: 79254, Avg. loss: 0.004738
Total training time: 0.11 seconds.
-- Epoch 7
Convergence after 11 epochs took 0.14 seconds
Norm: 32.86, NNZs: 35355, Bias: -1.084859, T: 70448, Avg. loss: 0.003064
-- Epoch 10
Total training time: 0.10 seconds.
-- Epoch 9
-- Epoch 9
-- Epoch 10
Norm: 40.95, NNZs: 48089, Bias: -1.078481, T: 61642, Avg. loss: 0.003178
Total training time: 0.10 seconds.
-- Epoch 8
Norm: 34.95, NNZs: 45378, Bias: -1.001113, T: 52836, Avg. loss: 0.003581
Total training time: 0.12 seconds.
Norm: 37.79, NNZs: 48838, Bias: -1.054056, T: 61642, Avg. loss: 0.006781
Total training time: 0.11 seconds.
Norm: 34.95, NNZs: 48897, Bias: -0.956783, T: 52836, Avg. loss: 0.002953
Total training time: 0.10 seconds.
-- Epoch 7
-- Epoch 8
-- Epoch 7
Norm: 35.02, NNZs: 44378, Bias: -0.983288, T: 96767, Avg. loss: 0.000999
-- Epoch 10
Norm: 38.49, NNZs: 46544, Bias: -1.066545, T: 79173, Avg. loss: 0.005367
-- Epoch 2
Norm: 33.74, NNZs: 37816, Bias: -0.910655, T: 61642, Avg. loss: 0.007001
Total training time: 0.09 seconds.
Norm: 30.25, NNZs: 34595, Bias: -0.888678, T: 35224, Avg. loss: 0.008181
Total training time: 0.07 seconds.
-- Epoch 8
Norm: 32.93, NNZs: 35360, Bias: -1.087752, T: 79254, Avg. loss: 0.002864
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 41.06, NNZs: 48090, Bias: -1.083066, T: 70448, Avg. loss: 0.002636
Total training time: 0.10 seconds.
Norm: 35.12, NNZs: 48908, Bias: -0.956286, T: 61642, Avg. loss: 0.002500
Total training time: 0.10 seconds.
Norm: 34.77, NNZs: 45756, Bias: -0.966345, T: 26418, Avg. loss: 0.021823
Total training time: 0.06 seconds.
Norm: 37.50, NNZs: 28915, Bias: -1.024398, T: 88060, Avg. loss: 0.004537
Total training time: 0.11 seconds.
-- Epoch 8
-- Epoch 4
Norm: 36.94, NNZs: 31161, Bias: -1.006805, T: 70448, Avg. loss: 0.001790
Total training time: 0.11 seconds.
-- Epoch 11
-- Epoch 9
Norm: 35.16, NNZs: 45387, Bias: -0.993576, T: 61642, Avg. loss: 0.002864
Total training time: 0.13 seconds.
-- Epoch 9
-- Epoch 8
Norm: 37.91, NNZs: 48838, Bias: -1.062467, T: 70448, Avg. loss: 0.006543
Total training time: 0.12 seconds.
Norm: 35.66, NNZs: 37178, Bias: -0.846068, T: 52836, Avg. loss: 0.007738
Total training time: 0.13 seconds.
-- Epoch 9
Norm: 38.84, NNZs: 49123, Bias: -1.001549, T: 52836, Avg. loss: 0.005098
Total training time: 0.10 seconds.
-- Epoch 7
-- Epoch 7
Norm: 33.79, NNZs: 37818, Bias: -0.911229, T: 70448, Avg. loss: 0.006848
Total training time: 0.10 seconds.
-- Epoch 9
Norm: 30.60, NNZs: 34623, Bias: -0.911757, T: 44030, Avg. loss: 0.006719
Total training time: 0.07 seconds.
-- Epoch 10
-- Epoch 6
Norm: 36.38, NNZs: 26840, Bias: -1.002443, T: 61579, Avg. loss: 0.004294
Total training time: 0.15 seconds.
Total training time: 0.15 seconds.
Norm: 37.53, NNZs: 28915, Bias: -1.036948, T: 96866, Avg. loss: 0.004178
Total training time: 0.12 seconds.
Convergence after 11 epochs took 0.15 seconds
Norm: 35.93, NNZs: 45838, Bias: -1.009364, T: 35224, Avg. loss: 0.013827
Total training time: 0.06 seconds.
-- Epoch 10
Norm: 33.59, NNZs: 49303, Bias: -1.070972, T: 87970, Avg. loss: 0.002275
Total training time: 0.14 seconds.
Norm: 37.41, NNZs: 37906, Bias: -1.006204, T: 52782, Avg. loss: 0.007036
Total training time: 0.15 seconds.
Convergence after 11 epochs took 0.12 seconds
Total training time: 0.14 seconds.
Norm: 36.97, NNZs: 31161, Bias: -1.008897, T: 79254, Avg. loss: 0.001719
Norm: 41.34, NNZs: 38613, Bias: -1.094144, T: 61579, Avg. loss: 0.003945
Total training time: 0.15 seconds.
Norm: 33.55, NNZs: 40349, Bias: -0.917276, T: 79173, Avg. loss: 0.006525
-- Epoch 11
-- Epoch 8
Total training time: 0.11 seconds.
-- Epoch 5
Norm: 35.73, NNZs: 37178, Bias: -0.834238, T: 61642, Avg. loss: 0.007325
Total training time: 0.15 seconds.
-- Epoch 8
Norm: 26.30, NNZs: 36978, Bias: -0.970062, T: 8797, Avg. loss: 0.130959
Total training time: 0.03 seconds.
Total training time: 0.14 seconds.
Norm: 41.13, NNZs: 48090, Bias: -1.083121, T: 79254, Avg. loss: 0.002361
Total training time: 0.12 seconds.
Norm: 31.18, NNZs: 35187, Bias: -0.920316, T: 87970, Avg. loss: 0.005596
Total training time: 0.16 seconds.
-- Epoch 7
-- Epoch 10
Convergence after 10 epochs took 0.16 seconds
-- Epoch 2
Norm: 35.32, NNZs: 45393, Bias: -1.022622, T: 70448, Avg. loss: 0.003123
Total training time: 0.14 seconds.
-- Epoch 8
-- Epoch 9
-- Epoch 1
-- Epoch 1
Norm: 31.22, NNZs: 45521, Bias: -0.920271, T: 17594, Avg. loss: 0.028017
Total training time: 0.05 seconds.
Norm: 38.01, NNZs: 48838, Bias: -1.060527, T: 79254, Avg. loss: 0.006331
-- Epoch 3
-- Epoch 10
Total training time: 0.13 seconds.
Norm: 39.03, NNZs: 49143, Bias: -1.026438, T: 61642, Avg. loss: 0.004539
-- Epoch 10
Norm: 33.82, NNZs: 37818, Bias: -0.911852, T: 79254, Avg. loss: 0.006768
Total training time: 0.11 seconds.
-- Epoch 1
Total training time: 0.11 seconds.
Norm: 35.02, NNZs: 47114, Bias: -0.973407, T: 87970, Avg. loss: 0.002594
Norm: 38.54, NNZs: 46544, Bias: -1.069182, T: 87970, Avg. loss: 0.005335
Total training time: 0.16 seconds.
Total training time: 0.16 seconds.
Norm: 36.43, NNZs: 26840, Bias: -1.001658, T: 70376, Avg. loss: 0.004068
-- Epoch 11
Total training time: 0.15 seconds.
-- Epoch 1
Convergence after 10 epochs took 0.16 seconds
-- Epoch 1
Norm: 37.65, NNZs: 37906, Bias: -1.002392, T: 61579, Avg. loss: 0.006238
Total training time: 0.16 seconds.
Norm: 30.92, NNZs: 34641, Bias: -0.912382, T: 52836, Avg. loss: 0.006129
Norm: 33.63, NNZs: 49303, Bias: -1.068399, T: 96767, Avg. loss: 0.002169
Total training time: 0.09 seconds.
Total training time: 0.15 seconds.
-- Epoch 10
-- Epoch 8
Convergence after 11 epochs took 0.15 seconds
-- Epoch 7
Norm: 25.61, NNZs: 30321, Bias: -0.935449, T: 8797, Avg. loss: 0.112610
-- Epoch 8
Total training time: 0.01 seconds.
Norm: 35.23, NNZs: 48908, Bias: -0.954120, T: 70448, Avg. loss: 0.002305
-- Epoch 1
-- Epoch 2
Norm: 26.62, NNZs: 38463, Bias: -0.870384, T: 8797, Avg. loss: 0.112015
Total training time: 0.01 seconds.
Norm: 32.71, NNZs: 46246, Bias: -0.946778, T: 26391, Avg. loss: 0.011548
Total training time: 0.06 seconds.
Norm: 34.74, NNZs: 43213, Bias: -0.934142, T: 17594, Avg. loss: 0.048735
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 4
-- Epoch 3
Norm: 38.60, NNZs: 46544, Bias: -1.069818, T: 96767, Avg. loss: 0.005323
Total training time: 0.17 seconds.
Convergence after 11 epochs took 0.17 seconds
-- Epoch 9
Norm: 25.23, NNZs: 38286, Bias: -0.979712, T: 8797, Avg. loss: 0.106363
Total training time: 0.01 seconds.
Total training time: 0.12 seconds.
-- Epoch 2
Norm: 37.74, NNZs: 37906, Bias: -1.027153, T: 70376, Avg. loss: 0.005956
Total training time: 0.16 seconds.
-- Epoch 9
-- Epoch 9
Norm: 39.20, NNZs: 49143, Bias: -1.015173, T: 70448, Avg. loss: 0.004056
Norm: 37.00, NNZs: 31161, Bias: -1.008607, T: 88060, Avg. loss: 0.001654
Total training time: 0.13 seconds.
Norm: 32.99, NNZs: 35360, Bias: -1.086649, T: 88060, Avg. loss: 0.002764
Total training time: 0.13 seconds.
Norm: 32.73, NNZs: 32149, Bias: -0.981685, T: 17594, Avg. loss: 0.038628
Total training time: 0.01 seconds.
Convergence after 10 epochs took 0.13 seconds
-- Epoch 3
Norm: 41.46, NNZs: 38615, Bias: -1.108377, T: 70376, Avg. loss: 0.003423
Total training time: 0.17 seconds.
Norm: 37.80, NNZs: 43660, Bias: -0.956592, T: 26391, Avg. loss: 0.019342
Total training time: 0.05 seconds.
Norm: 32.39, NNZs: 40357, Bias: -0.870588, T: 17594, Avg. loss: 0.034406
Total training time: 0.01 seconds.
-- Epoch 9
Norm: 33.24, NNZs: 46294, Bias: -0.946895, T: 35188, Avg. loss: 0.006605
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 5
Norm: 36.46, NNZs: 26840, Bias: -1.002887, T: 79173, Avg. loss: 0.003984
Total training time: 0.17 seconds.
-- Epoch 10
Norm: 31.47, NNZs: 49666, Bias: -0.959111, T: 17594, Avg. loss: 0.033424
Total training time: 0.01 seconds.
-- Epoch 3
Norm: 26.55, NNZs: 43774, Bias: -0.962616, T: 8806, Avg. loss: 0.131365
Total training time: 0.01 seconds.
Norm: 37.87, NNZs: 37909, Bias: -1.014253, T: 79173, Avg. loss: 0.005705
Total training time: 0.17 seconds.
-- Epoch 10
-- Epoch 11
-- Epoch 10
Norm: 36.65, NNZs: 45859, Bias: -0.994852, T: 44030, Avg. loss: 0.010618
Norm: 33.57, NNZs: 40351, Bias: -0.916644, T: 87970, Avg. loss: 0.006447
Total training time: 0.17 seconds.
Norm: 35.33, NNZs: 32323, Bias: -1.013506, T: 26391, Avg. loss: 0.013853
Total training time: 0.02 seconds.
Convergence after 10 epochs took 0.17 seconds
Norm: 41.55, NNZs: 38615, Bias: -1.104399, T: 79173, Avg. loss: 0.003051
Total training time: 0.18 seconds.
Norm: 34.23, NNZs: 40503, Bias: -0.918300, T: 26391, Avg. loss: 0.014338
Norm: 38.91, NNZs: 43728, Bias: -0.966009, T: 35188, Avg. loss: 0.009296
Total training time: 0.06 seconds.
Total training time: 0.02 seconds.
-- Epoch 4
-- Epoch 10
Norm: 33.42, NNZs: 46305, Bias: -0.961618, T: 43985, Avg. loss: 0.005231
Total training time: 0.07 seconds.
-- Epoch 5
-- Epoch 4
-- Epoch 6
Norm: 24.57, NNZs: 33106, Bias: -0.999277, T: 8797, Avg. loss: 0.110146
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 36.47, NNZs: 26840, Bias: -1.001807, T: 87970, Avg. loss: 0.003928
Total training time: 0.17 seconds.
-- Epoch 11
Norm: 33.62, NNZs: 49843, Bias: -0.982690, T: 26391, Avg. loss: 0.012937
Total training time: 0.02 seconds.
-- Epoch 4
Norm: 37.94, NNZs: 37909, Bias: -1.015409, T: 87970, Avg. loss: 0.005554
Total training time: 0.17 seconds.
-- Epoch 11
Norm: 33.84, NNZs: 37818, Bias: -0.911473, T: 88060, Avg. loss: 0.006688
Total training time: 0.13 seconds.
Norm: 35.76, NNZs: 37178, Bias: -0.836191, T: 70448, Avg. loss: 0.007340
Total training time: 0.10 seconds.
-- Epoch 1
Convergence after 10 epochs took 0.13 seconds
-- Epoch 2
Norm: 35.29, NNZs: 48908, Bias: -0.964367, T: 79254, Avg. loss: 0.002139
Total training time: 0.14 seconds.
Total training time: 0.16 seconds.
-- Epoch 6
-- Epoch 9
Norm: 41.60, NNZs: 38617, Bias: -1.108916, T: 87970, Avg. loss: 0.002970
Total training time: 0.18 seconds.
Norm: 34.98, NNZs: 40545, Bias: -0.915935, T: 35188, Avg. loss: 0.008282
Total training time: 0.02 seconds.
-- Epoch 11
Norm: 33.60, NNZs: 46307, Bias: -0.961668, T: 52782, Avg. loss: 0.004763
Total training time: 0.08 seconds.
-- Epoch 5
-- Epoch 7
Norm: 24.28, NNZs: 36679, Bias: -0.890988, T: 8797, Avg. loss: 0.089436
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 36.35, NNZs: 32412, Bias: -1.028651, T: 35188, Avg. loss: 0.006226
Total training time: 0.03 seconds.
-- Epoch 5
Norm: 34.35, NNZs: 49889, Bias: -0.993521, T: 35188, Avg. loss: 0.006103
Total training time: 0.02 seconds.
Norm: 35.46, NNZs: 45394, Bias: -1.022069, T: 79254, Avg. loss: 0.002654
-- Epoch 5
Norm: 41.16, NNZs: 48090, Bias: -1.082462, T: 88060, Avg. loss: 0.002221
Total training time: 0.17 seconds.
Norm: 37.99, NNZs: 37920, Bias: -1.018672, T: 96767, Avg. loss: 0.005491
Total training time: 0.18 seconds.
Convergence after 11 epochs took 0.18 seconds
-- Epoch 1
Total training time: 0.15 seconds.
-- Epoch 11
Norm: 38.10, NNZs: 48838, Bias: -1.073387, T: 88060, Avg. loss: 0.006486
Total training time: 0.16 seconds.
-- Epoch 11
Norm: 20.61, NNZs: 32320, Bias: -0.906086, T: 8797, Avg. loss: 0.102917
Total training time: 0.02 seconds.
Norm: 37.09, NNZs: 45904, Bias: -1.006270, T: 52836, Avg. loss: 0.009485
Total training time: 0.10 seconds.
Norm: 35.79, NNZs: 37178, Bias: -0.831092, T: 79254, Avg. loss: 0.007239
Total training time: 0.17 seconds.
Norm: 34.68, NNZs: 46479, Bias: -0.915311, T: 17612, Avg. loss: 0.047642
-- Epoch 2
Total training time: 0.03 seconds.
Norm: 41.66, NNZs: 38617, Bias: -1.105119, T: 96767, Avg. loss: 0.002869
Total training time: 0.19 seconds.
-- Epoch 7
-- Epoch 10
Norm: 35.34, NNZs: 40551, Bias: -0.917248, T: 43985, Avg. loss: 0.006387
-- Epoch 3
Total training time: 0.03 seconds.
Convergence after 11 epochs took 0.19 seconds
-- Epoch 6
Norm: 33.73, NNZs: 46332, Bias: -0.973563, T: 61579, Avg. loss: 0.004559
Total training time: 0.09 seconds.
-- Epoch 8
Norm: 34.57, NNZs: 49892, Bias: -0.994873, T: 43985, Avg. loss: 0.003865
Total training time: 0.03 seconds.
-- Epoch 6
Norm: 36.45, NNZs: 26840, Bias: -1.009313, T: 96767, Avg. loss: 0.003821
Total training time: 0.19 seconds.
Norm: 31.90, NNZs: 35063, Bias: -1.022917, T: 17594, Avg. loss: 0.037268
Total training time: 0.03 seconds.
Norm: 41.18, NNZs: 48090, Bias: -1.081973, T: 96866, Avg. loss: 0.002177
Total training time: 0.15 seconds.
Convergence after 11 epochs took 0.19 seconds
Total training time: 0.14 seconds.
-- Epoch 3
Convergence after 11 epochs took 0.15 seconds
Norm: 26.46, NNZs: 42458, Bias: -0.946275, T: 8806, Avg. loss: 0.107552
-- Epoch 10
Total training time: 0.01 seconds.
Norm: 31.15, NNZs: 34641, Bias: -0.912529, T: 61642, Avg. loss: 0.005534
Total training time: 0.12 seconds.
Norm: 38.17, NNZs: 48838, Bias: -1.075217, T: 96866, Avg. loss: 0.006233
Total training time: 0.17 seconds.
Norm: 30.36, NNZs: 34880, Bias: -0.966012, T: 17594, Avg. loss: 0.049304
Total training time: 0.03 seconds.
Norm: 39.32, NNZs: 43730, Bias: -0.975655, T: 43985, Avg. loss: 0.005756
Total training time: 0.08 seconds.
Norm: 35.50, NNZs: 40553, Bias: -0.937714, T: 52782, Avg. loss: 0.005638
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 8
-- Epoch 3
-- Epoch 9
Norm: 33.84, NNZs: 46333, Bias: -0.977630, T: 70376, Avg. loss: 0.004216
-- Epoch 6
Total training time: 0.09 seconds.
-- Epoch 7
-- Epoch 9
Norm: 34.64, NNZs: 49897, Bias: -0.998193, T: 52782, Avg. loss: 0.003250
Total training time: 0.04 seconds.
Norm: 29.18, NNZs: 37971, Bias: -0.921498, T: 17594, Avg. loss: 0.025965
Total training time: 0.04 seconds.
-- Epoch 7
-- Epoch 3
Norm: 36.81, NNZs: 32431, Bias: -1.044879, T: 43985, Avg. loss: 0.003743
Total training time: 0.04 seconds.
Convergence after 11 epochs took 0.17 seconds
-- Epoch 6
-- Epoch 10
Norm: 34.23, NNZs: 35300, Bias: -1.038150, T: 26391, Avg. loss: 0.012825
Total training time: 0.04 seconds.
-- Epoch 4
Norm: 35.74, NNZs: 40557, Bias: -0.923617, T: 61579, Avg. loss: 0.005422
Total training time: 0.04 seconds.
-- Epoch 8
Norm: 33.76, NNZs: 35171, Bias: -0.940216, T: 26391, Avg. loss: 0.018622
Total training time: 0.03 seconds.
Norm: 39.48, NNZs: 43735, Bias: -0.972010, T: 52782, Avg. loss: 0.004455
Total training time: 0.08 seconds.
-- Epoch 4
-- Epoch 7
Norm: 33.90, NNZs: 46333, Bias: -0.976712, T: 79173, Avg. loss: 0.003933
Total training time: 0.10 seconds.
-- Epoch 10
Norm: 37.35, NNZs: 45912, Bias: -1.025822, T: 61642, Avg. loss: 0.008727
Total training time: 0.12 seconds.
Norm: 34.66, NNZs: 49897, Bias: -0.997840, T: 61579, Avg. loss: 0.003020
Total training time: 0.04 seconds.
-- Epoch 8
Norm: 36.99, NNZs: 32485, Bias: -1.050889, T: 52782, Avg. loss: 0.002457
Total training time: 0.05 seconds.
-- Epoch 7
Norm: 35.01, NNZs: 35422, Bias: -1.045037, T: 35188, Avg. loss: 0.005945
Total training time: 0.04 seconds.
Norm: 35.83, NNZs: 40561, Bias: -0.932426, T: 70376, Avg. loss: 0.004957
Total training time: 0.05 seconds.
-- Epoch 9
-- Epoch 5
Norm: 39.54, NNZs: 43740, Bias: -0.975161, T: 61579, Avg. loss: 0.004048
Total training time: 0.08 seconds.
Norm: 34.00, NNZs: 46341, Bias: -0.983259, T: 87970, Avg. loss: 0.004067
Total training time: 0.10 seconds.
-- Epoch 8
Norm: 30.75, NNZs: 38183, Bias: -0.935761, T: 26391, Avg. loss: 0.010416
Total training time: 0.04 seconds.
Convergence after 10 epochs took 0.10 seconds
-- Epoch 4
Norm: 34.99, NNZs: 35242, Bias: -1.004174, T: 35188, Avg. loss: 0.009421
Total training time: 0.04 seconds.
-- Epoch 5
-- Epoch 8
Norm: 37.01, NNZs: 31161, Bias: -1.008664, T: 96866, Avg. loss: 0.001593
Total training time: 0.17 seconds.
-- Epoch 1
Norm: 37.67, NNZs: 46816, Bias: -0.968955, T: 26418, Avg. loss: 0.019751
Total training time: 0.05 seconds.
Norm: 34.67, NNZs: 49897, Bias: -0.998055, T: 70376, Avg. loss: 0.002955
Total training time: 0.05 seconds.
-- Epoch 9
Norm: 37.11, NNZs: 32485, Bias: -1.054444, T: 61579, Avg. loss: 0.002128
Total training time: 0.05 seconds.
Norm: 35.91, NNZs: 40561, Bias: -0.925255, T: 79173, Avg. loss: 0.004777
Total training time: 0.05 seconds.
Norm: 39.57, NNZs: 43744, Bias: -0.975079, T: 70376, Avg. loss: 0.003853
Total training time: 0.09 seconds.
-- Epoch 8
-- Epoch 10
-- Epoch 9
Norm: 35.29, NNZs: 35426, Bias: -1.051329, T: 43985, Avg. loss: 0.003935
Total training time: 0.05 seconds.
Norm: 31.30, NNZs: 34652, Bias: -0.921645, T: 70448, Avg. loss: 0.005148
Convergence after 11 epochs took 0.17 seconds
Total training time: 0.13 seconds.
-- Epoch 6
-- Epoch 9
Norm: 31.22, NNZs: 38239, Bias: -0.941081, T: 35188, Avg. loss: 0.005451
Total training time: 0.05 seconds.
-- Epoch 5
Norm: 35.61, NNZs: 35278, Bias: -0.997672, T: 43985, Avg. loss: 0.005605
Total training time: 0.04 seconds.
-- Epoch 6
Norm: 35.94, NNZs: 40565, Bias: -0.928275, T: 87970, Avg. loss: 0.004655
Total training time: 0.05 seconds.
Convergence after 10 epochs took 0.05 seconds
Norm: 34.67, NNZs: 49897, Bias: -0.998161, T: 79173, Avg. loss: 0.002932
Total training time: 0.05 seconds.
-- Epoch 10
Norm: 39.59, NNZs: 43744, Bias: -0.975014, T: 79173, Avg. loss: 0.003768
Total training time: 0.09 seconds.
Norm: 37.20, NNZs: 32485, Bias: -1.054502, T: 70376, Avg. loss: 0.001990
Total training time: 0.06 seconds.
-- Epoch 10
-- Epoch 9
Norm: 35.51, NNZs: 45415, Bias: -1.018584, T: 88060, Avg. loss: 0.002263
Norm: 35.82, NNZs: 37180, Bias: -0.860048, T: 88060, Avg. loss: 0.007792
-- Epoch 4
Norm: 39.30, NNZs: 49152, Bias: -1.031882, T: 79254, Avg. loss: 0.003976
Total training time: 0.17 seconds.
Norm: 35.39, NNZs: 35426, Bias: -1.053973, T: 52782, Avg. loss: 0.003140
Total training time: 0.05 seconds.
-- Epoch 10
Total training time: 0.20 seconds.
Total training time: 0.20 seconds.
-- Epoch 7
Norm: 31.45, NNZs: 38245, Bias: -0.942517, T: 43985, Avg. loss: 0.004364
Total training time: 0.05 seconds.
-- Epoch 6
Norm: 35.85, NNZs: 35292, Bias: -1.008859, T: 52782, Avg. loss: 0.004227
Total training time: 0.05 seconds.
-- Epoch 7
Norm: 34.68, NNZs: 49897, Bias: -0.998122, T: 87970, Avg. loss: 0.002923
Total training time: 0.06 seconds.
Convergence after 10 epochs took 0.06 seconds
Norm: 39.59, NNZs: 43744, Bias: -0.975058, T: 87970, Avg. loss: 0.003742
Total training time: 0.10 seconds.
-- Epoch 11
-- Epoch 1
Norm: 37.25, NNZs: 32485, Bias: -1.056351, T: 79173, Avg. loss: 0.001797
Total training time: 0.06 seconds.
Convergence after 10 epochs took 0.20 seconds
-- Epoch 10
Convergence after 10 epochs took 0.20 seconds
Norm: 25.75, NNZs: 32351, Bias: -0.943952, T: 8806, Avg. loss: 0.111695
Norm: 35.44, NNZs: 35434, Bias: -1.055140, T: 61579, Avg. loss: 0.002938
Norm: 35.38, NNZs: 48908, Bias: -0.963060, T: 88060, Avg. loss: 0.002117
Total training time: 0.06 seconds.
Total training time: 0.18 seconds.
-- Epoch 8
Norm: 31.53, NNZs: 38245, Bias: -0.943047, T: 52782, Avg. loss: 0.003760
Total training time: 0.06 seconds.
Norm: 31.59, NNZs: 44049, Bias: -0.907424, T: 17612, Avg. loss: 0.027227
-- Epoch 7
Total training time: 0.03 seconds.
Norm: 35.97, NNZs: 35292, Bias: -1.008112, T: 61579, Avg. loss: 0.003612
Total training time: 0.05 seconds.
Norm: 39.59, NNZs: 43744, Bias: -0.975057, T: 96767, Avg. loss: 0.003727
Total training time: 0.10 seconds.
Total training time: 0.01 seconds.
-- Epoch 11
Norm: 38.76, NNZs: 46837, Bias: -0.983423, T: 35224, Avg. loss: 0.009504
Total training time: 0.06 seconds.
-- Epoch 8
Convergence after 11 epochs took 0.10 seconds
Norm: 37.27, NNZs: 32485, Bias: -1.057268, T: 87970, Avg. loss: 0.001702
Total training time: 0.07 seconds.
-- Epoch 11
Norm: 39.42, NNZs: 49175, Bias: -1.009932, T: 88060, Avg. loss: 0.003602
Total training time: 0.18 seconds.
Norm: 31.43, NNZs: 34668, Bias: -0.936668, T: 79254, Avg. loss: 0.005075
Total training time: 0.15 seconds.
-- Epoch 1
-- Epoch 5
-- Epoch 11
Norm: 35.46, NNZs: 35434, Bias: -1.054923, T: 70376, Avg. loss: 0.002790
Total training time: 0.06 seconds.
-- Epoch 3
-- Epoch 10
-- Epoch 9
Norm: 31.55, NNZs: 38245, Bias: -0.945508, T: 61579, Avg. loss: 0.003542
Total training time: 0.06 seconds.
-- Epoch 8
Norm: 36.03, NNZs: 35292, Bias: -1.008689, T: 70376, Avg. loss: 0.003348
Total training time: 0.06 seconds.
-- Epoch 9
-- Epoch 1
-- Epoch 2
Norm: 37.55, NNZs: 45912, Bias: -1.023350, T: 70448, Avg. loss: 0.008178
Total training time: 0.14 seconds.
Norm: 37.29, NNZs: 32486, Bias: -1.056967, T: 96767, Avg. loss: 0.001681
Total training time: 0.07 seconds.
Convergence after 11 epochs took 0.07 seconds
-- Epoch 1
Norm: 35.47, NNZs: 35434, Bias: -1.055576, T: 79173, Avg. loss: 0.002769
Total training time: 0.07 seconds.
-- Epoch 10
-- Epoch 1
Norm: 35.45, NNZs: 48919, Bias: -0.958065, T: 96866, Avg. loss: 0.001914
Total training time: 0.19 seconds.
Norm: 31.57, NNZs: 38245, Bias: -0.944381, T: 70376, Avg. loss: 0.003474
Total training time: 0.07 seconds.
-- Epoch 9
-- Epoch 9
Norm: 36.06, NNZs: 35292, Bias: -1.009912, T: 79173, Avg. loss: 0.003236
Total training time: 0.06 seconds.
-- Epoch 10
Convergence after 11 epochs took 0.19 seconds
Norm: 31.59, NNZs: 34673, Bias: -0.931063, T: 88060, Avg. loss: 0.004714
Norm: 39.16, NNZs: 46858, Bias: -0.987585, T: 44030, Avg. loss: 0.006043
Total training time: 0.07 seconds.
Norm: 25.48, NNZs: 33520, Bias: -0.969244, T: 8806, Avg. loss: 0.106363
Total training time: 0.01 seconds.
Norm: 26.55, NNZs: 38231, Bias: -0.867631, T: 8806, Avg. loss: 0.109415
Total training time: 0.16 seconds.
Total training time: 0.02 seconds.
-- Epoch 2
Convergence after 10 epochs took 0.16 seconds
Norm: 35.47, NNZs: 35434, Bias: -1.055381, T: 87970, Avg. loss: 0.002743
Total training time: 0.07 seconds.
Convergence after 10 epochs took 0.07 seconds
-- Epoch 2
-- Epoch 6
Norm: 31.58, NNZs: 38245, Bias: -0.946064, T: 79173, Avg. loss: 0.003455
Total training time: 0.07 seconds.
-- Epoch 10
Norm: 32.68, NNZs: 33960, Bias: -0.956772, T: 17612, Avg. loss: 0.038225
Total training time: 0.03 seconds.
Norm: 23.98, NNZs: 38208, Bias: -0.897346, T: 8806, Avg. loss: 0.089191
Total training time: 0.01 seconds.
Norm: 20.67, NNZs: 37082, Bias: -0.912860, T: 8806, Avg. loss: 0.101516
Total training time: 0.01 seconds.
Norm: 36.08, NNZs: 35292, Bias: -1.009365, T: 87970, Avg. loss: 0.003181
Total training time: 0.07 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 2
Norm: 37.72, NNZs: 45912, Bias: -1.025010, T: 79254, Avg. loss: 0.008192
Total training time: 0.15 seconds.
-- Epoch 11
Norm: 32.91, NNZs: 44884, Bias: -0.929794, T: 26418, Avg. loss: 0.010521
Total training time: 0.05 seconds.
-- Epoch 10
-- Epoch 4
Norm: 39.47, NNZs: 49182, Bias: -1.019105, T: 96866, Avg. loss: 0.003715
Total training time: 0.19 seconds.
Convergence after 11 epochs took 0.19 seconds
Norm: 31.63, NNZs: 49746, Bias: -0.964675, T: 17612, Avg. loss: 0.032319
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 39.36, NNZs: 46858, Bias: -0.978329, T: 52836, Avg. loss: 0.004841
Norm: 24.81, NNZs: 34422, Bias: -1.001101, T: 8806, Avg. loss: 0.107445
Total training time: 0.01 seconds.
Total training time: 0.08 seconds.
Norm: 32.31, NNZs: 39684, Bias: -0.857355, T: 17612, Avg. loss: 0.033642
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 7
-- Epoch 3
Norm: 31.59, NNZs: 38245, Bias: -0.945150, T: 87970, Avg. loss: 0.003421
Total training time: 0.08 seconds.
Norm: 28.84, NNZs: 39499, Bias: -0.949106, T: 17612, Avg. loss: 0.027079
Total training time: 0.01 seconds.
Convergence after 10 epochs took 0.08 seconds
-- Epoch 3
Norm: 35.35, NNZs: 34250, Bias: -0.995078, T: 26418, Avg. loss: 0.014634
Total training time: 0.03 seconds.
Norm: 37.84, NNZs: 45912, Bias: -1.025916, T: 88060, Avg. loss: 0.007896
Total training time: 0.15 seconds.
Norm: 30.37, NNZs: 40097, Bias: -0.946473, T: 17612, Avg. loss: 0.047478
Total training time: 0.01 seconds.
-- Epoch 4
-- Epoch 11
Norm: 33.39, NNZs: 44897, Bias: -0.922954, T: 35224, Avg. loss: 0.006132
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 5
Norm: 36.08, NNZs: 35292, Bias: -1.010452, T: 96767, Avg. loss: 0.003152
Total training time: 0.08 seconds.
Convergence after 11 epochs took 0.08 seconds
Norm: 33.67, NNZs: 49873, Bias: -0.962406, T: 26418, Avg. loss: 0.011179
Total training time: 0.02 seconds.
-- Epoch 4
Norm: 39.46, NNZs: 46865, Bias: -0.986083, T: 61642, Avg. loss: 0.004610
Total training time: 0.09 seconds.
Norm: 31.70, NNZs: 36133, Bias: -1.019629, T: 17612, Avg. loss: 0.033468
Total training time: 0.02 seconds.
Norm: 34.06, NNZs: 39823, Bias: -0.891099, T: 26418, Avg. loss: 0.013828
Total training time: 0.03 seconds.
-- Epoch 8
-- Epoch 3
-- Epoch 4
Norm: 30.52, NNZs: 39783, Bias: -0.937418, T: 26418, Avg. loss: 0.011735
Total training time: 0.02 seconds.
Norm: 36.42, NNZs: 34340, Bias: -1.005191, T: 35224, Avg. loss: 0.005976
Total training time: 0.04 seconds.
Norm: 37.90, NNZs: 45915, Bias: -1.027974, T: 96866, Avg. loss: 0.007700
Total training time: 0.16 seconds.
-- Epoch 4
-- Epoch 5
Convergence after 11 epochs took 0.16 seconds
Norm: 33.73, NNZs: 40516, Bias: -0.945755, T: 26418, Avg. loss: 0.017532
Total training time: 0.02 seconds.
Norm: 33.57, NNZs: 44902, Bias: -0.946703, T: 44030, Avg. loss: 0.005119
Total training time: 0.06 seconds.
-- Epoch 4
-- Epoch 6
Norm: 34.69, NNZs: 39855, Bias: -0.902748, T: 35224, Avg. loss: 0.008383
Total training time: 0.03 seconds.
-- Epoch 5
Norm: 33.76, NNZs: 36397, Bias: -1.059930, T: 26418, Avg. loss: 0.010864
Total training time: 0.02 seconds.
-- Epoch 4
Norm: 34.30, NNZs: 49968, Bias: -0.991013, T: 35224, Avg. loss: 0.005209
Total training time: 0.02 seconds.
-- Epoch 5
Norm: 39.54, NNZs: 46865, Bias: -0.984019, T: 70448, Avg. loss: 0.004325
Total training time: 0.09 seconds.
-- Epoch 9
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.3s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished
Norm: 31.11, NNZs: 39819, Bias: -0.951104, T: 35224, Avg. loss: 0.007181
Total training time: 0.02 seconds.
Norm: 36.91, NNZs: 34363, Bias: -1.023314, T: 44030, Avg. loss: 0.003473
Total training time: 0.04 seconds.
-- Epoch 5
-- Epoch 6
Norm: 34.96, NNZs: 39859, Bias: -0.900178, T: 44030, Avg. loss: 0.006387
Total training time: 0.03 seconds.
Norm: 35.02, NNZs: 40593, Bias: -1.012813, T: 35224, Avg. loss: 0.008602
Total training time: 0.02 seconds.
Norm: 33.74, NNZs: 44931, Bias: -0.945187, T: 52836, Avg. loss: 0.004533
Total training time: 0.06 seconds.
Norm: 34.46, NNZs: 36412, Bias: -1.065277, T: 35224, Avg. loss: 0.004675
-- Epoch 6
Total training time: 0.02 seconds.
-- Epoch 5
-- Epoch 7
-- Epoch 5
Norm: 34.49, NNZs: 49976, Bias: -0.989808, T: 44030, Avg. loss: 0.002944
Total training time: 0.03 seconds.
-- Epoch 6
Norm: 39.56, NNZs: 46872, Bias: -0.986746, T: 79254, Avg. loss: 0.004176
Total training time: 0.09 seconds.
-- Epoch 10
Norm: 35.08, NNZs: 39874, Bias: -0.918400, T: 52836, Avg. loss: 0.005923
Total training time: 0.04 seconds.
Norm: 37.12, NNZs: 34378, Bias: -1.028591, T: 52836, Avg. loss: 0.002075
Total training time: 0.05 seconds.
Norm: 31.33, NNZs: 39833, Bias: -0.949590, T: 44030, Avg. loss: 0.005487
Total training time: 0.03 seconds.
Norm: 34.69, NNZs: 36422, Bias: -1.071949, T: 44030, Avg. loss: 0.002821
Total training time: 0.03 seconds.
-- Epoch 7
-- Epoch 7
-- Epoch 6
-- Epoch 6
Norm: 35.62, NNZs: 40610, Bias: -1.012945, T: 44030, Avg. loss: 0.004771
Total training time: 0.03 seconds.
Norm: 33.87, NNZs: 44941, Bias: -0.960069, T: 61642, Avg. loss: 0.004422
Total training time: 0.07 seconds.
-- Epoch 6
-- Epoch 8
Norm: 34.55, NNZs: 49980, Bias: -0.993704, T: 52836, Avg. loss: 0.002436
Total training time: 0.03 seconds.
Norm: 35.36, NNZs: 39882, Bias: -0.904432, T: 61642, Avg. loss: 0.005983
Total training time: 0.04 seconds.
Norm: 39.62, NNZs: 46872, Bias: -0.983183, T: 88060, Avg. loss: 0.004183
-- Epoch 7
Total training time: 0.10 seconds.
Norm: 34.78, NNZs: 36427, Bias: -1.072771, T: 52836, Avg. loss: 0.002196
Total training time: 0.03 seconds.
-- Epoch 8
-- Epoch 7
-- Epoch 11
Norm: 37.25, NNZs: 34389, Bias: -1.031028, T: 61642, Avg. loss: 0.001728
Total training time: 0.05 seconds.
Norm: 31.41, NNZs: 39833, Bias: -0.950196, T: 52836, Avg. loss: 0.004980
Total training time: 0.03 seconds.
-- Epoch 8
-- Epoch 7
Norm: 35.50, NNZs: 39891, Bias: -0.922147, T: 70448, Avg. loss: 0.005582
Total training time: 0.04 seconds.
Norm: 34.84, NNZs: 36427, Bias: -1.074394, T: 61642, Avg. loss: 0.002051
Total training time: 0.03 seconds.
Norm: 35.86, NNZs: 40616, Bias: -1.026227, T: 52836, Avg. loss: 0.003400
Total training time: 0.03 seconds.
-- Epoch 9
-- Epoch 8
Norm: 33.97, NNZs: 44946, Bias: -0.963309, T: 70448, Avg. loss: 0.004062
Total training time: 0.07 seconds.
-- Epoch 7
-- Epoch 9
Norm: 34.57, NNZs: 49980, Bias: -0.993402, T: 61642, Avg. loss: 0.002234
Total training time: 0.04 seconds.
Norm: 39.63, NNZs: 46872, Bias: -0.983963, T: 96866, Avg. loss: 0.004071
Total training time: 0.10 seconds.
-- Epoch 8
Convergence after 11 epochs took 0.10 seconds
Norm: 31.43, NNZs: 39833, Bias: -0.953132, T: 61642, Avg. loss: 0.004757
Total training time: 0.03 seconds.
Norm: 37.34, NNZs: 34389, Bias: -1.035810, T: 70448, Avg. loss: 0.001589
Total training time: 0.06 seconds.
-- Epoch 8
-- Epoch 9
Norm: 35.71, NNZs: 39894, Bias: -0.910551, T: 79254, Avg. loss: 0.005374
Total training time: 0.04 seconds.
Norm: 34.85, NNZs: 36427, Bias: -1.074567, T: 70448, Avg. loss: 0.001911
Total training time: 0.04 seconds.
-- Epoch 10
-- Epoch 9
Norm: 36.01, NNZs: 40620, Bias: -1.025798, T: 61642, Avg. loss: 0.002838
Total training time: 0.04 seconds.
-- Epoch 8
Norm: 34.06, NNZs: 44946, Bias: -0.964391, T: 79254, Avg. loss: 0.003813
Total training time: 0.08 seconds.
-- Epoch 10
Norm: 31.45, NNZs: 39833, Bias: -0.952464, T: 70448, Avg. loss: 0.004686
Total training time: 0.04 seconds.
Norm: 34.58, NNZs: 49980, Bias: -0.993762, T: 70448, Avg. loss: 0.002174
Total training time: 0.04 seconds.
-- Epoch 9
-- Epoch 9
Norm: 35.84, NNZs: 39902, Bias: -0.918057, T: 88060, Avg. loss: 0.005157
Total training time: 0.05 seconds.
Norm: 34.87, NNZs: 36427, Bias: -1.075340, T: 79254, Avg. loss: 0.001886
Total training time: 0.04 seconds.
Convergence after 10 epochs took 0.05 seconds
-- Epoch 10
Norm: 37.40, NNZs: 34401, Bias: -1.038325, T: 79254, Avg. loss: 0.001412
Total training time: 0.06 seconds.
-- Epoch 10
Norm: 31.45, NNZs: 39833, Bias: -0.953636, T: 79254, Avg. loss: 0.004665
Total training time: 0.04 seconds.
-- Epoch 10
Norm: 34.87, NNZs: 36427, Bias: -1.074849, T: 88060, Avg. loss: 0.001854
Total training time: 0.04 seconds.
Norm: 36.08, NNZs: 40620, Bias: -1.023756, T: 70448, Avg. loss: 0.002500
Total training time: 0.04 seconds.
Convergence after 10 epochs took 0.04 seconds
-- Epoch 9
Norm: 34.15, NNZs: 44948, Bias: -0.963908, T: 88060, Avg. loss: 0.003705
Total training time: 0.08 seconds.
Norm: 34.58, NNZs: 49980, Bias: -0.993746, T: 79254, Avg. loss: 0.002155
Total training time: 0.05 seconds.
Convergence after 10 epochs took 0.08 seconds
-- Epoch 10
Norm: 37.44, NNZs: 34401, Bias: -1.039634, T: 88060, Avg. loss: 0.001303
Total training time: 0.06 seconds.
-- Epoch 11
Norm: 31.46, NNZs: 39833, Bias: -0.952938, T: 88060, Avg. loss: 0.004646
Total training time: 0.04 seconds.
Convergence after 10 epochs took 0.04 seconds
Norm: 36.12, NNZs: 40622, Bias: -1.024524, T: 79254, Avg. loss: 0.002410
Total training time: 0.04 seconds.
-- Epoch 10
Norm: 34.58, NNZs: 49980, Bias: -0.993767, T: 88060, Avg. loss: 0.002148
Total training time: 0.05 seconds.
Convergence after 10 epochs took 0.05 seconds
Norm: 37.47, NNZs: 34402, Bias: -1.040718, T: 96866, Avg. loss: 0.001269
Total training time: 0.07 seconds.
Convergence after 11 epochs took 0.07 seconds
Norm: 36.14, NNZs: 40622, Bias: -1.025217, T: 88060, Avg. loss: 0.002346
Total training time: 0.05 seconds.
-- Epoch 11
Norm: 36.15, NNZs: 40622, Bias: -1.026398, T: 96866, Avg. loss: 0.002299
Total training time: 0.05 seconds.
Convergence after 11 epochs took 0.05 seconds
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.3s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished
	accuracy: 5-fold cross validation: [0.76093681 0.7410517  0.74326116 0.76226248 0.73784262]
	test accuracy: 5-fold cross validation accuracy: 0.75 (+/- 0.02)
dimensionality: 101322
density: 0.433389



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.6848114710568242
	accuracy score (normalize=False):  5158

compute the precision
	precision score (average=macro):  0.682649826309239
	precision score (average=micro):  0.6848114710568242
	precision score (average=weighted):  0.6915436304843668
	precision score (average=None):  [0.51       0.65165877 0.62972973 0.61691542 0.71428571 0.8034188
 0.74754902 0.76       0.49044586 0.86413043 0.8718593  0.82798834
 0.63920455 0.77237852 0.74045802 0.65795207 0.59390863 0.83185841
 0.52255639 0.40669856]
	precision score (average=None, zero_division=1):  [0.51       0.65165877 0.62972973 0.61691542 0.71428571 0.8034188
 0.74754902 0.76       0.49044586 0.86413043 0.8718593  0.82798834
 0.63920455 0.77237852 0.74045802 0.65795207 0.59390863 0.83185841
 0.52255639 0.40669856]

compute the precision
	recall score (average=macro):  0.6737857878221761
	recall score (average=micro):  0.6848114710568242
	recall score (average=weighted):  0.6848114710568242
	recall score (average=None):  [0.47962382 0.70694087 0.59137056 0.63265306 0.67532468 0.71392405
 0.78205128 0.71969697 0.77386935 0.80100756 0.86967419 0.71717172
 0.57251908 0.76262626 0.73857868 0.75879397 0.64285714 0.75
 0.4483871  0.33864542]
	recall score (average=None, zero_division=1):  [0.47962382 0.70694087 0.59137056 0.63265306 0.67532468 0.71392405
 0.78205128 0.71969697 0.77386935 0.80100756 0.86967419 0.71717172
 0.57251908 0.76262626 0.73857868 0.75879397 0.64285714 0.75
 0.4483871  0.33864542]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.6753259253449568
	f1 score (average=micro):  0.6848114710568242
	f1 score (average=weighted):  0.6852372571086598
	f1 score (average=None):  [0.49434572 0.67817509 0.60994764 0.62468514 0.69425901 0.75603217
 0.76441103 0.73929961 0.60038986 0.83137255 0.87076537 0.76860622
 0.60402685 0.76747141 0.73951715 0.70478413 0.61741425 0.78881119
 0.48263889 0.36956522]

compute the F-beta score
	f beta score (average=macro):  0.6791279599914145
	f beta score (average=micro):  0.6848114710568242
	f beta score (average=weighted):  0.6884264006768367
	f beta score (average=None):  [0.5036208  0.66201252 0.62166489 0.62       0.70613797 0.78376876
 0.75420376 0.75158228 0.52920962 0.85072231 0.8714214  0.80316742
 0.62465297 0.77040816 0.74008138 0.67591764 0.60309278 0.81408776
 0.50582242 0.39098436]

compute the average Hamming loss
	hamming loss:  0.3151885289431758

jaccard similarity coefficient score
	jaccard score (average=macro):  0.5221692851096817
	jaccard score (average=None):  [0.32832618 0.5130597  0.43879473 0.45421245 0.53169734 0.60775862
 0.61866126 0.58641975 0.42896936 0.7114094  0.77111111 0.62417582
 0.43269231 0.62268041 0.58669355 0.54414414 0.44656489 0.65127021
 0.3180778  0.22666667]


================================================================================
Classifier.PERCEPTRON
________________________________________________________________________________
Training: 
Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,
           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=-1,
           penalty=None, random_state=0, shuffle=True, tol=0.001,
           validation_fraction=0.1, verbose=True, warm_start=False)
-- Epoch 1
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
Norm: 20.62, NNZs: 7918, Bias: -0.200000, T: 11314, Avg. loss: 0.008704
Total training time: 0.00 seconds.
-- Epoch 2
Norm: 23.71, NNZs: 9048, Bias: -0.340000, T: 22628, Avg. loss: 0.003264
Total training time: 0.01 seconds.
-- Epoch 1
Norm: 19.88, NNZs: 6980, Bias: -0.190000, T: 11314, Avg. loss: 0.005904
Total training time: 0.00 seconds.
-- Epoch 1
Norm: 19.60, NNZs: 6427, Bias: -0.170000, T: 11314, Avg. loss: 0.004413
Total training time: 0.00 seconds.
-- Epoch 2
-- Epoch 2
Norm: 22.19, NNZs: 7835, Bias: -0.180000, T: 22628, Avg. loss: 0.001949
Total training time: 0.02 seconds.
-- Epoch 3
-- Epoch 1
Norm: 18.85, NNZs: 7676, Bias: -0.220000, T: 11314, Avg. loss: 0.005340
Total training time: 0.00 seconds.
-- Epoch 2
Norm: 21.76, NNZs: 7233, Bias: -0.200000, T: 22628, Avg. loss: 0.001191
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 22.45, NNZs: 7439, Bias: -0.200000, T: 33942, Avg. loss: 0.000578
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 24.99, NNZs: 9465, Bias: -0.340000, T: 33942, Avg. loss: 0.001355
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 19.30, NNZs: 8905, Bias: -0.240000, T: 11314, Avg. loss: 0.005700
Total training time: 0.06 seconds.
-- Epoch 2
Norm: 21.52, NNZs: 9850, Bias: -0.260000, T: 22628, Avg. loss: 0.001481
Total training time: 0.07 seconds.
-- Epoch 3
-- Epoch 1
Norm: 23.23, NNZs: 8415, Bias: -0.130000, T: 33942, Avg. loss: 0.001079
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 22.43, NNZs: 10417, Bias: -0.290000, T: 33942, Avg. loss: 0.000622
Total training time: 0.08 seconds.
-- Epoch 1
-- Epoch 4
Norm: 19.29, NNZs: 18626, Bias: -0.150000, T: 11314, Avg. loss: 0.006543
Norm: 25.58, NNZs: 9624, Bias: -0.280000, T: 45256, Avg. loss: 0.000741
Total training time: 0.08 seconds.
Total training time: 0.01 seconds.
-- Epoch 4
-- Epoch 2
-- Epoch 1
-- Epoch 5
Norm: 23.01, NNZs: 10561, Bias: -0.220000, T: 45256, Avg. loss: 0.000476
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 24.13, NNZs: 8611, Bias: -0.120000, T: 45256, Avg. loss: 0.000777
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 19.19, NNZs: 6532, Bias: -0.210000, T: 11314, Avg. loss: 0.005987
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 26.02, NNZs: 9811, Bias: -0.300000, T: 56570, Avg. loss: 0.000536
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 20.97, NNZs: 8776, Bias: -0.320000, T: 22628, Avg. loss: 0.001648
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 23.32, NNZs: 10723, Bias: -0.250000, T: 56570, Avg. loss: 0.000332
Total training time: 0.10 seconds.
Norm: 24.89, NNZs: 8876, Bias: -0.060000, T: 56570, Avg. loss: 0.000635
Total training time: 0.09 seconds.
-- Epoch 6
-- Epoch 6
Norm: 26.23, NNZs: 9841, Bias: -0.260000, T: 67884, Avg. loss: 0.000521
Total training time: 0.10 seconds.
-- Epoch 7
-- Epoch 1
Norm: 21.33, NNZs: 7481, Bias: -0.220000, T: 22628, Avg. loss: 0.001912
Total training time: 0.04 seconds.
-- Epoch 1
-- Epoch 3
Norm: 25.74, NNZs: 9114, Bias: -0.030000, T: 67884, Avg. loss: 0.000402
Norm: 23.60, NNZs: 10830, Bias: -0.200000, T: 67884, Avg. loss: 0.000282
Total training time: 0.12 seconds.
-- Epoch 7
Norm: 22.74, NNZs: 7517, Bias: -0.150000, T: 45256, Avg. loss: 0.000479
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 23.83, NNZs: 10910, Bias: -0.200000, T: 79198, Avg. loss: 0.000246
Total training time: 0.13 seconds.
Norm: 20.63, NNZs: 8513, Bias: -0.240000, T: 11314, Avg. loss: 0.006834
Total training time: 0.04 seconds.
Norm: 21.91, NNZs: 8989, Bias: -0.270000, T: 33942, Avg. loss: 0.000908
Total training time: 0.09 seconds.
-- Epoch 2
Norm: 21.78, NNZs: 23952, Bias: -0.260000, T: 22628, Avg. loss: 0.002653
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 17.97, NNZs: 5380, Bias: -0.170000, T: 11314, Avg. loss: 0.003644
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 2
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 22.86, NNZs: 24950, Bias: -0.170000, T: 33942, Avg. loss: 0.001145
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 23.18, NNZs: 9685, Bias: -0.280000, T: 22628, Avg. loss: 0.002558
Convergence after 7 epochs took 0.13 seconds
Norm: 26.43, NNZs: 9908, Bias: -0.230000, T: 79198, Avg. loss: 0.000358
Total training time: 0.13 seconds.
-- Epoch 1
Norm: 22.38, NNZs: 7803, Bias: -0.190000, T: 33942, Avg. loss: 0.000855
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 22.30, NNZs: 9147, Bias: -0.260000, T: 45256, Avg. loss: 0.000814
Total training time: 0.10 seconds.
Norm: 26.34, NNZs: 9509, Bias: -0.020000, T: 79198, Avg. loss: 0.000250
Total training time: 0.12 seconds.
-- Epoch 5
Convergence after 7 epochs took 0.12 seconds
Total training time: 0.05 seconds.
Norm: 23.14, NNZs: 7634, Bias: -0.130000, T: 56570, Avg. loss: 0.000351
Total training time: 0.12 seconds.
-- Epoch 1
Norm: 22.61, NNZs: 9258, Bias: -0.250000, T: 56570, Avg. loss: 0.000706
Norm: 22.95, NNZs: 7988, Bias: -0.150000, T: 45256, Avg. loss: 0.000622
Total training time: 0.07 seconds.
-- Epoch 8
Norm: 21.33, NNZs: 8590, Bias: -0.210000, T: 11314, Avg. loss: 0.006747
Norm: 23.71, NNZs: 25149, Bias: -0.140000, T: 45256, Avg. loss: 0.000924
Total training time: 0.07 seconds.
-- Epoch 5
Total training time: 0.11 seconds.
-- Epoch 1
Norm: 17.42, NNZs: 9812, Bias: -0.180000, T: 11314, Avg. loss: 0.003640
-- Epoch 5
Norm: 26.65, NNZs: 9964, Bias: -0.200000, T: 90512, Avg. loss: 0.000394
Total training time: 0.15 seconds.
Convergence after 8 epochs took 0.15 seconds
-- Epoch 6
Total training time: 0.05 seconds.
Norm: 23.64, NNZs: 8226, Bias: -0.140000, T: 56570, Avg. loss: 0.000444
Total training time: 0.10 seconds.
-- Epoch 6
-- Epoch 6
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 1
Norm: 23.46, NNZs: 7767, Bias: -0.120000, T: 67884, Avg. loss: 0.000426
Norm: 22.84, NNZs: 9337, Bias: -0.230000, T: 67884, Avg. loss: 0.000391
Norm: 17.99, NNZs: 8447, Bias: -0.200000, T: 11314, Avg. loss: 0.004400
Total training time: 0.06 seconds.
Norm: 16.12, NNZs: 5554, Bias: -0.170000, T: 11314, Avg. loss: 0.002815
Norm: 24.57, NNZs: 25311, Bias: -0.100000, T: 56570, Avg. loss: 0.000718
Total training time: 0.16 seconds.
-- Epoch 2
-- Epoch 7
-- Epoch 1
Total training time: 0.02 seconds.
-- Epoch 2
Total training time: 0.14 seconds.
Norm: 18.96, NNZs: 10453, Bias: -0.280000, T: 22628, Avg. loss: 0.001115
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 23.85, NNZs: 7864, Bias: -0.070000, T: 79198, Avg. loss: 0.000388
Total training time: 0.17 seconds.
Norm: 17.91, NNZs: 6153, Bias: -0.160000, T: 22628, Avg. loss: 0.000944
Total training time: 0.03 seconds.
Convergence after 7 epochs took 0.17 seconds
-- Epoch 3
Norm: 19.81, NNZs: 6026, Bias: -0.150000, T: 22628, Avg. loss: 0.001120
Total training time: 0.09 seconds.
-- Epoch 3
-- Epoch 7
-- Epoch 1
-- Epoch 3
Norm: 19.90, NNZs: 8955, Bias: -0.290000, T: 22628, Avg. loss: 0.001069
Total training time: 0.09 seconds.
Norm: 20.26, NNZs: 11776, Bias: -0.240000, T: 11314, Avg. loss: 0.006116
Total training time: 0.01 seconds.
-- Epoch 3
Norm: 20.77, NNZs: 6259, Bias: -0.150000, T: 33942, Avg. loss: 0.000558
Total training time: 0.11 seconds.
-- Epoch 2
-- Epoch 4
Norm: 18.19, NNZs: 10573, Bias: -0.200000, T: 11314, Avg. loss: 0.003505
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 24.08, NNZs: 10232, Bias: -0.260000, T: 33942, Avg. loss: 0.001313
Total training time: 0.12 seconds.
Norm: 20.57, NNZs: 9099, Bias: -0.270000, T: 33942, Avg. loss: 0.000234
Total training time: 0.09 seconds.
-- Epoch 4
-- Epoch 4
Norm: 19.10, NNZs: 7931, Bias: -0.180000, T: 11314, Avg. loss: 0.004092
Total training time: 0.03 seconds.
Norm: 20.20, NNZs: 11540, Bias: -0.220000, T: 22628, Avg. loss: 0.001118
Total training time: 0.06 seconds.
-- Epoch 2
-- Epoch 3
Total training time: 0.13 seconds.
-- Epoch 6
Norm: 21.18, NNZs: 8897, Bias: -0.250000, T: 22628, Avg. loss: 0.001225
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 19.77, NNZs: 10701, Bias: -0.230000, T: 33942, Avg. loss: 0.000517
Total training time: 0.11 seconds.
Norm: 24.39, NNZs: 8423, Bias: -0.100000, T: 67884, Avg. loss: 0.000406
Total training time: 0.14 seconds.
-- Epoch 4
-- Epoch 7
Norm: 25.34, NNZs: 25446, Bias: -0.090000, T: 67884, Avg. loss: 0.000607
Total training time: 0.14 seconds.
-- Epoch 7
Norm: 22.16, NNZs: 9175, Bias: -0.210000, T: 33942, Avg. loss: 0.000699
Norm: 23.66, NNZs: 9647, Bias: -0.250000, T: 22628, Avg. loss: 0.002051
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 24.84, NNZs: 8510, Bias: -0.050000, T: 79198, Avg. loss: 0.000466
Total training time: 0.15 seconds.
-- Epoch 8
Norm: 20.22, NNZs: 10848, Bias: -0.220000, T: 45256, Avg. loss: 0.000481
Total training time: 0.11 seconds.
-- Epoch 5
Norm: 22.42, NNZs: 13061, Bias: -0.330000, T: 22628, Avg. loss: 0.001592
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 25.32, NNZs: 8626, Bias: -0.050000, T: 90512, Avg. loss: 0.000196
Norm: 24.63, NNZs: 10374, Bias: -0.210000, T: 45256, Avg. loss: 0.000931
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 20.64, NNZs: 11107, Bias: -0.200000, T: 56570, Avg. loss: 0.000317
Total training time: 0.12 seconds.
Total training time: 0.04 seconds.
Norm: 20.85, NNZs: 11878, Bias: -0.190000, T: 33942, Avg. loss: 0.000524
Norm: 18.80, NNZs: 6387, Bias: -0.180000, T: 33942, Avg. loss: 0.000581
Total training time: 0.08 seconds.
Total training time: 0.16 seconds.
Total training time: 0.06 seconds.
-- Epoch 4
-- Epoch 4
Norm: 23.10, NNZs: 9361, Bias: -0.220000, T: 79198, Avg. loss: 0.000527
Norm: 21.52, NNZs: 6432, Bias: -0.060000, T: 45256, Avg. loss: 0.000389
Norm: 24.41, NNZs: 9960, Bias: -0.200000, T: 33942, Avg. loss: 0.000818
Total training time: 0.09 seconds.
Total training time: 0.19 seconds.
-- Epoch 4
Total training time: 0.13 seconds.
Convergence after 7 epochs took 0.19 seconds
-- Epoch 4
-- Epoch 6
-- Epoch 5
Norm: 23.25, NNZs: 13295, Bias: -0.330000, T: 33942, Avg. loss: 0.000690
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 24.85, NNZs: 10108, Bias: -0.210000, T: 45256, Avg. loss: 0.000635
Convergence after 8 epochs took 0.17 seconds
Total training time: 0.10 seconds.
Norm: 21.07, NNZs: 9218, Bias: -0.290000, T: 45256, Avg. loss: 0.000237
Norm: 19.32, NNZs: 6518, Bias: -0.150000, T: 45256, Avg. loss: 0.000468
Total training time: 0.07 seconds.
-- Epoch 1
Total training time: 0.12 seconds.
-- Epoch 5
-- Epoch 5
Norm: 22.68, NNZs: 9697, Bias: -0.200000, T: 45256, Avg. loss: 0.000466
Norm: 25.86, NNZs: 25544, Bias: -0.050000, T: 79198, Avg. loss: 0.000552
Total training time: 0.16 seconds.
Total training time: 0.06 seconds.
-- Epoch 5
Norm: 23.67, NNZs: 13423, Bias: -0.300000, T: 45256, Avg. loss: 0.000379
Norm: 20.96, NNZs: 11191, Bias: -0.170000, T: 67884, Avg. loss: 0.000263
Total training time: 0.14 seconds.
Norm: 25.28, NNZs: 10239, Bias: -0.130000, T: 56570, Avg. loss: 0.000566
-- Epoch 7
Norm: 19.84, NNZs: 6602, Bias: -0.100000, T: 56570, Avg. loss: 0.000289
Total training time: 0.08 seconds.
Norm: 24.99, NNZs: 10465, Bias: -0.180000, T: 56570, Avg. loss: 0.000819
-- Epoch 8
Total training time: 0.16 seconds.
-- Epoch 6
-- Epoch 6
Norm: 21.40, NNZs: 12330, Bias: -0.170000, T: 45256, Avg. loss: 0.000448
Total training time: 0.10 seconds.
-- Epoch 5
Total training time: 0.11 seconds.
-- Epoch 6
Norm: 21.18, NNZs: 11263, Bias: -0.150000, T: 79198, Avg. loss: 0.000215
Total training time: 0.15 seconds.
Convergence after 7 epochs took 0.15 seconds
Norm: 25.57, NNZs: 11488, Bias: -0.170000, T: 67884, Avg. loss: 0.000664
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 21.77, NNZs: 12484, Bias: -0.140000, T: 56570, Avg. loss: 0.000368
Total training time: 0.10 seconds.
-- Epoch 6
Norm: 19.41, NNZs: 12186, Bias: -0.220000, T: 11314, Avg. loss: 0.004918
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 1
Norm: 22.95, NNZs: 9786, Bias: -0.150000, T: 56570, Avg. loss: 0.000311
Norm: 25.78, NNZs: 10466, Bias: -0.140000, T: 67884, Avg. loss: 0.000477
Total training time: 0.12 seconds.
-- Epoch 7
Norm: 22.27, NNZs: 12633, Bias: -0.140000, T: 67884, Avg. loss: 0.000370
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 20.24, NNZs: 6682, Bias: -0.060000, T: 67884, Avg. loss: 0.000216
Total training time: 0.10 seconds.
Norm: 22.45, NNZs: 12671, Bias: -0.090000, T: 79198, Avg. loss: 0.000281
Total training time: 0.11 seconds.
Norm: 22.22, NNZs: 6683, Bias: -0.030000, T: 56570, Avg. loss: 0.000238
Total training time: 0.16 seconds.
-- Epoch 6
Norm: 26.52, NNZs: 25680, Bias: -0.030000, T: 90512, Avg. loss: 0.000324
Total training time: 0.19 seconds.
Convergence after 8 epochs took 0.19 seconds
-- Epoch 7
Norm: 21.28, NNZs: 17413, Bias: -0.260000, T: 22628, Avg. loss: 0.001119
Total training time: 0.07 seconds.
-- Epoch 1
-- Epoch 7
-- Epoch 5
Norm: 26.09, NNZs: 10551, Bias: -0.100000, T: 79198, Avg. loss: 0.000459
Total training time: 0.14 seconds.
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 16.86, NNZs: 7416, Bias: -0.160000, T: 11314, Avg. loss: 0.003266
Total training time: 0.02 seconds.
Convergence after 7 epochs took 0.13 seconds
Norm: 25.92, NNZs: 11552, Bias: -0.150000, T: 79198, Avg. loss: 0.000560
-- Epoch 8
Norm: 24.08, NNZs: 13566, Bias: -0.310000, T: 56570, Avg. loss: 0.000372
Total training time: 0.08 seconds.
-- Epoch 1
Norm: 19.48, NNZs: 10373, Bias: -0.270000, T: 11314, Avg. loss: 0.006088
Total training time: 0.02 seconds.
-- Epoch 6
-- Epoch 2
Norm: 21.79, NNZs: 17543, Bias: -0.220000, T: 33942, Avg. loss: 0.000464
Total training time: 0.04 seconds.
Total training time: 0.20 seconds.
-- Epoch 8
-- Epoch 4
Norm: 26.54, NNZs: 11573, Bias: -0.080000, T: 90512, Avg. loss: 0.000349
Total training time: 0.15 seconds.
Convergence after 8 epochs took 0.15 seconds
Norm: 21.35, NNZs: 9254, Bias: -0.290000, T: 56570, Avg. loss: 0.000157
Total training time: 0.11 seconds.
-- Epoch 6
Norm: 20.79, NNZs: 6830, Bias: -0.080000, T: 79198, Avg. loss: 0.000159
Total training time: 0.13 seconds.
Convergence after 7 epochs took 0.13 seconds
Norm: 26.25, NNZs: 11641, Bias: -0.110000, T: 90512, Avg. loss: 0.000650
Total training time: 0.21 seconds.
Norm: 22.20, NNZs: 17680, Bias: -0.210000, T: 45256, Avg. loss: 0.000419
Total training time: 0.05 seconds.
Convergence after 8 epochs took 0.21 seconds
-- Epoch 2
-- Epoch 5
Norm: 24.35, NNZs: 13638, Bias: -0.270000, T: 67884, Avg. loss: 0.000290
Total training time: 0.10 seconds.
-- Epoch 7
Norm: 22.97, NNZs: 6939, Bias: -0.030000, T: 67884, Avg. loss: 0.000163
Total training time: 0.20 seconds.
-- Epoch 7
Norm: 19.21, NNZs: 11546, Bias: -0.220000, T: 11314, Avg. loss: 0.004775
Total training time: 0.02 seconds.
Norm: 22.64, NNZs: 18225, Bias: -0.190000, T: 56570, Avg. loss: 0.000341
Total training time: 0.06 seconds.
-- Epoch 2
Total training time: 0.18 seconds.
-- Epoch 6
-- Epoch 6
Norm: 22.90, NNZs: 18310, Bias: -0.150000, T: 67884, Avg. loss: 0.000261
Total training time: 0.07 seconds.
Norm: 24.51, NNZs: 13662, Bias: -0.240000, T: 79198, Avg. loss: 0.000245
Total training time: 0.11 seconds.
-- Epoch 7
Convergence after 7 epochs took 0.11 seconds
Norm: 23.39, NNZs: 9872, Bias: -0.120000, T: 67884, Avg. loss: 0.000256
Total training time: 0.13 seconds.
Norm: 21.70, NNZs: 12911, Bias: -0.330000, T: 22628, Avg. loss: 0.001566
Total training time: 0.03 seconds.
-- Epoch 7
-- Epoch 3
Norm: 23.22, NNZs: 18421, Bias: -0.140000, T: 79198, Avg. loss: 0.000215
Total training time: 0.07 seconds.
Convergence after 7 epochs took 0.07 seconds
Norm: 21.54, NNZs: 9340, Bias: -0.280000, T: 67884, Avg. loss: 0.000101
Total training time: 0.19 seconds.
-- Epoch 7
Norm: 21.77, NNZs: 11762, Bias: -0.260000, T: 22628, Avg. loss: 0.002045
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 23.81, NNZs: 9980, Bias: -0.100000, T: 79198, Avg. loss: 0.000213
Total training time: 0.13 seconds.
Norm: 22.41, NNZs: 13819, Bias: -0.310000, T: 33942, Avg. loss: 0.000524
Total training time: 0.03 seconds.
Norm: 18.76, NNZs: 8919, Bias: -0.220000, T: 22628, Avg. loss: 0.000984
Total training time: 0.05 seconds.
-- Epoch 4
-- Epoch 3
Norm: 21.66, NNZs: 9645, Bias: -0.280000, T: 79198, Avg. loss: 0.000094
Total training time: 0.20 seconds.
Norm: 23.33, NNZs: 7034, Bias: 0.000000, T: 79198, Avg. loss: 0.000084
Total training time: 0.21 seconds.
Convergence after 7 epochs took 0.20 seconds
Convergence after 7 epochs took 0.21 seconds
Norm: 22.74, NNZs: 13875, Bias: -0.270000, T: 45256, Avg. loss: 0.000386
Total training time: 0.04 seconds.
-- Epoch 5
Norm: 22.53, NNZs: 12051, Bias: -0.280000, T: 33942, Avg. loss: 0.000879
Total training time: 0.05 seconds.
Norm: 19.32, NNZs: 9480, Bias: -0.190000, T: 33942, Avg. loss: 0.000447
-- Epoch 4
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 23.11, NNZs: 13987, Bias: -0.200000, T: 56570, Avg. loss: 0.000377
Total training time: 0.04 seconds.
-- Epoch 6
Norm: 19.89, NNZs: 9636, Bias: -0.140000, T: 45256, Avg. loss: 0.000344
Total training time: 0.06 seconds.
-- Epoch 5
Norm: 23.08, NNZs: 12206, Bias: -0.220000, T: 45256, Avg. loss: 0.000550
Total training time: 0.06 seconds.
-- Epoch 5
Norm: 23.54, NNZs: 14064, Bias: -0.220000, T: 67884, Avg. loss: 0.000296
Total training time: 0.04 seconds.
Norm: 20.39, NNZs: 9841, Bias: -0.130000, T: 56570, Avg. loss: 0.000298
Total training time: 0.07 seconds.
-- Epoch 7
-- Epoch 6
Norm: 23.56, NNZs: 12332, Bias: -0.210000, T: 56570, Avg. loss: 0.000421
Total training time: 0.06 seconds.
-- Epoch 6
Norm: 23.78, NNZs: 14146, Bias: -0.160000, T: 79198, Avg. loss: 0.000266
Total training time: 0.05 seconds.
Norm: 20.79, NNZs: 9938, Bias: -0.090000, T: 67884, Avg. loss: 0.000246
Total training time: 0.07 seconds.
-- Epoch 8
-- Epoch 7
Convergence after 7 epochs took 0.15 seconds
Norm: 23.86, NNZs: 12444, Bias: -0.190000, T: 67884, Avg. loss: 0.000321
Total training time: 0.06 seconds.
-- Epoch 7
Norm: 21.19, NNZs: 10017, Bias: -0.060000, T: 79198, Avg. loss: 0.000188
Total training time: 0.07 seconds.
Norm: 24.12, NNZs: 14246, Bias: -0.170000, T: 90512, Avg. loss: 0.000255
Total training time: 0.05 seconds.
Convergence after 7 epochs took 0.07 seconds
Convergence after 8 epochs took 0.05 seconds
Norm: 24.23, NNZs: 12503, Bias: -0.170000, T: 79198, Avg. loss: 0.000314
Total training time: 0.07 seconds.
-- Epoch 8
Norm: 24.71, NNZs: 12605, Bias: -0.150000, T: 90512, Avg. loss: 0.000334
Total training time: 0.07 seconds.
Convergence after 8 epochs took 0.07 seconds
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.3s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished
train time: 0.413s
test time:  0.012s
accuracy:   0.634


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
-- Epoch 1
-- Epoch 1
[LibSVM]-- Epoch 1
[LibSVM]-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 17.94, NNZs: 18580, Bias: -0.220000, T: 9051, Avg. loss: 0.006691
-- Epoch 1
-- Epoch 1
Norm: 17.90, NNZs: 5699, Bias: -0.150000, T: 9051, Avg. loss: 0.004730
Total training time: 0.01 seconds.
Norm: 17.58, NNZs: 5646, Bias: -0.240000, T: 9051, Avg. loss: 0.005711
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 2
Norm: 15.37, NNZs: 4984, Bias: -0.140000, T: 9051, Avg. loss: 0.002923
Total training time: 0.01 seconds.
Norm: 16.91, NNZs: 6572, Bias: -0.180000, T: 9051, Avg. loss: 0.004943
Total training time: 0.01 seconds.
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 16.11, NNZs: 8639, Bias: -0.200000, T: 9051, Avg. loss: 0.003810
Total training time: 0.01 seconds.
Norm: 17.35, NNZs: 7211, Bias: -0.180000, T: 9051, Avg. loss: 0.004797
Norm: 18.30, NNZs: 19127, Bias: -0.180000, T: 9051, Avg. loss: 0.007009
-- Epoch 1
Norm: 18.10, NNZs: 7085, Bias: -0.160000, T: 9051, Avg. loss: 0.004751
Total training time: 0.02 seconds.
Total training time: 0.02 seconds.
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 16.01, NNZs: 8104, Bias: -0.220000, T: 9051, Avg. loss: 0.004050
Total training time: 0.02 seconds.
-- Epoch 1
-- Epoch 2
-- Epoch 2
-- Epoch 2
[LibSVM]-- Epoch 1
Norm: 17.57, NNZs: 5708, Bias: -0.240000, T: 9051, Avg. loss: 0.005742
Total training time: 0.02 seconds.
Norm: 18.45, NNZs: 6710, Bias: -0.190000, T: 9051, Avg. loss: 0.005951
Total training time: 0.01 seconds.
-- Epoch 1
Norm: 19.03, NNZs: 7503, Bias: -0.230000, T: 9051, Avg. loss: 0.008780
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 2
Norm: 18.67, NNZs: 7190, Bias: -0.210000, T: 9051, Avg. loss: 0.006630
Total training time: 0.02 seconds.
Norm: 14.96, NNZs: 4626, Bias: -0.160000, T: 9051, Avg. loss: 0.002953
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 2
Norm: 20.06, NNZs: 7681, Bias: -0.200000, T: 18102, Avg. loss: 0.001275
Total training time: 0.02 seconds.
Norm: 16.61, NNZs: 4820, Bias: -0.160000, T: 9051, Avg. loss: 0.003375
Total training time: 0.01 seconds.
-- Epoch 3
-- Epoch 2
Norm: 20.80, NNZs: 25876, Bias: -0.210000, T: 18102, Avg. loss: 0.002304
Norm: 17.51, NNZs: 9391, Bias: -0.270000, T: 18102, Avg. loss: 0.001211
Total training time: 0.02 seconds.
Total training time: 0.02 seconds.
Norm: 18.24, NNZs: 8761, Bias: -0.220000, T: 9051, Avg. loss: 0.005635
Total training time: 0.01 seconds.
Norm: 19.42, NNZs: 8001, Bias: -0.260000, T: 18102, Avg. loss: 0.001347
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 3
-- Epoch 2
-- Epoch 3
-- Epoch 2
-- Epoch 1
Norm: 20.27, NNZs: 7444, Bias: -0.190000, T: 18102, Avg. loss: 0.001929
Total training time: 0.01 seconds.
Norm: 21.45, NNZs: 8283, Bias: -0.330000, T: 18102, Avg. loss: 0.002539
Total training time: 0.01 seconds.
Norm: 19.55, NNZs: 6307, Bias: -0.260000, T: 18102, Avg. loss: 0.001464
Total training time: 0.02 seconds.
-- Epoch 3
-- Epoch 3
-- Epoch 3
Norm: 16.43, NNZs: 5431, Bias: -0.160000, T: 18102, Avg. loss: 0.000862
Total training time: 0.02 seconds.
Norm: 21.25, NNZs: 8259, Bias: -0.260000, T: 18102, Avg. loss: 0.002554
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 1
-- Epoch 2
-- Epoch 3
-- Epoch 1
-- Epoch 1
Norm: 16.87, NNZs: 6143, Bias: -0.200000, T: 9051, Avg. loss: 0.004584
Total training time: 0.01 seconds.
Norm: 20.78, NNZs: 7785, Bias: -0.170000, T: 27153, Avg. loss: 0.000608
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 16.82, NNZs: 6577, Bias: -0.180000, T: 9051, Avg. loss: 0.004944
-- Epoch 4
Norm: 21.72, NNZs: 26120, Bias: -0.180000, T: 27153, Avg. loss: 0.000871
Total training time: 0.03 seconds.
-- Epoch 1
-- Epoch 4
-- Epoch 1
-- Epoch 1
Norm: 17.09, NNZs: 5639, Bias: -0.140000, T: 27153, Avg. loss: 0.000451
Total training time: 0.03 seconds.
Norm: 22.23, NNZs: 8572, Bias: -0.300000, T: 27153, Avg. loss: 0.001076
Total training time: 0.02 seconds.
-- Epoch 4
-- Epoch 4
-- Epoch 1
-- Epoch 2
-- Epoch 1
-- Epoch 1
Norm: 21.17, NNZs: 7891, Bias: -0.170000, T: 36204, Avg. loss: 0.000331
Total training time: 0.04 seconds.
Total training time: 0.04 seconds.
-- Epoch 5
-- Epoch 1
Norm: 18.20, NNZs: 5429, Bias: -0.160000, T: 18102, Avg. loss: 0.001305
Total training time: 0.02 seconds.
Norm: 22.38, NNZs: 26301, Bias: -0.130000, T: 36204, Avg. loss: 0.000537
Total training time: 0.04 seconds.
Norm: 19.13, NNZs: 7404, Bias: -0.240000, T: 9051, Avg. loss: 0.006877
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 5
-- Epoch 2
Norm: 18.90, NNZs: 6377, Bias: -0.270000, T: 9051, Avg. loss: 0.008372
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 1
-- Epoch 2
-- Epoch 1
Norm: 19.60, NNZs: 6302, Bias: -0.230000, T: 18102, Avg. loss: 0.002079
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 1
Norm: 17.90, NNZs: 18981, Bias: -0.160000, T: 9051, Avg. loss: 0.006783
Total training time: 0.04 seconds.
-- Epoch 1
Norm: 18.41, NNZs: 6629, Bias: -0.130000, T: 9051, Avg. loss: 0.005804
Total training time: 0.03 seconds.
Norm: 20.06, NNZs: 9461, Bias: -0.250000, T: 18102, Avg. loss: 0.001213
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 1
-- Epoch 1
-- Epoch 1
Total training time: 0.04 seconds.
Norm: 18.29, NNZs: 22864, Bias: -0.180000, T: 9051, Avg. loss: 0.007180
Norm: 16.13, NNZs: 13778, Bias: -0.180000, T: 9051, Avg. loss: 0.003763
Total training time: 0.01 seconds.
Norm: 20.47, NNZs: 6571, Bias: -0.180000, T: 27153, Avg. loss: 0.001042
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 4
-- Epoch 1
Norm: 17.68, NNZs: 7556, Bias: -0.250000, T: 9052, Avg. loss: 0.005220
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 20.68, NNZs: 7489, Bias: -0.220000, T: 18102, Avg. loss: 0.002255
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 20.97, NNZs: 9862, Bias: -0.230000, T: 27153, Avg. loss: 0.000618
Total training time: 0.03 seconds.
Norm: 19.96, NNZs: 8377, Bias: -0.230000, T: 27153, Avg. loss: 0.000473
Total training time: 0.05 seconds.
Norm: 19.77, NNZs: 6431, Bias: -0.180000, T: 18102, Avg. loss: 0.001193
Total training time: 0.04 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 4
Norm: 16.47, NNZs: 10413, Bias: -0.140000, T: 9051, Avg. loss: 0.003554
Total training time: 0.02 seconds.
-- Epoch 1
-- Epoch 2
-- Epoch 2
Norm: 19.00, NNZs: 5609, Bias: -0.110000, T: 27153, Avg. loss: 0.000517
Total training time: 0.03 seconds.
-- Epoch 1
-- Epoch 4
Norm: 17.93, NNZs: 5863, Bias: -0.140000, T: 9051, Avg. loss: 0.004350
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 22.10, NNZs: 8504, Bias: -0.200000, T: 27153, Avg. loss: 0.000970
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 21.60, NNZs: 8028, Bias: -0.140000, T: 45255, Avg. loss: 0.000423
Total training time: 0.06 seconds.
Norm: 17.67, NNZs: 5805, Bias: -0.100000, T: 36204, Avg. loss: 0.000343
Norm: 18.32, NNZs: 7417, Bias: -0.250000, T: 18102, Avg. loss: 0.000802
Total training time: 0.05 seconds.
Norm: 18.86, NNZs: 8443, Bias: -0.200000, T: 9051, Avg. loss: 0.006424
Total training time: 0.03 seconds.
Norm: 18.88, NNZs: 8632, Bias: -0.170000, T: 9052, Avg. loss: 0.007166
-- Epoch 2
-- Epoch 1
Norm: 17.99, NNZs: 7500, Bias: -0.220000, T: 9051, Avg. loss: 0.005516
Total training time: 0.02 seconds.
Norm: 20.12, NNZs: 24429, Bias: -0.280000, T: 18102, Avg. loss: 0.002106
Total training time: 0.06 seconds.
Norm: 19.11, NNZs: 9606, Bias: -0.280000, T: 9051, Avg. loss: 0.006510
Total training time: 0.02 seconds.
Total training time: 0.05 seconds.
Norm: 20.34, NNZs: 6725, Bias: -0.140000, T: 27153, Avg. loss: 0.000428
Total training time: 0.05 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 1
-- Epoch 1
-- Epoch 4
-- Epoch 1
Total training time: 0.03 seconds.
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 1
Norm: 18.35, NNZs: 7054, Bias: -0.140000, T: 9051, Avg. loss: 0.005512
Norm: 16.87, NNZs: 5509, Bias: -0.160000, T: 9051, Avg. loss: 0.003732
Norm: 21.07, NNZs: 10251, Bias: -0.230000, T: 18102, Avg. loss: 0.002449
Norm: 14.96, NNZs: 4237, Bias: -0.160000, T: 9051, Avg. loss: 0.002785
-- Epoch 2
-- Epoch 3
Total training time: 0.04 seconds.
Norm: 21.72, NNZs: 7852, Bias: -0.210000, T: 27153, Avg. loss: 0.001211
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 19.00, NNZs: 8258, Bias: -0.230000, T: 9051, Avg. loss: 0.008226
Norm: 21.22, NNZs: 8233, Bias: -0.240000, T: 18102, Avg. loss: 0.002191
Norm: 17.92, NNZs: 15909, Bias: -0.210000, T: 9052, Avg. loss: 0.007030
Total training time: 0.05 seconds.
Norm: 21.58, NNZs: 7422, Bias: -0.260000, T: 18102, Avg. loss: 0.002808
Total training time: 0.06 seconds.
Norm: 17.79, NNZs: 14506, Bias: -0.230000, T: 18102, Avg. loss: 0.001074
Total training time: 0.03 seconds.
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 3
Norm: 18.07, NNZs: 9318, Bias: -0.250000, T: 18102, Avg. loss: 0.001080
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 21.05, NNZs: 7371, Bias: -0.150000, T: 36204, Avg. loss: 0.000592
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 19.14, NNZs: 7511, Bias: -0.270000, T: 18102, Avg. loss: 0.001716
Total training time: 0.06 seconds.
Norm: 18.85, NNZs: 7574, Bias: -0.280000, T: 27153, Avg. loss: 0.000220
Total training time: 0.06 seconds.
Norm: 18.53, NNZs: 6130, Bias: -0.150000, T: 9052, Avg. loss: 0.005423
Total training time: 0.04 seconds.
Norm: 21.31, NNZs: 9951, Bias: -0.190000, T: 36204, Avg. loss: 0.000350
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 4
-- Epoch 2
-- Epoch 5
-- Epoch 5
Norm: 19.13, NNZs: 8153, Bias: -0.270000, T: 9051, Avg. loss: 0.008620
Total training time: 0.04 seconds.
Norm: 21.23, NNZs: 9587, Bias: -0.230000, T: 18102, Avg. loss: 0.002589
Total training time: 0.04 seconds.
-- Epoch 2
Total training time: 0.06 seconds.
Norm: 18.53, NNZs: 5907, Bias: -0.140000, T: 9052, Avg. loss: 0.004479
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 20.04, NNZs: 19576, Bias: -0.190000, T: 18102, Avg. loss: 0.002380
Total training time: 0.07 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 2
Norm: 18.63, NNZs: 14732, Bias: -0.230000, T: 27153, Avg. loss: 0.000650
Total training time: 0.04 seconds.
Norm: 18.00, NNZs: 8336, Bias: -0.200000, T: 9051, Avg. loss: 0.005999
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 2
Norm: 16.50, NNZs: 9226, Bias: -0.200000, T: 9051, Avg. loss: 0.003708
Total training time: 0.08 seconds.
-- Epoch 2
Norm: 20.35, NNZs: 6598, Bias: -0.210000, T: 27153, Avg. loss: 0.000898
Total training time: 0.07 seconds.
Norm: 19.14, NNZs: 7950, Bias: -0.270000, T: 36204, Avg. loss: 0.000156
Norm: 17.27, NNZs: 6587, Bias: -0.200000, T: 9052, Avg. loss: 0.005457
Norm: 16.82, NNZs: 6935, Bias: -0.180000, T: 9051, Avg. loss: 0.004336
Total training time: 0.06 seconds.
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 2
Norm: 18.72, NNZs: 6053, Bias: -0.150000, T: 18102, Avg. loss: 0.001311
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 19.76, NNZs: 6409, Bias: -0.190000, T: 18104, Avg. loss: 0.001040
Total training time: 0.04 seconds.
Norm: 22.55, NNZs: 7680, Bias: -0.310000, T: 27153, Avg. loss: 0.001008
Total training time: 0.07 seconds.
-- Epoch 3
-- Epoch 4
Norm: 19.99, NNZs: 9171, Bias: -0.270000, T: 18102, Avg. loss: 0.001390
Total training time: 0.07 seconds.
Norm: 17.02, NNZs: 7071, Bias: -0.170000, T: 9051, Avg. loss: 0.004714
-- Epoch 3
Norm: 17.94, NNZs: 5333, Bias: -0.190000, T: 9051, Avg. loss: 0.004508
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 2
Norm: 20.39, NNZs: 24509, Bias: -0.230000, T: 18102, Avg. loss: 0.002225
Total training time: 0.05 seconds.
Norm: 19.20, NNZs: 7058, Bias: -0.240000, T: 9052, Avg. loss: 0.008787
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 17.58, NNZs: 5615, Bias: -0.200000, T: 9051, Avg. loss: 0.005365
Total training time: 0.05 seconds.
Norm: 18.81, NNZs: 7428, Bias: -0.260000, T: 18102, Avg. loss: 0.001700
Total training time: 0.08 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 3
Norm: 18.40, NNZs: 8302, Bias: -0.210000, T: 9051, Avg. loss: 0.005949
Total training time: 0.08 seconds.
-- Epoch 2
Norm: 19.01, NNZs: 7549, Bias: -0.250000, T: 18104, Avg. loss: 0.001741
Norm: 18.77, NNZs: 7462, Bias: -0.300000, T: 18102, Avg. loss: 0.001040
Total training time: 0.06 seconds.
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 3
Norm: 19.76, NNZs: 6672, Bias: -0.180000, T: 18102, Avg. loss: 0.001265
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 17.48, NNZs: 6040, Bias: -0.200000, T: 9051, Avg. loss: 0.005793
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 19.59, NNZs: 6330, Bias: -0.130000, T: 27153, Avg. loss: 0.000601
Total training time: 0.07 seconds.
Norm: 16.17, NNZs: 4643, Bias: -0.210000, T: 18102, Avg. loss: 0.000892
Total training time: 0.06 seconds.
Norm: 20.65, NNZs: 9614, Bias: -0.230000, T: 27153, Avg. loss: 0.000645
Total training time: 0.08 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 4
Norm: 21.20, NNZs: 24708, Bias: -0.170000, T: 27153, Avg. loss: 0.001113
Total training time: 0.06 seconds.
Norm: 19.40, NNZs: 7690, Bias: -0.240000, T: 27153, Avg. loss: 0.000719
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 17.98, NNZs: 9751, Bias: -0.240000, T: 18102, Avg. loss: 0.001108
Total training time: 0.09 seconds.
Norm: 20.03, NNZs: 6446, Bias: -0.230000, T: 18102, Avg. loss: 0.002157
Total training time: 0.05 seconds.
Total training time: 0.08 seconds.
Total training time: 0.09 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 3
Norm: 17.66, NNZs: 5608, Bias: -0.170000, T: 9052, Avg. loss: 0.005338
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 16.91, NNZs: 5787, Bias: -0.210000, T: 9051, Avg. loss: 0.004373
Total training time: 0.07 seconds.
Norm: 20.34, NNZs: 6824, Bias: -0.160000, T: 27153, Avg. loss: 0.000533
Total training time: 0.09 seconds.
-- Epoch 2
-- Epoch 4
Total training time: 0.05 seconds.
Norm: 15.15, NNZs: 5132, Bias: -0.190000, T: 9052, Avg. loss: 0.002976
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 17.12, NNZs: 6256, Bias: -0.220000, T: 9052, Avg. loss: 0.004352
Norm: 16.84, NNZs: 4992, Bias: -0.190000, T: 9051, Avg. loss: 0.003672
Total training time: 0.04 seconds.
Total training time: 0.09 seconds.
Norm: 16.08, NNZs: 10007, Bias: -0.210000, T: 9052, Avg. loss: 0.003659
Total training time: 0.05 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 2
Norm: 18.18, NNZs: 5917, Bias: -0.090000, T: 45255, Avg. loss: 0.000231
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 18.36, NNZs: 6943, Bias: -0.150000, T: 9051, Avg. loss: 0.006204
Total training time: 0.10 seconds.
Norm: 19.65, NNZs: 6880, Bias: -0.230000, T: 18104, Avg. loss: 0.002136
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 19.55, NNZs: 8760, Bias: -0.250000, T: 18104, Avg. loss: 0.001286
Total training time: 0.08 seconds.
-- Epoch 3
-- Epoch 3
Norm: 18.56, NNZs: 9453, Bias: -0.220000, T: 27153, Avg. loss: 0.000439
Total training time: 0.10 seconds.
Norm: 18.64, NNZs: 6385, Bias: -0.250000, T: 18102, Avg. loss: 0.001148
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 3
Norm: 18.73, NNZs: 6764, Bias: -0.290000, T: 18104, Avg. loss: 0.000851
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 22.38, NNZs: 8181, Bias: -0.110000, T: 36204, Avg. loss: 0.000724
Total training time: 0.09 seconds.
Norm: 18.49, NNZs: 5598, Bias: -0.150000, T: 18102, Avg. loss: 0.001058
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 15.29, NNZs: 4418, Bias: -0.150000, T: 9051, Avg. loss: 0.003042
Total training time: 0.09 seconds.
Norm: 21.12, NNZs: 24634, Bias: -0.260000, T: 27153, Avg. loss: 0.001199
Total training time: 0.10 seconds.
-- Epoch 5
-- Epoch 2
Norm: 18.65, NNZs: 6046, Bias: -0.070000, T: 54306, Avg. loss: 0.000243
-- Epoch 4
Total training time: 0.10 seconds.
Norm: 20.68, NNZs: 6820, Bias: -0.110000, T: 36204, Avg. loss: 0.000296
Total training time: 0.10 seconds.
-- Epoch 7
-- Epoch 5
Total training time: 0.07 seconds.
Norm: 20.22, NNZs: 16824, Bias: -0.230000, T: 18104, Avg. loss: 0.001974
Total training time: 0.08 seconds.
Norm: 21.59, NNZs: 10024, Bias: -0.190000, T: 45255, Avg. loss: 0.000288
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 18.94, NNZs: 10165, Bias: -0.210000, T: 36204, Avg. loss: 0.000467
Total training time: 0.10 seconds.
-- Epoch 3
-- Epoch 6
-- Epoch 5
Norm: 20.95, NNZs: 9693, Bias: -0.180000, T: 36204, Avg. loss: 0.000374
Total training time: 0.10 seconds.
Norm: 20.54, NNZs: 6606, Bias: -0.170000, T: 27156, Avg. loss: 0.000668
Total training time: 0.06 seconds.
-- Epoch 5
-- Epoch 4
Norm: 21.88, NNZs: 9070, Bias: -0.270000, T: 18102, Avg. loss: 0.002477
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 21.37, NNZs: 9559, Bias: -0.230000, T: 18104, Avg. loss: 0.002303
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 20.68, NNZs: 6902, Bias: -0.130000, T: 36204, Avg. loss: 0.000452
-- Epoch 2
Norm: 21.67, NNZs: 7986, Bias: -0.300000, T: 18104, Avg. loss: 0.002722
Total training time: 0.08 seconds.
Norm: 20.18, NNZs: 7827, Bias: -0.310000, T: 27153, Avg. loss: 0.001125
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 19.66, NNZs: 8526, Bias: -0.250000, T: 18102, Avg. loss: 0.001649
Total training time: 0.08 seconds.
Norm: 19.34, NNZs: 5697, Bias: -0.070000, T: 36204, Avg. loss: 0.000316
Total training time: 0.09 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 5
Norm: 17.25, NNZs: 7334, Bias: -0.200000, T: 9051, Avg. loss: 0.004825
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 18.19, NNZs: 10823, Bias: -0.160000, T: 18102, Avg. loss: 0.001249
Total training time: 0.09 seconds.
Norm: 20.03, NNZs: 8892, Bias: -0.270000, T: 27156, Avg. loss: 0.000461
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 19.01, NNZs: 14858, Bias: -0.210000, T: 36204, Avg. loss: 0.000383
Total training time: 0.08 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 5
-- Epoch 3
Norm: 22.69, NNZs: 8630, Bias: -0.160000, T: 36204, Avg. loss: 0.000701
Norm: 17.14, NNZs: 5399, Bias: -0.160000, T: 18102, Avg. loss: 0.001036
Total training time: 0.11 seconds.
Total training time: 0.12 seconds.
Norm: 17.04, NNZs: 4884, Bias: -0.160000, T: 27153, Avg. loss: 0.000467
Total training time: 0.09 seconds.
-- Epoch 3
-- Epoch 4
Norm: 20.84, NNZs: 7168, Bias: -0.150000, T: 18104, Avg. loss: 0.002115
Total training time: 0.08 seconds.
-- Epoch 5
Norm: 22.43, NNZs: 8162, Bias: -0.290000, T: 27156, Avg. loss: 0.001145
Total training time: 0.09 seconds.
-- Epoch 3
-- Epoch 4
Norm: 22.99, NNZs: 26503, Bias: -0.090000, T: 45255, Avg. loss: 0.000435
Norm: 22.85, NNZs: 8691, Bias: -0.280000, T: 36204, Avg. loss: 0.000535
Total training time: 0.10 seconds.
Total training time: 0.12 seconds.
Total training time: 0.10 seconds.
-- Epoch 2
-- Epoch 5
-- Epoch 6
-- Epoch 5
Norm: 20.91, NNZs: 6872, Bias: -0.050000, T: 45255, Avg. loss: 0.000259
Norm: 18.92, NNZs: 7759, Bias: -0.260000, T: 18102, Avg. loss: 0.001264
-- Epoch 4
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 21.80, NNZs: 24892, Bias: -0.160000, T: 36204, Avg. loss: 0.000730
Total training time: 0.09 seconds.
Norm: 21.30, NNZs: 7783, Bias: -0.180000, T: 27153, Avg. loss: 0.000839
Total training time: 0.11 seconds.
Norm: 21.97, NNZs: 10524, Bias: -0.200000, T: 27153, Avg. loss: 0.000880
Total training time: 0.08 seconds.
-- Epoch 5
-- Epoch 4
-- Epoch 4
Norm: 18.22, NNZs: 9613, Bias: -0.250000, T: 27153, Avg. loss: 0.000511
Total training time: 0.12 seconds.
-- Epoch 4
Total training time: 0.12 seconds.
-- Epoch 6
Norm: 21.63, NNZs: 7561, Bias: -0.110000, T: 45255, Avg. loss: 0.000481
Total training time: 0.12 seconds.
Norm: 23.03, NNZs: 8785, Bias: -0.260000, T: 45255, Avg. loss: 0.000498
Total training time: 0.11 seconds.
Norm: 23.79, NNZs: 26680, Bias: -0.070000, T: 54306, Avg. loss: 0.000415
Total training time: 0.13 seconds.
-- Epoch 6
Norm: 20.58, NNZs: 7888, Bias: -0.200000, T: 18102, Avg. loss: 0.001804
Total training time: 0.08 seconds.
-- Epoch 6
-- Epoch 7
-- Epoch 3
Norm: 21.87, NNZs: 8930, Bias: -0.270000, T: 18102, Avg. loss: 0.002675
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 19.36, NNZs: 7986, Bias: -0.270000, T: 45255, Avg. loss: 0.000094
Total training time: 0.11 seconds.
-- Epoch 6
Norm: 20.69, NNZs: 7166, Bias: -0.190000, T: 27156, Avg. loss: 0.001012
Total training time: 0.06 seconds.
-- Epoch 6
-- Epoch 4
Norm: 20.62, NNZs: 8007, Bias: -0.290000, T: 36204, Avg. loss: 0.000915
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 20.29, NNZs: 8500, Bias: -0.230000, T: 36204, Avg. loss: 0.000317
Total training time: 0.13 seconds.
Norm: 19.94, NNZs: 8133, Bias: -0.230000, T: 36204, Avg. loss: 0.000697
Total training time: 0.13 seconds.
Norm: 23.03, NNZs: 8374, Bias: -0.060000, T: 45255, Avg. loss: 0.000491
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 23.00, NNZs: 7782, Bias: -0.290000, T: 36204, Avg. loss: 0.000614
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 19.59, NNZs: 7638, Bias: -0.280000, T: 27153, Avg. loss: 0.000386
Total training time: 0.08 seconds.
Norm: 17.82, NNZs: 5580, Bias: -0.130000, T: 27153, Avg. loss: 0.000468
Total training time: 0.12 seconds.
Norm: 19.95, NNZs: 6120, Bias: -0.230000, T: 18102, Avg. loss: 0.001121
Total training time: 0.10 seconds.
-- Epoch 4
-- Epoch 3
Norm: 19.70, NNZs: 7818, Bias: -0.260000, T: 27156, Avg. loss: 0.000847
Total training time: 0.11 seconds.
Norm: 23.26, NNZs: 8753, Bias: -0.150000, T: 45255, Avg. loss: 0.000467
Total training time: 0.13 seconds.
-- Epoch 4
-- Epoch 6
Norm: 24.34, NNZs: 26815, Bias: -0.060000, T: 63357, Avg. loss: 0.000290
Total training time: 0.13 seconds.
-- Epoch 8
Norm: 17.40, NNZs: 5010, Bias: -0.160000, T: 36204, Avg. loss: 0.000334
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 18.93, NNZs: 11038, Bias: -0.130000, T: 27153, Avg. loss: 0.000498
Total training time: 0.10 seconds.
Norm: 21.22, NNZs: 7317, Bias: -0.150000, T: 36208, Avg. loss: 0.000431
Norm: 19.50, NNZs: 8007, Bias: -0.250000, T: 54306, Avg. loss: 0.000086
Total training time: 0.07 seconds.
Total training time: 0.12 seconds.
-- Epoch 4
-- Epoch 7
-- Epoch 5
Total training time: 0.11 seconds.
Norm: 21.47, NNZs: 6983, Bias: -0.070000, T: 54306, Avg. loss: 0.000298
Total training time: 0.13 seconds.
-- Epoch 3
-- Epoch 7
Norm: 16.52, NNZs: 5599, Bias: -0.200000, T: 18104, Avg. loss: 0.000769
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 22.49, NNZs: 9921, Bias: -0.220000, T: 27153, Avg. loss: 0.001473
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 18.58, NNZs: 9718, Bias: -0.250000, T: 36204, Avg. loss: 0.000336
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 20.52, NNZs: 6345, Bias: -0.190000, T: 27153, Avg. loss: 0.000454
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 20.79, NNZs: 6744, Bias: -0.190000, T: 36204, Avg. loss: 0.000610
Total training time: 0.13 seconds.
Norm: 20.96, NNZs: 6696, Bias: -0.160000, T: 36208, Avg. loss: 0.000450
Total training time: 0.09 seconds.
-- Epoch 5
-- Epoch 5
Norm: 16.31, NNZs: 4709, Bias: -0.160000, T: 9052, Avg. loss: 0.003356
Total training time: 0.08 seconds.
-- Epoch 2
Norm: 21.72, NNZs: 8080, Bias: -0.130000, T: 54306, Avg. loss: 0.000369
Total training time: 0.14 seconds.
Norm: 22.85, NNZs: 9149, Bias: -0.270000, T: 27153, Avg. loss: 0.000993
Total training time: 0.10 seconds.
Norm: 18.61, NNZs: 10044, Bias: -0.240000, T: 27153, Avg. loss: 0.000419
Total training time: 0.14 seconds.
-- Epoch 7
-- Epoch 4
-- Epoch 4
Norm: 19.67, NNZs: 8021, Bias: -0.260000, T: 63357, Avg. loss: 0.000106
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 22.68, NNZs: 8253, Bias: -0.280000, T: 36208, Avg. loss: 0.000820
Total training time: 0.11 seconds.
Convergence after 7 epochs took 0.13 seconds
-- Epoch 5
-- Epoch 1
Norm: 20.29, NNZs: 9227, Bias: -0.270000, T: 18102, Avg. loss: 0.001433
Norm: 21.12, NNZs: 20061, Bias: -0.190000, T: 27153, Avg. loss: 0.001290
Total training time: 0.14 seconds.
Norm: 18.79, NNZs: 9847, Bias: -0.210000, T: 45255, Avg. loss: 0.000272
Total training time: 0.14 seconds.
Norm: 22.57, NNZs: 25039, Bias: -0.130000, T: 45255, Avg. loss: 0.000523
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 6
-- Epoch 6
Norm: 21.78, NNZs: 7435, Bias: -0.130000, T: 27156, Avg. loss: 0.000865
Total training time: 0.11 seconds.
Norm: 20.10, NNZs: 6492, Bias: -0.090000, T: 36204, Avg. loss: 0.000436
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 19.22, NNZs: 6873, Bias: -0.280000, T: 27153, Avg. loss: 0.000372
Norm: 19.88, NNZs: 6901, Bias: -0.200000, T: 18102, Avg. loss: 0.001966
Total training time: 0.15 seconds.
Total training time: 0.10 seconds.
Norm: 23.16, NNZs: 8833, Bias: -0.250000, T: 54306, Avg. loss: 0.000423
Total training time: 0.13 seconds.
-- Epoch 3
-- Epoch 3
-- Epoch 7
Norm: 23.57, NNZs: 8834, Bias: -0.140000, T: 54306, Avg. loss: 0.000454
Total training time: 0.15 seconds.
-- Epoch 4
-- Epoch 7
Norm: 21.21, NNZs: 7399, Bias: -0.110000, T: 45260, Avg. loss: 0.000330
Total training time: 0.11 seconds.
Norm: 20.44, NNZs: 8537, Bias: -0.230000, T: 45255, Avg. loss: 0.000293
Total training time: 0.15 seconds.
Norm: 20.55, NNZs: 6593, Bias: -0.040000, T: 45255, Avg. loss: 0.000269
Total training time: 0.14 seconds.
-- Epoch 6
-- Epoch 6
-- Epoch 6
Norm: 19.56, NNZs: 5875, Bias: -0.120000, T: 27153, Avg. loss: 0.000597
Total training time: 0.15 seconds.
Norm: 21.58, NNZs: 8145, Bias: -0.150000, T: 27153, Avg. loss: 0.000962
Total training time: 0.11 seconds.
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 22.36, NNZs: 10683, Bias: -0.190000, T: 36204, Avg. loss: 0.000810
Total training time: 0.11 seconds.
-- Epoch 5
Norm: 22.91, NNZs: 10046, Bias: -0.210000, T: 36204, Avg. loss: 0.000756
Total training time: 0.13 seconds.
Norm: 19.28, NNZs: 8096, Bias: -0.310000, T: 18102, Avg. loss: 0.001519
-- Epoch 4
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 22.28, NNZs: 9798, Bias: -0.210000, T: 27156, Avg. loss: 0.001376
Total training time: 0.12 seconds.
-- Epoch 3
-- Epoch 4
Norm: 21.38, NNZs: 17238, Bias: -0.200000, T: 27156, Avg. loss: 0.001545
Total training time: 0.13 seconds.
Norm: 19.88, NNZs: 8069, Bias: -0.300000, T: 36204, Avg. loss: 0.000134
Total training time: 0.10 seconds.
Norm: 23.28, NNZs: 9237, Bias: -0.310000, T: 36204, Avg. loss: 0.000717
Total training time: 0.12 seconds.
-- Epoch 4
-- Epoch 5
-- Epoch 5
Norm: 18.87, NNZs: 10146, Bias: -0.250000, T: 36204, Avg. loss: 0.000392
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 17.58, NNZs: 10487, Bias: -0.260000, T: 18104, Avg. loss: 0.000840
Total training time: 0.12 seconds.
-- Epoch 3
Norm: 21.31, NNZs: 9798, Bias: -0.170000, T: 45255, Avg. loss: 0.000352
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 22.42, NNZs: 8395, Bias: -0.090000, T: 36204, Avg. loss: 0.000835
Total training time: 0.11 seconds.
-- Epoch 5
Norm: 17.16, NNZs: 5771, Bias: -0.150000, T: 27156, Avg. loss: 0.000529
Total training time: 0.10 seconds.
-- Epoch 4
Norm: 21.01, NNZs: 6751, Bias: -0.190000, T: 27153, Avg. loss: 0.001013
Total training time: 0.13 seconds.
-- Epoch 4
-- Epoch 4
Norm: 19.91, NNZs: 7062, Bias: -0.280000, T: 36204, Avg. loss: 0.000356
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 21.94, NNZs: 8140, Bias: -0.100000, T: 63357, Avg. loss: 0.000342
Total training time: 0.16 seconds.
Norm: 21.73, NNZs: 7485, Bias: -0.120000, T: 45260, Avg. loss: 0.000459
Total training time: 0.10 seconds.
Norm: 18.37, NNZs: 5525, Bias: -0.170000, T: 18104, Avg. loss: 0.001131
Convergence after 7 epochs took 0.16 seconds
Total training time: 0.10 seconds.
-- Epoch 6
-- Epoch 3
Norm: 17.82, NNZs: 5103, Bias: -0.130000, T: 45255, Avg. loss: 0.000294
Total training time: 0.14 seconds.
Norm: 19.13, NNZs: 10210, Bias: -0.190000, T: 45255, Avg. loss: 0.000274
Total training time: 0.16 seconds.
Norm: 21.81, NNZs: 8446, Bias: -0.230000, T: 27153, Avg. loss: 0.000764
Total training time: 0.16 seconds.
-- Epoch 6
-- Epoch 4
Norm: 22.18, NNZs: 7757, Bias: -0.100000, T: 54306, Avg. loss: 0.000413
-- Epoch 6
Norm: 16.90, NNZs: 5108, Bias: -0.230000, T: 18102, Avg. loss: 0.000933
Total training time: 0.16 seconds.
Total training time: 0.16 seconds.
-- Epoch 3
-- Epoch 7
Norm: 19.57, NNZs: 7880, Bias: -0.250000, T: 27153, Avg. loss: 0.000309
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 23.31, NNZs: 7845, Bias: -0.240000, T: 45255, Avg. loss: 0.000412
Total training time: 0.16 seconds.
-- Epoch 6
Norm: 22.61, NNZs: 7800, Bias: -0.080000, T: 36208, Avg. loss: 0.000770
Total training time: 0.13 seconds.
-- Epoch 5
-- Epoch 4
Norm: 20.63, NNZs: 8617, Bias: -0.200000, T: 54306, Avg. loss: 0.000308
Total training time: 0.17 seconds.
-- Epoch 7
Norm: 23.05, NNZs: 8340, Bias: -0.270000, T: 45260, Avg. loss: 0.000594
Total training time: 0.15 seconds.
Norm: 20.95, NNZs: 7186, Bias: -0.200000, T: 27153, Avg. loss: 0.000938
Total training time: 0.12 seconds.
-- Epoch 6
-- Epoch 4
Norm: 20.15, NNZs: 6166, Bias: -0.080000, T: 36204, Avg. loss: 0.000496
Norm: 23.62, NNZs: 11310, Bias: -0.030000, T: 54306, Avg. loss: 0.000352
Total training time: 0.16 seconds.
Total training time: 0.17 seconds.
Norm: 20.85, NNZs: 9628, Bias: -0.290000, T: 27153, Avg. loss: 0.000502
Total training time: 0.17 seconds.
Norm: 20.58, NNZs: 7748, Bias: -0.170000, T: 18102, Avg. loss: 0.002460
Total training time: 0.17 seconds.
Norm: 21.87, NNZs: 7765, Bias: -0.060000, T: 63357, Avg. loss: 0.000165
Total training time: 0.17 seconds.
-- Epoch 4
-- Epoch 3
Norm: 20.86, NNZs: 8149, Bias: -0.280000, T: 45255, Avg. loss: 0.000703
Total training time: 0.17 seconds.
Convergence after 7 epochs took 0.17 seconds
Norm: 20.20, NNZs: 8158, Bias: -0.210000, T: 45255, Avg. loss: 0.000581
Total training time: 0.18 seconds.
-- Epoch 6
Norm: 22.56, NNZs: 10769, Bias: -0.140000, T: 45255, Avg. loss: 0.000657
Total training time: 0.13 seconds.
-- Epoch 6
Norm: 19.33, NNZs: 6887, Bias: -0.280000, T: 27156, Avg. loss: 0.000343
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 23.01, NNZs: 25276, Bias: -0.100000, T: 54306, Avg. loss: 0.000612
Total training time: 0.15 seconds.
Norm: 22.04, NNZs: 21697, Bias: -0.140000, T: 36208, Avg. loss: 0.000835
Total training time: 0.16 seconds.
-- Epoch 7
Norm: 20.85, NNZs: 8682, Bias: -0.160000, T: 63357, Avg. loss: 0.000301
Total training time: 0.18 seconds.
Convergence after 7 epochs took 0.18 seconds
-- Epoch 6
Norm: 21.45, NNZs: 7483, Bias: -0.110000, T: 54312, Avg. loss: 0.000340
Total training time: 0.14 seconds.
-- Epoch 7
-- Epoch 1
Norm: 21.74, NNZs: 10063, Bias: -0.150000, T: 54306, Avg. loss: 0.000244
Norm: 20.33, NNZs: 8796, Bias: -0.260000, T: 27153, Avg. loss: 0.000567
Total training time: 0.15 seconds.
-- Epoch 4
Norm: 18.23, NNZs: 5705, Bias: -0.110000, T: 36204, Avg. loss: 0.000334
Norm: 22.39, NNZs: 9195, Bias: -0.210000, T: 36204, Avg. loss: 0.000577
Total training time: 0.17 seconds.
Total training time: 0.17 seconds.
Norm: 21.10, NNZs: 8217, Bias: -0.270000, T: 54306, Avg. loss: 0.000813
Total training time: 0.18 seconds.
-- Epoch 5
-- Epoch 5
Norm: 17.32, NNZs: 5241, Bias: -0.190000, T: 27153, Avg. loss: 0.000471
Total training time: 0.17 seconds.
Norm: 21.86, NNZs: 24459, Bias: -0.140000, T: 36204, Avg. loss: 0.000805
Total training time: 0.18 seconds.
-- Epoch 7
-- Epoch 5
-- Epoch 4
Norm: 19.54, NNZs: 6979, Bias: -0.290000, T: 36208, Avg. loss: 0.000196
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 21.49, NNZs: 6909, Bias: -0.170000, T: 36204, Avg. loss: 0.000472
Total training time: 0.16 seconds.
-- Epoch 7
-- Epoch 5
Norm: 21.63, NNZs: 24835, Bias: -0.210000, T: 36204, Avg. loss: 0.000944
Total training time: 0.19 seconds.
Norm: 22.87, NNZs: 9872, Bias: -0.180000, T: 36208, Avg. loss: 0.000962
Total training time: 0.15 seconds.
Norm: 22.59, NNZs: 9242, Bias: -0.270000, T: 27153, Avg. loss: 0.000657
Total training time: 0.19 seconds.
Norm: 17.95, NNZs: 10653, Bias: -0.240000, T: 27156, Avg. loss: 0.000436
Total training time: 0.14 seconds.
Norm: 19.94, NNZs: 8396, Bias: -0.260000, T: 27153, Avg. loss: 0.000726
Total training time: 0.15 seconds.
-- Epoch 5
-- Epoch 5
-- Epoch 4
-- Epoch 4
-- Epoch 4
Norm: 21.66, NNZs: 7511, Bias: -0.050000, T: 63364, Avg. loss: 0.000236
Total training time: 0.14 seconds.
Convergence after 7 epochs took 0.14 seconds
Norm: 18.21, NNZs: 5279, Bias: -0.130000, T: 54306, Avg. loss: 0.000226
Total training time: 0.16 seconds.
Norm: 20.58, NNZs: 9067, Bias: -0.230000, T: 36204, Avg. loss: 0.000354
Total training time: 0.15 seconds.
-- Epoch 7
-- Epoch 5
Norm: 21.49, NNZs: 9842, Bias: -0.140000, T: 54306, Avg. loss: 0.000261
Total training time: 0.18 seconds.
Norm: 21.66, NNZs: 7375, Bias: -0.150000, T: 36204, Avg. loss: 0.000723
Norm: 22.13, NNZs: 7594, Bias: -0.060000, T: 54312, Avg. loss: 0.000313
Total training time: 0.13 seconds.
Norm: 21.14, NNZs: 9779, Bias: -0.290000, T: 36204, Avg. loss: 0.000295
Total training time: 0.19 seconds.
-- Epoch 7
-- Epoch 5
-- Epoch 1
Norm: 23.14, NNZs: 9930, Bias: -0.180000, T: 45260, Avg. loss: 0.000822
Total training time: 0.16 seconds.
Norm: 18.38, NNZs: 10838, Bias: -0.230000, T: 36208, Avg. loss: 0.000343
Total training time: 0.15 seconds.
-- Epoch 6
Total training time: 0.18 seconds.
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 23.11, NNZs: 9353, Bias: -0.270000, T: 36204, Avg. loss: 0.000422
Total training time: 0.19 seconds.
Norm: 22.52, NNZs: 7875, Bias: -0.060000, T: 63357, Avg. loss: 0.000333
Total training time: 0.19 seconds.
Norm: 20.76, NNZs: 6415, Bias: -0.170000, T: 36204, Avg. loss: 0.000320
Total training time: 0.16 seconds.
-- Epoch 5
-- Epoch 8
-- Epoch 5
Norm: 23.35, NNZs: 25351, Bias: -0.060000, T: 63357, Avg. loss: 0.000458
Total training time: 0.17 seconds.
Norm: 20.54, NNZs: 9218, Bias: -0.250000, T: 36208, Avg. loss: 0.000362
Total training time: 0.17 seconds.
-- Epoch 8
-- Epoch 5
Norm: 23.39, NNZs: 8870, Bias: -0.210000, T: 63357, Avg. loss: 0.000411
-- Epoch 5
Norm: 21.66, NNZs: 8146, Bias: -0.170000, T: 27153, Avg. loss: 0.001063
Total training time: 0.20 seconds.
-- Epoch 6
-- Epoch 5
-- Epoch 4
Norm: 23.49, NNZs: 8500, Bias: -0.240000, T: 54312, Avg. loss: 0.000701
Total training time: 0.17 seconds.
-- Epoch 7
Norm: 23.61, NNZs: 7898, Bias: -0.240000, T: 54306, Avg. loss: 0.000448
Total training time: 0.19 seconds.
Norm: 22.83, NNZs: 9262, Bias: -0.210000, T: 45255, Avg. loss: 0.000650
Total training time: 0.19 seconds.
-- Epoch 6
-- Epoch 7
Norm: 20.91, NNZs: 6457, Bias: -0.150000, T: 45255, Avg. loss: 0.000213
Total training time: 0.17 seconds.
Norm: 23.10, NNZs: 8370, Bias: -0.070000, T: 72408, Avg. loss: 0.000253
Total training time: 0.20 seconds.
-- Epoch 6
Norm: 23.91, NNZs: 25512, Bias: -0.050000, T: 72408, Avg. loss: 0.000462
Total training time: 0.17 seconds.
-- Epoch 7
-- Epoch 5
Norm: 23.22, NNZs: 10109, Bias: -0.200000, T: 45255, Avg. loss: 0.000787
Total training time: 0.17 seconds.
-- Epoch 1
-- Epoch 6
Norm: 19.26, NNZs: 10228, Bias: -0.130000, T: 54306, Avg. loss: 0.000218
Total training time: 0.20 seconds.
-- Epoch 7
-- Epoch 7
Norm: 21.24, NNZs: 9636, Bias: -0.030000, T: 54306, Avg. loss: 0.000214
Total training time: 0.19 seconds.
Norm: 24.09, NNZs: 8058, Bias: -0.220000, T: 63357, Avg. loss: 0.000368
Total training time: 0.20 seconds.
-- Epoch 7
Norm: 23.80, NNZs: 8581, Bias: -0.230000, T: 63364, Avg. loss: 0.000618
Total training time: 0.18 seconds.
-- Epoch 8
-- Epoch 8
Norm: 22.42, NNZs: 24602, Bias: -0.070000, T: 45255, Avg. loss: 0.000630
Total training time: 0.21 seconds.
Norm: 19.99, NNZs: 8088, Bias: -0.300000, T: 45255, Avg. loss: 0.000053
Total training time: 0.16 seconds.
-- Epoch 6
-- Epoch 6
Norm: 24.16, NNZs: 11464, Bias: -0.040000, T: 63357, Avg. loss: 0.000189
Total training time: 0.20 seconds.
Norm: 21.08, NNZs: 6992, Bias: -0.100000, T: 45255, Avg. loss: 0.000415
Total training time: 0.21 seconds.
Norm: 22.02, NNZs: 7448, Bias: -0.130000, T: 45255, Avg. loss: 0.000521
Total training time: 0.16 seconds.
-- Epoch 6
-- Epoch 6
Norm: 23.98, NNZs: 9452, Bias: -0.130000, T: 63357, Avg. loss: 0.000385
Total training time: 0.21 seconds.
Total training time: 0.19 seconds.
-- Epoch 8
-- Epoch 8
-- Epoch 1
Norm: 23.27, NNZs: 8629, Bias: -0.070000, T: 45255, Avg. loss: 0.000674
Total training time: 0.16 seconds.
Norm: 19.36, NNZs: 11213, Bias: -0.080000, T: 36204, Avg. loss: 0.000326
Total training time: 0.18 seconds.
-- Epoch 6
-- Epoch 5
Norm: 21.98, NNZs: 10118, Bias: -0.150000, T: 63357, Avg. loss: 0.000256
Total training time: 0.19 seconds.
Convergence after 7 epochs took 0.19 seconds
Norm: 20.81, NNZs: 6338, Bias: -0.070000, T: 45255, Avg. loss: 0.000313
Total training time: 0.21 seconds.
-- Epoch 6
Norm: 19.12, NNZs: 10218, Bias: -0.220000, T: 45255, Avg. loss: 0.000283
Total training time: 0.21 seconds.
Norm: 20.37, NNZs: 8064, Bias: -0.260000, T: 36208, Avg. loss: 0.000819
-- Epoch 1
Total training time: 0.19 seconds.
Norm: 19.91, NNZs: 8043, Bias: -0.270000, T: 36204, Avg. loss: 0.000145
Total training time: 0.15 seconds.
-- Epoch 6
-- Epoch 5
-- Epoch 5
Norm: 19.21, NNZs: 14955, Bias: -0.160000, T: 45255, Avg. loss: 0.000301
Total training time: 0.18 seconds.
-- Epoch 6
Norm: 19.95, NNZs: 5886, Bias: -0.070000, T: 45255, Avg. loss: 0.000361
Total training time: 0.20 seconds.
-- Epoch 6
Norm: 23.61, NNZs: 10064, Bias: -0.140000, T: 54312, Avg. loss: 0.000807
Total training time: 0.18 seconds.
Norm: 20.88, NNZs: 9294, Bias: -0.240000, T: 45260, Avg. loss: 0.000316
Total training time: 0.20 seconds.
-- Epoch 6
Norm: 19.38, NNZs: 7716, Bias: -0.210000, T: 9052, Avg. loss: 0.006237
Total training time: 0.01 seconds.
Norm: 23.80, NNZs: 8754, Bias: -0.060000, T: 54306, Avg. loss: 0.000482
Total training time: 0.17 seconds.
Norm: 18.56, NNZs: 5830, Bias: -0.110000, T: 45255, Avg. loss: 0.000232
Total training time: 0.21 seconds.
-- Epoch 6
Norm: 19.02, NNZs: 9926, Bias: -0.200000, T: 54306, Avg. loss: 0.000230
Total training time: 0.22 seconds.
Norm: 21.88, NNZs: 7968, Bias: -0.130000, T: 36204, Avg. loss: 0.000661
Total training time: 0.20 seconds.
-- Epoch 8
-- Epoch 7
-- Epoch 5
Norm: 24.39, NNZs: 8088, Bias: -0.190000, T: 72408, Avg. loss: 0.000433
Total training time: 0.21 seconds.
Norm: 17.20, NNZs: 9419, Bias: -0.210000, T: 9051, Avg. loss: 0.003332
Norm: 20.81, NNZs: 9118, Bias: -0.200000, T: 45255, Avg. loss: 0.000274
Total training time: 0.04 seconds.
Total training time: 0.19 seconds.
Convergence after 8 epochs took 0.21 seconds
-- Epoch 7
Norm: 21.89, NNZs: 7032, Bias: -0.110000, T: 45255, Avg. loss: 0.000488
Total training time: 0.19 seconds.
-- Epoch 2
-- Epoch 6
Norm: 23.64, NNZs: 8930, Bias: -0.200000, T: 72408, Avg. loss: 0.000318
Total training time: 0.21 seconds.
Norm: 22.21, NNZs: 8290, Bias: -0.080000, T: 36204, Avg. loss: 0.000769
Total training time: 0.22 seconds.
Norm: 22.67, NNZs: 7731, Bias: -0.080000, T: 63364, Avg. loss: 0.000335
Total training time: 0.16 seconds.
Convergence after 8 epochs took 0.21 seconds
-- Epoch 5
-- Epoch 8
-- Epoch 6
Norm: 22.51, NNZs: 21820, Bias: -0.130000, T: 45260, Avg. loss: 0.000872
Total training time: 0.20 seconds.
Norm: 17.68, NNZs: 5301, Bias: -0.150000, T: 36204, Avg. loss: 0.000455
Total training time: 0.21 seconds.
-- Epoch 6
-- Epoch 5
Norm: 20.31, NNZs: 8531, Bias: -0.260000, T: 36204, Avg. loss: 0.000474
Total training time: 0.19 seconds.
-- Epoch 5
Norm: 19.45, NNZs: 7333, Bias: -0.180000, T: 9051, Avg. loss: 0.006560
Norm: 24.09, NNZs: 10197, Bias: -0.130000, T: 63364, Avg. loss: 0.000768
Total training time: 0.04 seconds.
Total training time: 0.19 seconds.
Convergence after 7 epochs took 0.19 seconds
-- Epoch 2
Norm: 21.28, NNZs: 9797, Bias: -0.280000, T: 45255, Avg. loss: 0.000249
Total training time: 0.23 seconds.
-- Epoch 1
-- Epoch 6
Norm: 21.16, NNZs: 6536, Bias: -0.120000, T: 54306, Avg. loss: 0.000234
Norm: 21.26, NNZs: 8258, Bias: -0.250000, T: 63357, Avg. loss: 0.000645
Total training time: 0.22 seconds.
Convergence after 8 epochs took 0.23 seconds
-- Epoch 5
Norm: 19.77, NNZs: 7308, Bias: -0.260000, T: 45260, Avg. loss: 0.000143
Total training time: 0.17 seconds.
-- Epoch 1
-- Epoch 6
Norm: 19.40, NNZs: 8140, Bias: -0.210000, T: 9051, Avg. loss: 0.006593
Total training time: 0.09 seconds.
-- Epoch 2
Norm: 18.43, NNZs: 5317, Bias: -0.110000, T: 63357, Avg. loss: 0.000154
Norm: 23.23, NNZs: 10148, Bias: -0.040000, T: 45260, Avg. loss: 0.000514
Total training time: 0.20 seconds.
Total training time: 0.20 seconds.
Total training time: 0.20 seconds.
Norm: 17.56, NNZs: 5864, Bias: -0.110000, T: 36208, Avg. loss: 0.000312
Total training time: 0.17 seconds.
Convergence after 8 epochs took 0.21 seconds
Norm: 23.12, NNZs: 24738, Bias: -0.060000, T: 54306, Avg. loss: 0.000493
Total training time: 0.23 seconds.
-- Epoch 5
-- Epoch 6
-- Epoch 7
-- Epoch 1
-- Epoch 7
Norm: 22.24, NNZs: 25019, Bias: -0.160000, T: 45255, Avg. loss: 0.000760
Total training time: 0.24 seconds.
Norm: 21.05, NNZs: 9208, Bias: -0.180000, T: 54306, Avg. loss: 0.000273
Total training time: 0.20 seconds.
-- Epoch 1
Norm: 21.42, NNZs: 7545, Bias: -0.110000, T: 54306, Avg. loss: 0.000487
Total training time: 0.23 seconds.
-- Epoch 6
-- Epoch 7
-- Epoch 7
Norm: 23.72, NNZs: 9334, Bias: -0.260000, T: 45255, Avg. loss: 0.000582
Total training time: 0.20 seconds.
Norm: 24.37, NNZs: 9582, Bias: -0.080000, T: 72408, Avg. loss: 0.000377
Total training time: 0.24 seconds.
-- Epoch 7
Convergence after 7 epochs took 0.21 seconds
Convergence after 8 epochs took 0.24 seconds
Norm: 17.83, NNZs: 6770, Bias: -0.170000, T: 9051, Avg. loss: 0.004106
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 24.11, NNZs: 8680, Bias: -0.210000, T: 72416, Avg. loss: 0.000574
Total training time: 0.21 seconds.
Convergence after 8 epochs took 0.21 seconds
Norm: 21.79, NNZs: 9906, Bias: -0.130000, T: 63357, Avg. loss: 0.000240
Total training time: 0.23 seconds.
Norm: 20.03, NNZs: 8089, Bias: -0.260000, T: 45255, Avg. loss: 0.000102
Total training time: 0.18 seconds.
Norm: 18.69, NNZs: 10379, Bias: -0.190000, T: 18102, Avg. loss: 0.000793
Total training time: 0.06 seconds.
Convergence after 7 epochs took 0.23 seconds
-- Epoch 6
Norm: 19.46, NNZs: 5843, Bias: -0.090000, T: 27156, Avg. loss: 0.000639
Total training time: 0.18 seconds.
Norm: 19.72, NNZs: 11315, Bias: -0.080000, T: 45255, Avg. loss: 0.000233
Total training time: 0.21 seconds.
-- Epoch 6
Norm: 21.36, NNZs: 6999, Bias: -0.170000, T: 45255, Avg. loss: 0.000512
Total training time: 0.24 seconds.
Norm: 25.01, NNZs: 26961, Bias: -0.050000, T: 72408, Avg. loss: 0.000265
Total training time: 0.24 seconds.
-- Epoch 6
Convergence after 8 epochs took 0.24 seconds
Norm: 23.14, NNZs: 7818, Bias: -0.040000, T: 72416, Avg. loss: 0.000205
Total training time: 0.18 seconds.
Norm: 21.29, NNZs: 9277, Bias: -0.150000, T: 63357, Avg. loss: 0.000228
Total training time: 0.21 seconds.
Convergence after 8 epochs took 0.18 seconds
-- Epoch 8
Norm: 23.50, NNZs: 10214, Bias: -0.150000, T: 54306, Avg. loss: 0.000515
Total training time: 0.22 seconds.
-- Epoch 6
-- Epoch 7
Norm: 23.00, NNZs: 10883, Bias: -0.140000, T: 54306, Avg. loss: 0.000475
Total training time: 0.21 seconds.
Norm: 20.19, NNZs: 8090, Bias: -0.270000, T: 54306, Avg. loss: 0.000100
-- Epoch 1
Total training time: 0.19 seconds.
Norm: 20.04, NNZs: 7115, Bias: -0.270000, T: 45255, Avg. loss: 0.000185
Total training time: 0.22 seconds.
-- Epoch 6
-- Epoch 7
Norm: 21.27, NNZs: 6506, Bias: -0.040000, T: 54306, Avg. loss: 0.000178
Total training time: 0.25 seconds.
Norm: 21.56, NNZs: 9754, Bias: -0.040000, T: 63357, Avg. loss: 0.000076
-- Epoch 7
Norm: 22.54, NNZs: 7621, Bias: -0.090000, T: 54306, Avg. loss: 0.000465
-- Epoch 7
Total training time: 0.20 seconds.
Norm: 23.94, NNZs: 9348, Bias: -0.250000, T: 54306, Avg. loss: 0.000462
Total training time: 0.22 seconds.
Norm: 23.05, NNZs: 9299, Bias: -0.160000, T: 54306, Avg. loss: 0.000560
Total training time: 0.25 seconds.
Norm: 24.52, NNZs: 11527, Bias: -0.020000, T: 72408, Avg. loss: 0.000234
Total training time: 0.24 seconds.
Convergence after 7 epochs took 0.25 seconds
-- Epoch 7
-- Epoch 2
Norm: 22.34, NNZs: 7174, Bias: -0.130000, T: 54306, Avg. loss: 0.000448
Total training time: 0.22 seconds.
-- Epoch 7
Norm: 17.86, NNZs: 5955, Bias: -0.110000, T: 45260, Avg. loss: 0.000185
Total training time: 0.19 seconds.
Norm: 16.94, NNZs: 7107, Bias: -0.220000, T: 9052, Avg. loss: 0.003242
Total training time: 0.03 seconds.
Norm: 20.46, NNZs: 8609, Bias: -0.230000, T: 45255, Avg. loss: 0.000299
Total training time: 0.22 seconds.
-- Epoch 6
Norm: 18.75, NNZs: 11405, Bias: -0.200000, T: 45260, Avg. loss: 0.000267
Norm: 23.36, NNZs: 9410, Bias: -0.260000, T: 45255, Avg. loss: 0.000299
Total training time: 0.21 seconds.
Total training time: 0.26 seconds.
-- Epoch 1
Norm: 20.00, NNZs: 7436, Bias: -0.260000, T: 54312, Avg. loss: 0.000217
Total training time: 0.20 seconds.
Norm: 23.14, NNZs: 21952, Bias: -0.090000, T: 54312, Avg. loss: 0.000648
Total training time: 0.24 seconds.
-- Epoch 6
-- Epoch 6
-- Epoch 1
-- Epoch 7
Total training time: 0.24 seconds.
-- Epoch 7
Norm: 19.49, NNZs: 10294, Bias: -0.120000, T: 63357, Avg. loss: 0.000198
Total training time: 0.25 seconds.
Convergence after 7 epochs took 0.24 seconds
Convergence after 7 epochs took 0.26 seconds
Convergence after 8 epochs took 0.25 seconds
-- Epoch 1
Norm: 21.70, NNZs: 7686, Bias: -0.160000, T: 54306, Avg. loss: 0.000486
Total training time: 0.25 seconds.
-- Epoch 7
-- Epoch 7
Norm: 22.68, NNZs: 25107, Bias: -0.140000, T: 54306, Avg. loss: 0.000560
Total training time: 0.26 seconds.
-- Epoch 1
Norm: 20.05, NNZs: 8091, Bias: -0.240000, T: 54306, Avg. loss: 0.000085
Total training time: 0.20 seconds.
-- Epoch 7
-- Epoch 7
Norm: 17.80, NNZs: 9755, Bias: -0.140000, T: 9051, Avg. loss: 0.004274
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 17.90, NNZs: 7106, Bias: -0.190000, T: 9051, Avg. loss: 0.004535
Total training time: 0.02 seconds.
Norm: 21.05, NNZs: 9317, Bias: -0.220000, T: 54312, Avg. loss: 0.000219
Total training time: 0.24 seconds.
-- Epoch 2
Norm: 23.23, NNZs: 10960, Bias: -0.120000, T: 63357, Avg. loss: 0.000566
Total training time: 0.23 seconds.
-- Epoch 8
Norm: 19.31, NNZs: 10292, Bias: -0.180000, T: 54306, Avg. loss: 0.000248
Total training time: 0.27 seconds.
Norm: 19.51, NNZs: 15648, Bias: -0.130000, T: 54306, Avg. loss: 0.000253
Total training time: 0.23 seconds.
Norm: 18.43, NNZs: 10485, Bias: -0.240000, T: 9051, Avg. loss: 0.005908
Total training time: 0.05 seconds.
-- Epoch 7
Norm: 18.41, NNZs: 10315, Bias: -0.240000, T: 9051, Avg. loss: 0.006024
Total training time: 0.01 seconds.
-- Epoch 7
-- Epoch 7
Norm: 18.79, NNZs: 5901, Bias: -0.050000, T: 54306, Avg. loss: 0.000148
Total training time: 0.26 seconds.
-- Epoch 2
-- Epoch 2
Norm: 22.42, NNZs: 8160, Bias: -0.120000, T: 45255, Avg. loss: 0.000442
Total training time: 0.25 seconds.
Norm: 24.02, NNZs: 10498, Bias: -0.050000, T: 54312, Avg. loss: 0.000461
Total training time: 0.23 seconds.
-- Epoch 7
-- Epoch 6
-- Epoch 7
-- Epoch 4
Norm: 17.61, NNZs: 9202, Bias: -0.190000, T: 9051, Avg. loss: 0.003545
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 24.06, NNZs: 9403, Bias: -0.230000, T: 63357, Avg. loss: 0.000462
Total training time: 0.24 seconds.
-- Epoch 7
-- Epoch 1
-- Epoch 8
Norm: 20.19, NNZs: 11431, Bias: -0.030000, T: 54306, Avg. loss: 0.000193
Total training time: 0.24 seconds.
-- Epoch 7
-- Epoch 6
Norm: 19.38, NNZs: 7813, Bias: -0.230000, T: 18102, Avg. loss: 0.001269
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 19.66, NNZs: 10431, Bias: -0.180000, T: 63357, Avg. loss: 0.000246
Total training time: 0.27 seconds.
Convergence after 7 epochs took 0.28 seconds
Norm: 21.22, NNZs: 9363, Bias: -0.200000, T: 63364, Avg. loss: 0.000214
Norm: 20.60, NNZs: 6160, Bias: -0.050000, T: 54306, Avg. loss: 0.000239
Total training time: 0.25 seconds.
Total training time: 0.26 seconds.
Convergence after 7 epochs took 0.25 seconds
-- Epoch 7
Norm: 20.55, NNZs: 8138, Bias: -0.250000, T: 45260, Avg. loss: 0.000729
Norm: 20.50, NNZs: 8221, Bias: -0.200000, T: 54306, Avg. loss: 0.000642
Norm: 23.71, NNZs: 10285, Bias: -0.130000, T: 63357, Avg. loss: 0.000654
Total training time: 0.25 seconds.
-- Epoch 2
Norm: 21.51, NNZs: 9328, Bias: -0.160000, T: 72408, Avg. loss: 0.000194
Total training time: 0.24 seconds.
Total training time: 0.26 seconds.
Norm: 20.03, NNZs: 6063, Bias: -0.080000, T: 36208, Avg. loss: 0.000398
Total training time: 0.22 seconds.
-- Epoch 6
Norm: 16.63, NNZs: 8580, Bias: -0.210000, T: 9051, Avg. loss: 0.003400
Total training time: 0.05 seconds.
Convergence after 8 epochs took 0.24 seconds
-- Epoch 5
Norm: 21.92, NNZs: 9245, Bias: -0.280000, T: 18102, Avg. loss: 0.002039
Total training time: 0.14 seconds.
Norm: 19.05, NNZs: 6141, Bias: -0.060000, T: 63357, Avg. loss: 0.000173
Norm: 19.27, NNZs: 10508, Bias: -0.210000, T: 63357, Avg. loss: 0.000230
Total training time: 0.28 seconds.
Norm: 17.66, NNZs: 11314, Bias: -0.180000, T: 9051, Avg. loss: 0.004978
Total training time: 0.28 seconds.
Norm: 20.46, NNZs: 11363, Bias: -0.270000, T: 18102, Avg. loss: 0.001367
Total training time: 0.07 seconds.
-- Epoch 1
Norm: 19.61, NNZs: 7854, Bias: -0.190000, T: 18102, Avg. loss: 0.001189
-- Epoch 3
Convergence after 7 epochs took 0.28 seconds
Total training time: 0.04 seconds.
Norm: 22.94, NNZs: 8305, Bias: -0.050000, T: 54306, Avg. loss: 0.000387
Total training time: 0.27 seconds.
-- Epoch 1
-- Epoch 1
Norm: 21.40, NNZs: 6605, Bias: -0.120000, T: 63357, Avg. loss: 0.000181
Total training time: 0.25 seconds.
Norm: 19.61, NNZs: 7548, Bias: -0.180000, T: 9051, Avg. loss: 0.006643
Convergence after 7 epochs took 0.25 seconds
Total training time: 0.05 seconds.
Norm: 23.55, NNZs: 11406, Bias: -0.080000, T: 72408, Avg. loss: 0.000571
Norm: 17.95, NNZs: 11299, Bias: -0.170000, T: 9051, Avg. loss: 0.004977
Total training time: 0.24 seconds.
Norm: 20.25, NNZs: 8122, Bias: -0.300000, T: 63357, Avg. loss: 0.000102
Total training time: 0.23 seconds.
Norm: 18.04, NNZs: 5433, Bias: -0.150000, T: 45255, Avg. loss: 0.000245
Total training time: 0.28 seconds.
-- Epoch 1
Convergence after 7 epochs took 0.23 seconds
-- Epoch 1
Norm: 20.40, NNZs: 11625, Bias: -0.290000, T: 18102, Avg. loss: 0.001712
Total training time: 0.03 seconds.
-- Epoch 1
Convergence after 8 epochs took 0.25 seconds
Norm: 23.01, NNZs: 8499, Bias: -0.050000, T: 45255, Avg. loss: 0.000669
Total training time: 0.29 seconds.
-- Epoch 6
Convergence after 7 epochs took 0.28 seconds
-- Epoch 3
-- Epoch 6
Norm: 24.44, NNZs: 8897, Bias: -0.080000, T: 63357, Avg. loss: 0.000377
Total training time: 0.24 seconds.
-- Epoch 1
Convergence after 7 epochs took 0.24 seconds
Norm: 23.07, NNZs: 7764, Bias: -0.080000, T: 63357, Avg. loss: 0.000303
-- Epoch 2
Norm: 19.93, NNZs: 8026, Bias: -0.190000, T: 27153, Avg. loss: 0.000758
Total training time: 0.04 seconds.
Norm: 20.09, NNZs: 7438, Bias: -0.240000, T: 63364, Avg. loss: 0.000140
Total training time: 0.23 seconds.
Norm: 19.16, NNZs: 11499, Bias: -0.180000, T: 54312, Avg. loss: 0.000309
Total training time: 0.25 seconds.
Convergence after 7 epochs took 0.23 seconds
-- Epoch 8
-- Epoch 2
Total training time: 0.03 seconds.
Norm: 19.76, NNZs: 15689, Bias: -0.150000, T: 63357, Avg. loss: 0.000215
Total training time: 0.25 seconds.
Total training time: 0.29 seconds.
Total training time: 0.24 seconds.
Convergence after 7 epochs took 0.25 seconds
Norm: 21.01, NNZs: 6252, Bias: -0.030000, T: 63357, Avg. loss: 0.000145
-- Epoch 7
-- Epoch 4
Norm: 15.83, NNZs: 7633, Bias: -0.200000, T: 9051, Avg. loss: 0.003137
Total training time: 0.01 seconds.
Norm: 19.25, NNZs: 9923, Bias: -0.210000, T: 18102, Avg. loss: 0.000921
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 21.96, NNZs: 7767, Bias: -0.100000, T: 63357, Avg. loss: 0.000375
Total training time: 0.29 seconds.
-- Epoch 3
Total training time: 0.27 seconds.
Total training time: 0.09 seconds.
Convergence after 7 epochs took 0.29 seconds
-- Epoch 8
-- Epoch 1
Convergence after 7 epochs took 0.28 seconds
Norm: 20.13, NNZs: 7146, Bias: -0.240000, T: 54306, Avg. loss: 0.000120
Norm: 23.68, NNZs: 9512, Bias: -0.220000, T: 54306, Avg. loss: 0.000345
-- Epoch 2
-- Epoch 1
Norm: 23.62, NNZs: 22081, Bias: -0.080000, T: 63364, Avg. loss: 0.000672
Total training time: 0.27 seconds.
-- Epoch 1
Convergence after 7 epochs took 0.27 seconds
-- Epoch 1
-- Epoch 1
Norm: 20.70, NNZs: 11527, Bias: -0.040000, T: 63357, Avg. loss: 0.000122
Total training time: 0.29 seconds.
Total training time: 0.27 seconds.
Norm: 21.59, NNZs: 7604, Bias: -0.080000, T: 63357, Avg. loss: 0.000343
Total training time: 0.30 seconds.
Norm: 21.69, NNZs: 8451, Bias: -0.240000, T: 18102, Avg. loss: 0.002100
-- Epoch 3
Total training time: 0.06 seconds.
Norm: 15.97, NNZs: 6809, Bias: -0.180000, T: 9051, Avg. loss: 0.003026
-- Epoch 3
Norm: 20.87, NNZs: 8317, Bias: -0.200000, T: 63357, Avg. loss: 0.000492
Total training time: 0.30 seconds.
-- Epoch 3
Convergence after 7 epochs took 0.30 seconds
Convergence after 7 epochs took 0.30 seconds
-- Epoch 3
Norm: 18.09, NNZs: 10025, Bias: -0.230000, T: 9051, Avg. loss: 0.004959
Total training time: 0.01 seconds.
Norm: 23.68, NNZs: 7897, Bias: -0.060000, T: 72408, Avg. loss: 0.000236
Total training time: 0.25 seconds.
Norm: 19.72, NNZs: 16797, Bias: -0.250000, T: 18102, Avg. loss: 0.001379
Total training time: 0.04 seconds.
Norm: 20.57, NNZs: 8370, Bias: -0.170000, T: 36204, Avg. loss: 0.000562
Total training time: 0.05 seconds.
Convergence after 8 epochs took 0.25 seconds
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 5
Norm: 21.51, NNZs: 8341, Bias: -0.230000, T: 18102, Avg. loss: 0.002253
Norm: 18.09, NNZs: 10439, Bias: -0.200000, T: 9051, Avg. loss: 0.005195
Total training time: 0.01 seconds.
Norm: 17.67, NNZs: 7495, Bias: -0.210000, T: 9052, Avg. loss: 0.004826
Total training time: 0.01 seconds.
Norm: 24.32, NNZs: 10599, Bias: -0.020000, T: 63364, Avg. loss: 0.000348
Total training time: 0.26 seconds.
-- Epoch 2
Norm: 18.80, NNZs: 8218, Bias: -0.200000, T: 18104, Avg. loss: 0.001118
Total training time: 0.07 seconds.
-- Epoch 2
-- Epoch 8
-- Epoch 3
-- Epoch 7
Norm: 18.32, NNZs: 6085, Bias: -0.070000, T: 54312, Avg. loss: 0.000260
Total training time: 0.24 seconds.
-- Epoch 7
-- Epoch 2
Norm: 21.56, NNZs: 8740, Bias: -0.270000, T: 18104, Avg. loss: 0.002116
Total training time: 0.09 seconds.
-- Epoch 3
-- Epoch 3
Norm: 21.28, NNZs: 11945, Bias: -0.300000, T: 27153, Avg. loss: 0.000632
Total training time: 0.05 seconds.
Norm: 17.74, NNZs: 8904, Bias: -0.230000, T: 9051, Avg. loss: 0.006987
Total training time: 0.01 seconds.
-- Epoch 4
Norm: 19.96, NNZs: 10765, Bias: -0.290000, T: 18102, Avg. loss: 0.001431
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 1
-- Epoch 7
Norm: 19.37, NNZs: 8892, Bias: -0.190000, T: 27156, Avg. loss: 0.000503
Total training time: 0.08 seconds.
-- Epoch 1
Norm: 24.48, NNZs: 10652, Bias: -0.010000, T: 72416, Avg. loss: 0.000273
Total training time: 0.27 seconds.
Convergence after 8 epochs took 0.27 seconds
Norm: 22.63, NNZs: 7241, Bias: -0.060000, T: 63357, Avg. loss: 0.000308
Norm: 23.23, NNZs: 9341, Bias: -0.140000, T: 63357, Avg. loss: 0.000524
Total training time: 0.28 seconds.
Total training time: 0.30 seconds.
Norm: 21.12, NNZs: 8560, Bias: -0.140000, T: 45255, Avg. loss: 0.000474
Total training time: 0.06 seconds.
Norm: 18.26, NNZs: 5502, Bias: -0.100000, T: 54306, Avg. loss: 0.000182
Total training time: 0.30 seconds.
Norm: 23.64, NNZs: 24819, Bias: -0.030000, T: 63357, Avg. loss: 0.000357
Total training time: 0.31 seconds.
-- Epoch 6
Norm: 19.82, NNZs: 7202, Bias: -0.230000, T: 9051, Avg. loss: 0.006812
Total training time: 0.02 seconds.
-- Epoch 7
-- Epoch 8
-- Epoch 8
-- Epoch 2
Norm: 22.66, NNZs: 9616, Bias: -0.250000, T: 27153, Avg. loss: 0.000681
Total training time: 0.17 seconds.
Norm: 20.21, NNZs: 7205, Bias: -0.250000, T: 63357, Avg. loss: 0.000122
Total training time: 0.28 seconds.
Norm: 20.30, NNZs: 17040, Bias: -0.250000, T: 27153, Avg. loss: 0.000518
Total training time: 0.05 seconds.
-- Epoch 4
Convergence after 7 epochs took 0.28 seconds
-- Epoch 4
Norm: 21.66, NNZs: 12330, Bias: -0.280000, T: 36204, Avg. loss: 0.000326
Norm: 20.70, NNZs: 11058, Bias: -0.280000, T: 27153, Avg. loss: 0.000712
Total training time: 0.03 seconds.
Total training time: 0.05 seconds.
-- Epoch 1
-- Epoch 4
-- Epoch 5
Norm: 21.45, NNZs: 9829, Bias: -0.220000, T: 54306, Avg. loss: 0.000269
Total training time: 0.31 seconds.
Norm: 18.34, NNZs: 9523, Bias: -0.210000, T: 9051, Avg. loss: 0.006013
Total training time: 0.02 seconds.
Norm: 17.42, NNZs: 11118, Bias: -0.210000, T: 9051, Avg. loss: 0.005095
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 7
-- Epoch 2
Norm: 20.07, NNZs: 8102, Bias: -0.230000, T: 63357, Avg. loss: 0.000082
Total training time: 0.25 seconds.
Norm: 21.05, NNZs: 11590, Bias: -0.300000, T: 27153, Avg. loss: 0.000462
Total training time: 0.10 seconds.
Convergence after 7 epochs took 0.25 seconds
-- Epoch 4
Norm: 19.66, NNZs: 16660, Bias: -0.240000, T: 18102, Avg. loss: 0.001264
Total training time: 0.08 seconds.
Norm: 21.49, NNZs: 8088, Bias: -0.230000, T: 18102, Avg. loss: 0.001713
Total training time: 0.03 seconds.
-- Epoch 1
Norm: 18.65, NNZs: 5580, Bias: -0.080000, T: 63357, Avg. loss: 0.000173
Total training time: 0.31 seconds.
Norm: 24.34, NNZs: 25007, Bias: -0.010000, T: 72408, Avg. loss: 0.000454
Total training time: 0.32 seconds.
-- Epoch 3
-- Epoch 3
Convergence after 7 epochs took 0.31 seconds
Norm: 20.65, NNZs: 17274, Bias: -0.250000, T: 36204, Avg. loss: 0.000320
Total training time: 0.06 seconds.
Norm: 19.29, NNZs: 10672, Bias: -0.190000, T: 27153, Avg. loss: 0.000418
Total training time: 0.14 seconds.
-- Epoch 5
-- Epoch 4
Norm: 22.04, NNZs: 12452, Bias: -0.260000, T: 45255, Avg. loss: 0.000321
Total training time: 0.06 seconds.
-- Epoch 6
Norm: 17.50, NNZs: 8779, Bias: -0.190000, T: 18102, Avg. loss: 0.000804
Total training time: 0.04 seconds.
Norm: 19.98, NNZs: 11411, Bias: -0.270000, T: 18102, Avg. loss: 0.001371
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 3
Norm: 21.40, NNZs: 11982, Bias: -0.300000, T: 36204, Avg. loss: 0.000348
Norm: 20.25, NNZs: 8117, Bias: -0.210000, T: 27153, Avg. loss: 0.000441
Total training time: 0.11 seconds.
Total training time: 0.12 seconds.
-- Epoch 5
-- Epoch 4
Total training time: 0.13 seconds.
-- Epoch 1
Norm: 17.46, NNZs: 8575, Bias: -0.240000, T: 18102, Avg. loss: 0.000640
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 19.63, NNZs: 10803, Bias: -0.140000, T: 36204, Avg. loss: 0.000314
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 21.00, NNZs: 11234, Bias: -0.260000, T: 36204, Avg. loss: 0.000428
Total training time: 0.04 seconds.
-- Epoch 5
Norm: 22.24, NNZs: 12504, Bias: -0.250000, T: 54306, Avg. loss: 0.000229
Total training time: 0.07 seconds.
Norm: 20.76, NNZs: 11873, Bias: -0.250000, T: 27153, Avg. loss: 0.000667
-- Epoch 7
Total training time: 0.04 seconds.
-- Epoch 4
Norm: 17.94, NNZs: 9164, Bias: -0.150000, T: 27153, Avg. loss: 0.000333
Total training time: 0.04 seconds.
-- Epoch 7
-- Epoch 4
Norm: 22.97, NNZs: 9731, Bias: -0.170000, T: 36204, Avg. loss: 0.000418
Total training time: 0.18 seconds.
-- Epoch 5
Norm: 20.43, NNZs: 6184, Bias: -0.050000, T: 45260, Avg. loss: 0.000204
Total training time: 0.27 seconds.
Total training time: 0.30 seconds.
Norm: 18.12, NNZs: 8996, Bias: -0.240000, T: 27153, Avg. loss: 0.000327
Total training time: 0.05 seconds.
-- Epoch 6
Convergence after 7 epochs took 0.30 seconds
-- Epoch 1
-- Epoch 4
Norm: 19.14, NNZs: 6028, Bias: -0.040000, T: 63357, Avg. loss: 0.000108
Total training time: 0.32 seconds.
Convergence after 7 epochs took 0.32 seconds
Norm: 19.44, NNZs: 10220, Bias: -0.210000, T: 18102, Avg. loss: 0.001130
Norm: 19.89, NNZs: 10210, Bias: -0.170000, T: 27153, Avg. loss: 0.000445
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 21.19, NNZs: 11306, Bias: -0.240000, T: 45255, Avg. loss: 0.000345
Total training time: 0.04 seconds.
Norm: 22.26, NNZs: 12507, Bias: -0.200000, T: 63357, Avg. loss: 0.000185
Total training time: 0.07 seconds.
-- Epoch 6
-- Epoch 8
Norm: 18.30, NNZs: 9361, Bias: -0.140000, T: 36204, Avg. loss: 0.000242
Total training time: 0.05 seconds.
-- Epoch 5
Norm: 20.58, NNZs: 8256, Bias: -0.180000, T: 36204, Avg. loss: 0.000294
Total training time: 0.13 seconds.
-- Epoch 8
Norm: 21.75, NNZs: 6612, Bias: -0.010000, T: 63357, Avg. loss: 0.000102
Norm: 20.77, NNZs: 8185, Bias: -0.230000, T: 54312, Avg. loss: 0.000506
Norm: 20.71, NNZs: 8671, Bias: -0.220000, T: 54306, Avg. loss: 0.000474
Total training time: 0.29 seconds.
-- Epoch 7
Total training time: 0.08 seconds.
Norm: 23.13, NNZs: 25199, Bias: -0.090000, T: 63357, Avg. loss: 0.000616
Total training time: 0.33 seconds.
Norm: 23.44, NNZs: 8576, Bias: -0.010000, T: 54306, Avg. loss: 0.000385
Total training time: 0.31 seconds.
Total training time: 0.33 seconds.
Convergence after 8 epochs took 0.33 seconds
Norm: 15.90, NNZs: 7958, Bias: -0.220000, T: 9052, Avg. loss: 0.003125
Total training time: 0.04 seconds.
-- Epoch 7
-- Epoch 2
-- Epoch 7
Norm: 22.16, NNZs: 8313, Bias: -0.230000, T: 27153, Avg. loss: 0.000694
Total training time: 0.05 seconds.
-- Epoch 1
-- Epoch 4
Norm: 22.30, NNZs: 8955, Bias: -0.210000, T: 27156, Avg. loss: 0.000885
Total training time: 0.13 seconds.
Norm: 18.43, NNZs: 8912, Bias: -0.230000, T: 9052, Avg. loss: 0.005823
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 21.29, NNZs: 11370, Bias: -0.200000, T: 54306, Avg. loss: 0.000276
Total training time: 0.05 seconds.
-- Epoch 2
-- Epoch 7
Norm: 22.42, NNZs: 12567, Bias: -0.190000, T: 72408, Avg. loss: 0.000186
Total training time: 0.08 seconds.
Convergence after 8 epochs took 0.08 seconds
Norm: 23.45, NNZs: 9379, Bias: -0.120000, T: 72408, Avg. loss: 0.000523
Total training time: 0.33 seconds.
Norm: 20.83, NNZs: 17387, Bias: -0.190000, T: 45255, Avg. loss: 0.000304
Total training time: 0.08 seconds.
Norm: 20.86, NNZs: 8761, Bias: -0.190000, T: 63357, Avg. loss: 0.000385
Total training time: 0.30 seconds.
Convergence after 8 epochs took 0.33 seconds
-- Epoch 6
Convergence after 7 epochs took 0.30 seconds
Total training time: 0.34 seconds.
Norm: 21.58, NNZs: 8715, Bias: -0.120000, T: 54306, Avg. loss: 0.000355
Total training time: 0.10 seconds.
Norm: 19.51, NNZs: 10044, Bias: -0.260000, T: 18102, Avg. loss: 0.001868
Total training time: 0.05 seconds.
-- Epoch 1
-- Epoch 3
-- Epoch 7
-- Epoch 1
Norm: 21.55, NNZs: 11487, Bias: -0.180000, T: 63357, Avg. loss: 0.000268
Total training time: 0.06 seconds.
Convergence after 7 epochs took 0.06 seconds
Norm: 21.58, NNZs: 9858, Bias: -0.220000, T: 63357, Avg. loss: 0.000199
Total training time: 0.35 seconds.
Convergence after 7 epochs took 0.35 seconds
Norm: 17.75, NNZs: 8668, Bias: -0.180000, T: 9051, Avg. loss: 0.006306
Norm: 24.19, NNZs: 9439, Bias: -0.220000, T: 72408, Avg. loss: 0.000484
Total training time: 0.31 seconds.
Norm: 22.34, NNZs: 8648, Bias: -0.230000, T: 27153, Avg. loss: 0.000678
Total training time: 0.11 seconds.
Norm: 20.25, NNZs: 16889, Bias: -0.230000, T: 27153, Avg. loss: 0.000519
Total training time: 0.11 seconds.
Total training time: 0.01 seconds.
Norm: 18.49, NNZs: 10872, Bias: -0.210000, T: 9051, Avg. loss: 0.005154
-- Epoch 4
-- Epoch 4
Norm: 18.02, NNZs: 9360, Bias: -0.210000, T: 18102, Avg. loss: 0.001034
Total training time: 0.12 seconds.
-- Epoch 2
Norm: 21.83, NNZs: 8842, Bias: -0.100000, T: 63357, Avg. loss: 0.000253
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 20.22, NNZs: 10426, Bias: -0.260000, T: 27153, Avg. loss: 0.000744
Total training time: 0.05 seconds.
Norm: 19.44, NNZs: 8434, Bias: -0.240000, T: 18104, Avg. loss: 0.001221
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 15.80, NNZs: 7311, Bias: -0.180000, T: 9051, Avg. loss: 0.002894
Total training time: 0.05 seconds.
-- Epoch 3
Convergence after 8 epochs took 0.31 seconds
-- Epoch 3
Norm: 17.79, NNZs: 9856, Bias: -0.250000, T: 9051, Avg. loss: 0.006062
Total training time: 0.05 seconds.
Norm: 16.86, NNZs: 8423, Bias: -0.230000, T: 9051, Avg. loss: 0.003261
Total training time: 0.04 seconds.
-- Epoch 4
-- Epoch 2
-- Epoch 2
-- Epoch 2
Norm: 18.18, NNZs: 10100, Bias: -0.240000, T: 9052, Avg. loss: 0.006587
Total training time: 0.03 seconds.
Norm: 17.81, NNZs: 9649, Bias: -0.240000, T: 9051, Avg. loss: 0.006158
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 2
Norm: 18.59, NNZs: 6376, Bias: -0.030000, T: 63364, Avg. loss: 0.000144
Norm: 17.55, NNZs: 7459, Bias: -0.180000, T: 9052, Avg. loss: 0.003899
Total training time: 0.08 seconds.
Norm: 22.65, NNZs: 9032, Bias: -0.190000, T: 36208, Avg. loss: 0.000572
Total training time: 0.14 seconds.
Convergence after 7 epochs took 0.10 seconds
Norm: 19.39, NNZs: 11577, Bias: -0.160000, T: 63364, Avg. loss: 0.000228
Total training time: 0.31 seconds.
-- Epoch 2
Total training time: 0.29 seconds.
-- Epoch 5
Convergence after 7 epochs took 0.31 seconds
Norm: 23.93, NNZs: 8702, Bias: -0.030000, T: 63357, Avg. loss: 0.000282
Total training time: 0.35 seconds.
Norm: 20.94, NNZs: 6296, Bias: -0.030000, T: 54312, Avg. loss: 0.000180
Norm: 17.55, NNZs: 9338, Bias: -0.190000, T: 18104, Avg. loss: 0.000938
-- Epoch 8
Norm: 18.66, NNZs: 9632, Bias: -0.160000, T: 27153, Avg. loss: 0.000536
Total training time: 0.12 seconds.
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 23.92, NNZs: 10334, Bias: -0.110000, T: 72408, Avg. loss: 0.000639
Total training time: 0.32 seconds.
-- Epoch 3
Convergence after 8 epochs took 0.33 seconds
Norm: 20.03, NNZs: 8705, Bias: -0.200000, T: 27156, Avg. loss: 0.000426
Norm: 23.93, NNZs: 9574, Bias: -0.220000, T: 63357, Avg. loss: 0.000288
Total training time: 0.35 seconds.
Total training time: 0.07 seconds.
Convergence after 7 epochs took 0.35 seconds
-- Epoch 4
-- Epoch 8
Convergence after 7 epochs took 0.36 seconds
Norm: 20.03, NNZs: 11024, Bias: -0.280000, T: 18102, Avg. loss: 0.001769
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 20.22, NNZs: 10528, Bias: -0.190000, T: 27153, Avg. loss: 0.000686
Total training time: 0.10 seconds.
-- Epoch 4
Norm: 19.65, NNZs: 8992, Bias: -0.180000, T: 36208, Avg. loss: 0.000326
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 19.29, NNZs: 8202, Bias: -0.220000, T: 18104, Avg. loss: 0.001181
Total training time: 0.09 seconds.
Norm: 22.94, NNZs: 9073, Bias: -0.170000, T: 45260, Avg. loss: 0.000505
Total training time: 0.15 seconds.
-- Epoch 3
-- Epoch 6
Norm: 17.33, NNZs: 8126, Bias: -0.170000, T: 18102, Avg. loss: 0.000855
Total training time: 0.06 seconds.
-- Epoch 3
Convergence after 7 epochs took 0.30 seconds
Total training time: 0.04 seconds.
Norm: 23.41, NNZs: 10067, Bias: -0.160000, T: 45255, Avg. loss: 0.000392
Norm: 19.23, NNZs: 10561, Bias: -0.150000, T: 36204, Avg. loss: 0.000444
Total training time: 0.13 seconds.
Norm: 18.03, NNZs: 9481, Bias: -0.170000, T: 27156, Avg. loss: 0.000376
-- Epoch 2
-- Epoch 5
Norm: 20.86, NNZs: 11402, Bias: -0.310000, T: 27153, Avg. loss: 0.000640
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 20.69, NNZs: 10647, Bias: -0.150000, T: 36204, Avg. loss: 0.000344
Total training time: 0.11 seconds.
-- Epoch 5
-- Epoch 7
Total training time: 0.22 seconds.
Norm: 22.08, NNZs: 8623, Bias: -0.200000, T: 27153, Avg. loss: 0.000831
Total training time: 0.17 seconds.
-- Epoch 5
-- Epoch 6
-- Epoch 4
Norm: 21.65, NNZs: 12192, Bias: -0.280000, T: 45255, Avg. loss: 0.000302
Total training time: 0.15 seconds.
Norm: 20.04, NNZs: 10514, Bias: -0.270000, T: 18102, Avg. loss: 0.001710
Total training time: 0.07 seconds.
-- Epoch 6
-- Epoch 3
Norm: 19.69, NNZs: 9943, Bias: -0.250000, T: 18102, Avg. loss: 0.001800
Total training time: 0.03 seconds.
Norm: 20.97, NNZs: 17479, Bias: -0.160000, T: 54306, Avg. loss: 0.000235
Total training time: 0.11 seconds.
Norm: 23.05, NNZs: 7384, Bias: -0.060000, T: 72408, Avg. loss: 0.000296
Total training time: 0.34 seconds.
-- Epoch 7
-- Epoch 3
Norm: 20.65, NNZs: 17482, Bias: -0.190000, T: 36204, Avg. loss: 0.000374
Total training time: 0.13 seconds.
Norm: 19.84, NNZs: 10703, Bias: -0.130000, T: 45255, Avg. loss: 0.000296
Total training time: 0.14 seconds.
Total training time: 0.07 seconds.
Norm: 20.15, NNZs: 10918, Bias: -0.140000, T: 45255, Avg. loss: 0.000325
Total training time: 0.19 seconds.
-- Epoch 6
Norm: 18.89, NNZs: 9462, Bias: -0.230000, T: 18102, Avg. loss: 0.001008
-- Epoch 6
Total training time: 0.31 seconds.
-- Epoch 1
-- Epoch 5
Norm: 21.18, NNZs: 12011, Bias: -0.230000, T: 36204, Avg. loss: 0.000428
-- Epoch 7
Total training time: 0.08 seconds.
Norm: 21.21, NNZs: 11476, Bias: -0.290000, T: 36204, Avg. loss: 0.000383
Total training time: 0.07 seconds.
-- Epoch 5
-- Epoch 5
Norm: 20.98, NNZs: 10733, Bias: -0.130000, T: 45255, Avg. loss: 0.000282
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 4
Norm: 21.85, NNZs: 12229, Bias: -0.270000, T: 54306, Avg. loss: 0.000234
Total training time: 0.16 seconds.
Norm: 18.75, NNZs: 9499, Bias: -0.120000, T: 45255, Avg. loss: 0.000229
Total training time: 0.09 seconds.
Norm: 20.65, NNZs: 10740, Bias: -0.220000, T: 27153, Avg. loss: 0.000619
Total training time: 0.08 seconds.
-- Epoch 7
-- Epoch 6
-- Epoch 4
Norm: 17.99, NNZs: 8886, Bias: -0.150000, T: 27153, Avg. loss: 0.000416
Total training time: 0.08 seconds.
-- Epoch 4
Norm: 20.92, NNZs: 8285, Bias: -0.220000, T: 63364, Avg. loss: 0.000348
Total training time: 0.35 seconds.
Norm: 18.34, NNZs: 9078, Bias: -0.230000, T: 36204, Avg. loss: 0.000257
Total training time: 0.09 seconds.
Norm: 20.47, NNZs: 12224, Bias: -0.300000, T: 18102, Avg. loss: 0.001561
Total training time: 0.06 seconds.
Convergence after 7 epochs took 0.35 seconds
-- Epoch 5
Norm: 20.28, NNZs: 9016, Bias: -0.170000, T: 36208, Avg. loss: 0.000305
Total training time: 0.09 seconds.
-- Epoch 3
-- Epoch 5
Norm: 20.44, NNZs: 10113, Bias: -0.290000, T: 18104, Avg. loss: 0.001491
Total training time: 0.10 seconds.
Norm: 21.63, NNZs: 11600, Bias: -0.300000, T: 45255, Avg. loss: 0.000363
Total training time: 0.08 seconds.
Norm: 19.25, NNZs: 12113, Bias: -0.220000, T: 18102, Avg. loss: 0.001190
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 19.99, NNZs: 8503, Bias: -0.240000, T: 27156, Avg. loss: 0.000556
Total training time: 0.10 seconds.
-- Epoch 6
Norm: 20.74, NNZs: 10692, Bias: -0.210000, T: 36204, Avg. loss: 0.000511
-- Epoch 3
Total training time: 0.08 seconds.
-- Epoch 4
Norm: 22.74, NNZs: 9005, Bias: -0.220000, T: 36204, Avg. loss: 0.000603
Total training time: 0.14 seconds.
-- Epoch 5
-- Epoch 5
Norm: 20.51, NNZs: 10241, Bias: -0.230000, T: 27153, Avg. loss: 0.000700
Total training time: 0.04 seconds.
Norm: 23.75, NNZs: 10263, Bias: -0.120000, T: 54306, Avg. loss: 0.000314
Total training time: 0.24 seconds.
-- Epoch 4
Norm: 18.04, NNZs: 10132, Bias: -0.230000, T: 9052, Avg. loss: 0.005168
Total training time: 0.08 seconds.
-- Epoch 2
-- Epoch 7
Norm: 20.29, NNZs: 10332, Bias: -0.130000, T: 36204, Avg. loss: 0.000349
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 18.49, NNZs: 9112, Bias: -0.200000, T: 45255, Avg. loss: 0.000220
Total training time: 0.10 seconds.
-- Epoch 6
Norm: 20.56, NNZs: 9142, Bias: -0.180000, T: 45260, Avg. loss: 0.000245
Total training time: 0.09 seconds.
Norm: 20.36, NNZs: 10911, Bias: -0.100000, T: 54306, Avg. loss: 0.000388
Total training time: 0.15 seconds.
-- Epoch 6
-- Epoch 7
-- Epoch 1
Norm: 21.81, NNZs: 11648, Bias: -0.270000, T: 54306, Avg. loss: 0.000293
Total training time: 0.09 seconds.
Norm: 21.29, NNZs: 10605, Bias: -0.270000, T: 27156, Avg. loss: 0.000621
-- Epoch 7
Total training time: 0.10 seconds.
Norm: 19.82, NNZs: 10620, Bias: -0.280000, T: 18104, Avg. loss: 0.001655
Total training time: 0.06 seconds.
Norm: 20.42, NNZs: 8684, Bias: -0.210000, T: 36208, Avg. loss: 0.000396
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 5
Norm: 23.00, NNZs: 9260, Bias: -0.170000, T: 45255, Avg. loss: 0.000572
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 21.08, NNZs: 10615, Bias: -0.250000, T: 36204, Avg. loss: 0.000535
Total training time: 0.05 seconds.
-- Epoch 5
Norm: 19.61, NNZs: 11084, Bias: -0.290000, T: 18104, Avg. loss: 0.001315
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 23.22, NNZs: 9177, Bias: -0.130000, T: 54312, Avg. loss: 0.000411
Total training time: 0.18 seconds.
-- Epoch 7
Norm: 22.65, NNZs: 8804, Bias: -0.180000, T: 36204, Avg. loss: 0.000698
Total training time: 0.20 seconds.
-- Epoch 5
Norm: 20.65, NNZs: 10957, Bias: -0.090000, T: 63357, Avg. loss: 0.000206
Total training time: 0.16 seconds.
Norm: 21.09, NNZs: 10865, Bias: -0.210000, T: 45255, Avg. loss: 0.000375
Total training time: 0.09 seconds.
Convergence after 7 epochs took 0.16 seconds
Norm: 19.88, NNZs: 9044, Bias: -0.120000, T: 45260, Avg. loss: 0.000325
Total training time: 0.16 seconds.
Norm: 21.19, NNZs: 17602, Bias: -0.170000, T: 63357, Avg. loss: 0.000207
Total training time: 0.13 seconds.
-- Epoch 6
Convergence after 7 epochs took 0.13 seconds
Norm: 21.99, NNZs: 11713, Bias: -0.230000, T: 63357, Avg. loss: 0.000240
Total training time: 0.10 seconds.
Norm: 18.43, NNZs: 9592, Bias: -0.140000, T: 36208, Avg. loss: 0.000270
Total training time: 0.10 seconds.
Norm: 20.26, NNZs: 10782, Bias: -0.220000, T: 27156, Avg. loss: 0.000692
Total training time: 0.07 seconds.
Norm: 20.01, NNZs: 11148, Bias: -0.320000, T: 18102, Avg. loss: 0.001804
Total training time: 0.05 seconds.
-- Epoch 8
-- Epoch 4
-- Epoch 5
-- Epoch 3
Norm: 22.63, NNZs: 8446, Bias: -0.170000, T: 36204, Avg. loss: 0.000592
Total training time: 0.11 seconds.
Norm: 24.06, NNZs: 9607, Bias: -0.170000, T: 72408, Avg. loss: 0.000209
Total training time: 0.39 seconds.
-- Epoch 5
Norm: 18.46, NNZs: 7654, Bias: -0.190000, T: 9051, Avg. loss: 0.004254
Total training time: 0.08 seconds.
Convergence after 8 epochs took 0.39 seconds
Norm: 20.72, NNZs: 10492, Bias: -0.120000, T: 45255, Avg. loss: 0.000281
Total training time: 0.14 seconds.
Norm: 22.03, NNZs: 12316, Bias: -0.240000, T: 63357, Avg. loss: 0.000215
Total training time: 0.18 seconds.
-- Epoch 6
Convergence after 7 epochs took 0.18 seconds
-- Epoch 2
Norm: 21.39, NNZs: 6405, Bias: -0.040000, T: 63364, Avg. loss: 0.000127
Total training time: 0.33 seconds.
Norm: 20.96, NNZs: 8456, Bias: -0.170000, T: 45255, Avg. loss: 0.000307
Total training time: 0.19 seconds.
Convergence after 7 epochs took 0.33 seconds
-- Epoch 6
-- Epoch 6
Norm: 20.51, NNZs: 11004, Bias: -0.090000, T: 54306, Avg. loss: 0.000203
Total training time: 0.21 seconds.
Norm: 19.90, NNZs: 12486, Bias: -0.240000, T: 27153, Avg. loss: 0.000502
Total training time: 0.09 seconds.
-- Epoch 7
-- Epoch 4
Norm: 23.45, NNZs: 8388, Bias: -0.020000, T: 63357, Avg. loss: 0.000227
Total training time: 0.38 seconds.
Norm: 21.11, NNZs: 12944, Bias: -0.290000, T: 27153, Avg. loss: 0.000528
Total training time: 0.08 seconds.
Norm: 18.02, NNZs: 9056, Bias: -0.200000, T: 9051, Avg. loss: 0.005948
Total training time: 0.01 seconds.
-- Epoch 8
-- Epoch 4
-- Epoch 2
Norm: 22.05, NNZs: 11738, Bias: -0.210000, T: 72408, Avg. loss: 0.000229
Total training time: 0.10 seconds.
Convergence after 8 epochs took 0.10 seconds
Norm: 19.01, NNZs: 9579, Bias: -0.080000, T: 54306, Avg. loss: 0.000182
Total training time: 0.12 seconds.
Total training time: 0.09 seconds.
Convergence after 8 epochs took 0.37 seconds
-- Epoch 7
-- Epoch 6
Norm: 23.58, NNZs: 9279, Bias: -0.120000, T: 63364, Avg. loss: 0.000421
Total training time: 0.19 seconds.
-- Epoch 8
Norm: 21.54, NNZs: 10675, Bias: -0.270000, T: 36208, Avg. loss: 0.000369
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 20.98, NNZs: 11092, Bias: -0.060000, T: 63357, Avg. loss: 0.000235
Total training time: 0.22 seconds.
Convergence after 7 epochs took 0.22 seconds
Norm: 17.50, NNZs: 11602, Bias: -0.200000, T: 9051, Avg. loss: 0.004562
Total training time: 0.06 seconds.
Norm: 20.75, NNZs: 8742, Bias: -0.160000, T: 45260, Avg. loss: 0.000310
Total training time: 0.13 seconds.
-- Epoch 6
-- Epoch 1
Norm: 21.34, NNZs: 12126, Bias: -0.210000, T: 45255, Avg. loss: 0.000346
Total training time: 0.12 seconds.
Norm: 20.94, NNZs: 10547, Bias: -0.090000, T: 54306, Avg. loss: 0.000199
Total training time: 0.15 seconds.
-- Epoch 6
-- Epoch 7
Norm: 23.02, NNZs: 8947, Bias: -0.160000, T: 45255, Avg. loss: 0.000626
Total training time: 0.21 seconds.
Norm: 21.47, NNZs: 13088, Bias: -0.250000, T: 36204, Avg. loss: 0.000387
Norm: 20.85, NNZs: 9226, Bias: -0.170000, T: 54312, Avg. loss: 0.000243
Total training time: 0.12 seconds.
-- Epoch 6
-- Epoch 7
Norm: 21.43, NNZs: 10742, Bias: -0.210000, T: 45255, Avg. loss: 0.000389
Total training time: 0.07 seconds.
Norm: 20.96, NNZs: 8856, Bias: -0.140000, T: 54312, Avg. loss: 0.000240
Total training time: 0.14 seconds.
-- Epoch 6
-- Epoch 7
Norm: 18.47, NNZs: 9005, Bias: -0.110000, T: 36204, Avg. loss: 0.000328
Total training time: 0.11 seconds.
-- Epoch 5
Norm: 21.06, NNZs: 10879, Bias: -0.200000, T: 36204, Avg. loss: 0.000436
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 20.40, NNZs: 11400, Bias: -0.270000, T: 27156, Avg. loss: 0.000611
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 20.91, NNZs: 17585, Bias: -0.150000, T: 45255, Avg. loss: 0.000324
Total training time: 0.18 seconds.
Norm: 20.97, NNZs: 9244, Bias: -0.130000, T: 63364, Avg. loss: 0.000187
Total training time: 0.12 seconds.
-- Epoch 6
Convergence after 7 epochs took 0.13 seconds
Norm: 21.25, NNZs: 10599, Bias: -0.070000, T: 63357, Avg. loss: 0.000147
Total training time: 0.16 seconds.
Norm: 18.61, NNZs: 9316, Bias: -0.150000, T: 54306, Avg. loss: 0.000179
Total training time: 0.13 seconds.
Convergence after 7 epochs took 0.16 seconds
-- Epoch 7
Norm: 23.78, NNZs: 8560, Bias: -0.020000, T: 72408, Avg. loss: 0.000323
Total training time: 0.40 seconds.
Norm: 18.72, NNZs: 9895, Bias: -0.140000, T: 45260, Avg. loss: 0.000245
Total training time: 0.12 seconds.
-- Epoch 6
Convergence after 8 epochs took 0.40 seconds
Norm: 23.88, NNZs: 9346, Bias: -0.100000, T: 72416, Avg. loss: 0.000422
Total training time: 0.20 seconds.
Norm: 21.10, NNZs: 8899, Bias: -0.100000, T: 63364, Avg. loss: 0.000180
Total training time: 0.14 seconds.
-- Epoch 3
Convergence after 7 epochs took 0.14 seconds
Norm: 21.68, NNZs: 10812, Bias: -0.180000, T: 54306, Avg. loss: 0.000321
Total training time: 0.08 seconds.
Norm: 17.91, NNZs: 8681, Bias: -0.240000, T: 9051, Avg. loss: 0.004839
Total training time: 0.05 seconds.
-- Epoch 7
Norm: 19.92, NNZs: 8210, Bias: -0.210000, T: 18102, Avg. loss: 0.001044
Total training time: 0.10 seconds.
-- Epoch 2
-- Epoch 3
Norm: 18.88, NNZs: 9115, Bias: -0.100000, T: 45255, Avg. loss: 0.000243
Total training time: 0.12 seconds.
Norm: 19.49, NNZs: 9745, Bias: -0.090000, T: 63357, Avg. loss: 0.000150
Total training time: 0.13 seconds.
-- Epoch 6
Convergence after 7 epochs took 0.13 seconds
Norm: 21.24, NNZs: 17700, Bias: -0.120000, T: 54306, Avg. loss: 0.000272
Total training time: 0.18 seconds.
Norm: 23.99, NNZs: 10335, Bias: -0.110000, T: 63357, Avg. loss: 0.000207
Total training time: 0.28 seconds.
Total training time: 0.10 seconds.
-- Epoch 7
-- Epoch 8
-- Epoch 5
Norm: 21.33, NNZs: 10814, Bias: -0.120000, T: 54306, Avg. loss: 0.000254
Total training time: 0.16 seconds.
-- Epoch 2
Norm: 20.19, NNZs: 12606, Bias: -0.200000, T: 36204, Avg. loss: 0.000407
Total training time: 0.11 seconds.
-- Epoch 7
-- Epoch 5
Norm: 21.20, NNZs: 8656, Bias: -0.140000, T: 54306, Avg. loss: 0.000242
Total training time: 0.22 seconds.
Norm: 19.06, NNZs: 9980, Bias: -0.100000, T: 54312, Avg. loss: 0.000188
Total training time: 0.13 seconds.
-- Epoch 7
-- Epoch 7
Norm: 21.61, NNZs: 12228, Bias: -0.160000, T: 54306, Avg. loss: 0.000287
Total training time: 0.13 seconds.
Norm: 19.70, NNZs: 9847, Bias: -0.230000, T: 27153, Avg. loss: 0.000625
Total training time: 0.11 seconds.
Norm: 21.75, NNZs: 10719, Bias: -0.220000, T: 45260, Avg. loss: 0.000302
Total training time: 0.14 seconds.
-- Epoch 7
-- Epoch 4
-- Epoch 6
Norm: 20.30, NNZs: 8342, Bias: -0.210000, T: 27153, Avg. loss: 0.000441
Total training time: 0.11 seconds.
Norm: 23.17, NNZs: 8696, Bias: -0.160000, T: 45255, Avg. loss: 0.000540
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 21.40, NNZs: 10982, Bias: -0.180000, T: 45255, Avg. loss: 0.000378
Total training time: 0.13 seconds.
Norm: 24.33, NNZs: 10390, Bias: -0.080000, T: 72408, Avg. loss: 0.000191
Total training time: 0.28 seconds.
-- Epoch 6
Norm: 21.57, NNZs: 17771, Bias: -0.130000, T: 63357, Avg. loss: 0.000228
Total training time: 0.19 seconds.
Norm: 21.61, NNZs: 13149, Bias: -0.230000, T: 45255, Avg. loss: 0.000293
Total training time: 0.11 seconds.
Norm: 21.38, NNZs: 11016, Bias: -0.160000, T: 54306, Avg. loss: 0.000314
Total training time: 0.13 seconds.
Convergence after 8 epochs took 0.28 seconds
Convergence after 7 epochs took 0.19 seconds
-- Epoch 6
-- Epoch 7
Norm: 19.50, NNZs: 16714, Bias: -0.260000, T: 18102, Avg. loss: 0.001065
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 24.18, NNZs: 8754, Bias: -0.020000, T: 72408, Avg. loss: 0.000261
Total training time: 0.43 seconds.
Norm: 21.45, NNZs: 8780, Bias: -0.120000, T: 63357, Avg. loss: 0.000209
Total training time: 0.22 seconds.
Convergence after 8 epochs took 0.43 seconds
Convergence after 7 epochs took 0.22 seconds
Norm: 19.48, NNZs: 10094, Bias: -0.090000, T: 63364, Avg. loss: 0.000224
Total training time: 0.13 seconds.
Norm: 21.89, NNZs: 12354, Bias: -0.170000, T: 63357, Avg. loss: 0.000266
Total training time: 0.14 seconds.
Convergence after 7 epochs took 0.13 seconds
Convergence after 7 epochs took 0.14 seconds
Norm: 20.61, NNZs: 10926, Bias: -0.190000, T: 36208, Avg. loss: 0.000415
Total training time: 0.11 seconds.
Convergence after 8 epochs took 0.22 seconds
-- Epoch 5
Norm: 23.21, NNZs: 9348, Bias: -0.150000, T: 54306, Avg. loss: 0.000456
Total training time: 0.19 seconds.
-- Epoch 7
Norm: 20.75, NNZs: 11739, Bias: -0.250000, T: 36208, Avg. loss: 0.000440
Total training time: 0.13 seconds.
Norm: 23.51, NNZs: 9122, Bias: -0.120000, T: 54306, Avg. loss: 0.000548
Total training time: 0.24 seconds.
Norm: 20.04, NNZs: 9136, Bias: -0.100000, T: 54312, Avg. loss: 0.000289
Total training time: 0.20 seconds.
-- Epoch 5
-- Epoch 7
-- Epoch 4
Norm: 21.97, NNZs: 10849, Bias: -0.210000, T: 54312, Avg. loss: 0.000282
Total training time: 0.15 seconds.
-- Epoch 7
-- Epoch 7
Norm: 20.73, NNZs: 11392, Bias: -0.320000, T: 27153, Avg. loss: 0.000549
Norm: 21.74, NNZs: 13239, Bias: -0.200000, T: 54306, Avg. loss: 0.000257
Total training time: 0.12 seconds.
Norm: 21.70, NNZs: 11080, Bias: -0.150000, T: 63357, Avg. loss: 0.000272
Total training time: 0.14 seconds.
-- Epoch 7
-- Epoch 8
Norm: 23.54, NNZs: 9425, Bias: -0.140000, T: 63357, Avg. loss: 0.000503
Total training time: 0.20 seconds.
Norm: 20.30, NNZs: 9255, Bias: -0.070000, T: 63364, Avg. loss: 0.000249
Total training time: 0.21 seconds.
Norm: 22.24, NNZs: 11029, Bias: -0.200000, T: 63364, Avg. loss: 0.000261
Total training time: 0.16 seconds.
-- Epoch 8
Convergence after 7 epochs took 0.16 seconds
Convergence after 7 epochs took 0.21 seconds
Norm: 18.86, NNZs: 9429, Bias: -0.170000, T: 63357, Avg. loss: 0.000184
Total training time: 0.15 seconds.
Total training time: 0.09 seconds.
Norm: 21.01, NNZs: 11806, Bias: -0.220000, T: 45260, Avg. loss: 0.000374
Total training time: 0.14 seconds.
-- Epoch 6
Convergence after 7 epochs took 0.15 seconds
Norm: 19.74, NNZs: 10681, Bias: -0.300000, T: 18102, Avg. loss: 0.001265
Total training time: 0.07 seconds.
Norm: 21.69, NNZs: 11045, Bias: -0.150000, T: 54306, Avg. loss: 0.000311
Total training time: 0.14 seconds.
Norm: 23.83, NNZs: 9168, Bias: -0.120000, T: 63357, Avg. loss: 0.000447
Total training time: 0.24 seconds.
-- Epoch 3
-- Epoch 7
Norm: 21.00, NNZs: 10982, Bias: -0.160000, T: 45260, Avg. loss: 0.000395
Total training time: 0.11 seconds.
Norm: 20.08, NNZs: 10031, Bias: -0.180000, T: 36204, Avg. loss: 0.000355
Total training time: 0.13 seconds.
-- Epoch 8
Norm: 20.12, NNZs: 10238, Bias: -0.300000, T: 18102, Avg. loss: 0.001770
Total training time: 0.05 seconds.
-- Epoch 6
Norm: 21.91, NNZs: 10852, Bias: -0.160000, T: 63357, Avg. loss: 0.000264
Total training time: 0.10 seconds.
-- Epoch 5
-- Epoch 3
-- Epoch 8
Norm: 21.89, NNZs: 11109, Bias: -0.110000, T: 72408, Avg. loss: 0.000227
Total training time: 0.14 seconds.
Norm: 16.31, NNZs: 7649, Bias: -0.180000, T: 9051, Avg. loss: 0.003317
Total training time: 0.03 seconds.
Convergence after 8 epochs took 0.14 seconds
-- Epoch 2
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.4s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.4s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.4s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.4s finished
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished
Norm: 19.28, NNZs: 9616, Bias: -0.080000, T: 54306, Avg. loss: 0.000187
Total training time: 0.14 seconds.
Norm: 23.73, NNZs: 9521, Bias: -0.110000, T: 72408, Avg. loss: 0.000443
Total training time: 0.21 seconds.
-- Epoch 7
Convergence after 8 epochs took 0.21 seconds
Norm: 20.55, NNZs: 12756, Bias: -0.160000, T: 45255, Avg. loss: 0.000336
Total training time: 0.14 seconds.
Norm: 20.17, NNZs: 17055, Bias: -0.250000, T: 27153, Avg. loss: 0.000476
Total training time: 0.10 seconds.
-- Epoch 4
-- Epoch 6
Norm: 21.29, NNZs: 11876, Bias: -0.230000, T: 54312, Avg. loss: 0.000340
-- Epoch 4
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 20.69, NNZs: 11065, Bias: -0.310000, T: 27153, Avg. loss: 0.000579
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 20.39, NNZs: 10243, Bias: -0.170000, T: 45255, Avg. loss: 0.000283
Norm: 21.25, NNZs: 11047, Bias: -0.140000, T: 54312, Avg. loss: 0.000283
Total training time: 0.14 seconds.
Total training time: 0.12 seconds.
Norm: 22.08, NNZs: 11077, Bias: -0.140000, T: 63357, Avg. loss: 0.000269
Total training time: 0.15 seconds.
-- Epoch 7
-- Epoch 6
-- Epoch 8
Norm: 17.81, NNZs: 8668, Bias: -0.200000, T: 18102, Avg. loss: 0.000831
Total training time: 0.04 seconds.
Norm: 22.02, NNZs: 13320, Bias: -0.180000, T: 63357, Avg. loss: 0.000266
Total training time: 0.13 seconds.
-- Epoch 3
-- Epoch 8
Norm: 21.61, NNZs: 10907, Bias: -0.080000, T: 63357, Avg. loss: 0.000167
Total training time: 0.19 seconds.
Norm: 20.86, NNZs: 8517, Bias: -0.210000, T: 36204, Avg. loss: 0.000350
Total training time: 0.13 seconds.
Convergence after 7 epochs took 0.19 seconds
-- Epoch 5
Norm: 21.19, NNZs: 11640, Bias: -0.330000, T: 36204, Avg. loss: 0.000356
Total training time: 0.11 seconds.
Norm: 20.77, NNZs: 12827, Bias: -0.140000, T: 54306, Avg. loss: 0.000270
Total training time: 0.14 seconds.
Norm: 20.38, NNZs: 17153, Bias: -0.230000, T: 36204, Avg. loss: 0.000300
Total training time: 0.11 seconds.
-- Epoch 5
-- Epoch 7
-- Epoch 5
Norm: 21.44, NNZs: 11910, Bias: -0.210000, T: 63364, Avg. loss: 0.000303
Total training time: 0.15 seconds.
Convergence after 7 epochs took 0.15 seconds
Norm: 20.77, NNZs: 10339, Bias: -0.170000, T: 54306, Avg. loss: 0.000371
Total training time: 0.14 seconds.
Norm: 22.22, NNZs: 13366, Bias: -0.160000, T: 72408, Avg. loss: 0.000236
Total training time: 0.13 seconds.
-- Epoch 7
Convergence after 8 epochs took 0.13 seconds
Norm: 21.71, NNZs: 11122, Bias: -0.140000, T: 63364, Avg. loss: 0.000319
Total training time: 0.13 seconds.
Norm: 18.36, NNZs: 9019, Bias: -0.190000, T: 27153, Avg. loss: 0.000439
Total training time: 0.05 seconds.
Norm: 20.89, NNZs: 11135, Bias: -0.300000, T: 36204, Avg. loss: 0.000313
Total training time: 0.08 seconds.
Norm: 20.79, NNZs: 10602, Bias: -0.250000, T: 27153, Avg. loss: 0.000602
Total training time: 0.07 seconds.
Norm: 22.20, NNZs: 10975, Bias: -0.140000, T: 72408, Avg. loss: 0.000235
Total training time: 0.12 seconds.
Convergence after 7 epochs took 0.13 seconds
-- Epoch 4
-- Epoch 5
-- Epoch 4
Convergence after 8 epochs took 0.12 seconds
Norm: 24.25, NNZs: 9980, Bias: -0.090000, T: 72408, Avg. loss: 0.000501
Total training time: 0.26 seconds.
Norm: 22.36, NNZs: 11212, Bias: -0.120000, T: 72408, Avg. loss: 0.000223
Total training time: 0.16 seconds.
Convergence after 8 epochs took 0.26 seconds
Convergence after 8 epochs took 0.16 seconds
Norm: 21.34, NNZs: 11687, Bias: -0.310000, T: 45255, Avg. loss: 0.000214
Total training time: 0.11 seconds.
Norm: 23.54, NNZs: 9552, Bias: -0.140000, T: 54306, Avg. loss: 0.000377
Total training time: 0.17 seconds.
-- Epoch 6
-- Epoch 7
Norm: 20.73, NNZs: 17276, Bias: -0.230000, T: 45255, Avg. loss: 0.000283
Total training time: 0.11 seconds.
Norm: 19.71, NNZs: 9778, Bias: -0.070000, T: 63357, Avg. loss: 0.000156
-- Epoch 6
Total training time: 0.16 seconds.
Norm: 21.26, NNZs: 13217, Bias: -0.140000, T: 63357, Avg. loss: 0.000267
Total training time: 0.15 seconds.
Convergence after 7 epochs took 0.16 seconds
Norm: 21.25, NNZs: 8729, Bias: -0.150000, T: 45255, Avg. loss: 0.000291
Convergence after 7 epochs took 0.15 seconds
Total training time: 0.14 seconds.
Norm: 20.93, NNZs: 10357, Bias: -0.130000, T: 63357, Avg. loss: 0.000367
Total training time: 0.15 seconds.
-- Epoch 6
Convergence after 7 epochs took 0.15 seconds
Norm: 21.29, NNZs: 10861, Bias: -0.270000, T: 36204, Avg. loss: 0.000582
Total training time: 0.07 seconds.
Norm: 21.10, NNZs: 11205, Bias: -0.300000, T: 45255, Avg. loss: 0.000304
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 18.70, NNZs: 9108, Bias: -0.130000, T: 36204, Avg. loss: 0.000315
Total training time: 0.05 seconds.
-- Epoch 6
-- Epoch 5
Norm: 23.83, NNZs: 9617, Bias: -0.110000, T: 63357, Avg. loss: 0.000300
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 21.35, NNZs: 11688, Bias: -0.270000, T: 54306, Avg. loss: 0.000163
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 20.92, NNZs: 17318, Bias: -0.180000, T: 54306, Avg. loss: 0.000239
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 21.53, NNZs: 8770, Bias: -0.140000, T: 54306, Avg. loss: 0.000257
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 24.03, NNZs: 9689, Bias: -0.030000, T: 72408, Avg. loss: 0.000275
Total training time: 0.17 seconds.
Convergence after 8 epochs took 0.17 seconds
Norm: 21.18, NNZs: 11215, Bias: -0.250000, T: 54306, Avg. loss: 0.000239
Norm: 21.48, NNZs: 10902, Bias: -0.230000, T: 45255, Avg. loss: 0.000325
Total training time: 0.08 seconds.
Total training time: 0.09 seconds.
-- Epoch 6
-- Epoch 7
Norm: 19.14, NNZs: 9274, Bias: -0.120000, T: 45255, Avg. loss: 0.000260
Total training time: 0.06 seconds.
-- Epoch 6
Norm: 21.40, NNZs: 11709, Bias: -0.270000, T: 63357, Avg. loss: 0.000159
Total training time: 0.12 seconds.
-- Epoch 8
Norm: 21.11, NNZs: 17376, Bias: -0.160000, T: 63357, Avg. loss: 0.000208
Total training time: 0.12 seconds.
Convergence after 7 epochs took 0.12 seconds
Norm: 21.82, NNZs: 8850, Bias: -0.160000, T: 63357, Avg. loss: 0.000220
Total training time: 0.15 seconds.
Convergence after 7 epochs took 0.15 seconds
Norm: 21.34, NNZs: 11366, Bias: -0.230000, T: 63357, Avg. loss: 0.000249
Total training time: 0.09 seconds.
Norm: 21.56, NNZs: 10927, Bias: -0.190000, T: 54306, Avg. loss: 0.000248
Total training time: 0.08 seconds.
Convergence after 7 epochs took 0.09 seconds
-- Epoch 7
Norm: 19.56, NNZs: 9703, Bias: -0.120000, T: 54306, Avg. loss: 0.000244
Total training time: 0.06 seconds.
Norm: 21.48, NNZs: 11725, Bias: -0.250000, T: 72408, Avg. loss: 0.000152
Total training time: 0.12 seconds.
-- Epoch 7
Convergence after 8 epochs took 0.12 seconds
Norm: 21.90, NNZs: 11048, Bias: -0.210000, T: 63357, Avg. loss: 0.000259
Total training time: 0.08 seconds.
-- Epoch 8
Norm: 19.89, NNZs: 9776, Bias: -0.080000, T: 63357, Avg. loss: 0.000214
Total training time: 0.06 seconds.
Convergence after 7 epochs took 0.06 seconds
Norm: 21.97, NNZs: 11089, Bias: -0.160000, T: 72408, Avg. loss: 0.000211
Total training time: 0.09 seconds.
Convergence after 8 epochs took 0.09 seconds
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.5s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.5s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s finished
	accuracy: 5-fold cross validation: [0.70702607 0.69465312 0.70172338 0.71851525 0.69982317]
	test accuracy: 5-fold cross validation accuracy: 0.70 (+/- 0.02)
dimensionality: 101322
density: 0.114241



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.6336962294211365
	accuracy score (normalize=False):  4773

compute the precision
	precision score (average=macro):  0.6313316413791157
	precision score (average=micro):  0.6336962294211365
	precision score (average=weighted):  0.6386854830465993
	precision score (average=None):  [0.48788927 0.58851675 0.57894737 0.5914787  0.65536723 0.77777778
 0.72890026 0.68668407 0.68556701 0.50488599 0.84935065 0.7398374
 0.57361963 0.71698113 0.68475452 0.578      0.56510417 0.75070028
 0.44983819 0.43243243]
	precision score (average=None, zero_division=1):  [0.48788927 0.58851675 0.57894737 0.5914787  0.65536723 0.77777778
 0.72890026 0.68668407 0.68556701 0.50488599 0.84935065 0.7398374
 0.57361963 0.71698113 0.68475452 0.578      0.56510417 0.75070028
 0.44983819 0.43243243]

compute the precision
	recall score (average=macro):  0.6239325775732321
	recall score (average=micro):  0.6336962294211365
	recall score (average=weighted):  0.6336962294211365
	recall score (average=None):  [0.44200627 0.63239075 0.58629442 0.60204082 0.6025974  0.63797468
 0.73076923 0.66414141 0.66834171 0.78085642 0.81954887 0.68939394
 0.47582697 0.67171717 0.67258883 0.72613065 0.59615385 0.71276596
 0.4483871  0.3187251 ]
	recall score (average=None, zero_division=1):  [0.44200627 0.63239075 0.58629442 0.60204082 0.6025974  0.63797468
 0.73076923 0.66414141 0.66834171 0.78085642 0.81954887 0.68939394
 0.47582697 0.67171717 0.67258883 0.72613065 0.59615385 0.71276596
 0.4483871  0.3187251 ]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.6244147405060042
	f1 score (average=micro):  0.6336962294211365
	f1 score (average=weighted):  0.6329869109073981
	f1 score (average=None):  [0.46381579 0.60966543 0.58259773 0.59671302 0.62787551 0.70097357
 0.72983355 0.67522465 0.67684478 0.6132542  0.83418367 0.71372549
 0.5201669  0.69361147 0.67861716 0.64365256 0.5802139  0.73124147
 0.44911147 0.36697248]

compute the F-beta score
	f beta score (average=macro):  0.627865166800666
	f beta score (average=micro):  0.6336962294211363
	f beta score (average=weighted):  0.6357246082094467
	f beta score (average=None):  [0.4779661  0.59679767 0.58040201 0.59356137 0.64408662 0.74512123
 0.72927329 0.68205394 0.68205128 0.54328777 0.84321815 0.72916667
 0.5509723  0.70744681 0.6822863  0.60258549 0.57105263 0.74279379
 0.44954722 0.40363269]

compute the average Hamming loss
	hamming loss:  0.36630377057886354

jaccard similarity coefficient score
	jaccard score (average=macro):  0.46261128557111475
	jaccard score (average=None):  [0.30192719 0.43850267 0.41103203 0.42522523 0.45759369 0.53961456
 0.57459677 0.50968992 0.51153846 0.44222539 0.71553611 0.55487805
 0.35150376 0.53093812 0.51356589 0.47454844 0.4086629  0.57634409
 0.28958333 0.2247191 ]


================================================================================
Classifier.RANDOM_FOREST_CLASSIFIER
________________________________________________________________________________
Training: 
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=-1, oob_score=False, random_state=0, verbose=True,
                       warm_start=False)
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.3s
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.5s finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
train time: 6.548s
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished
test time:  0.206s
accuracy:   0.627


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.3s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.6s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.7s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.8s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.9s
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.4s finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.6s finished
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.2s finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.6s finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.6s finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.7s finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished
	accuracy: 5-fold cross validation: [0.68581529 0.66372072 0.66813964 0.68183827 0.66445623]
	test accuracy: 5-fold cross validation accuracy: 0.67 (+/- 0.02)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.6267923526287839
	accuracy score (normalize=False):  4721

compute the precision
	precision score (average=macro):  0.6193022080761014
	precision score (average=micro):  0.6267923526287839
	precision score (average=weighted):  0.6279833875703593
	precision score (average=None):  [0.44210526 0.625      0.53846154 0.62154696 0.67374005 0.65450122
 0.69284065 0.42128936 0.6763285  0.69476082 0.80291971 0.81212121
 0.50636943 0.74635569 0.68814433 0.5729927  0.53865337 0.82389937
 0.57865169 0.27536232]
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   44.1s finished
	precision score (average=None, zero_division=1):  [0.44210526 0.625      0.53846154 0.62154696 0.67374005 0.65450122
 0.69284065 0.42128936 0.6763285  0.69476082 0.80291971 0.81212121
 0.50636943 0.74635569 0.68814433 0.5729927  0.53865337 0.82389937
 0.57865169 0.27536232]

compute the precision
	recall score (average=macro):  0.6111845179762837
	recall score (average=micro):  0.6267923526287839
	recall score (average=weighted):  0.6267923526287839
	recall score (average=None):  [0.39498433 0.60411311 0.63959391 0.57397959 0.65974026 0.68101266
 0.76923077 0.70959596 0.70351759 0.76826196 0.82706767 0.67676768
 0.40458015 0.64646465 0.67766497 0.78894472 0.59340659 0.69680851
 0.33225806 0.07569721]
	recall score (average=None, zero_division=1):  [0.39498433 0.60411311 0.63959391 0.57397959 0.65974026 0.68101266
 0.76923077 0.70959596 0.70351759 0.76826196 0.82706767 0.67676768
 0.40458015 0.64646465 0.67766497 0.78894472 0.59340659 0.69680851
 0.33225806 0.07569721]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.6063689929719989
	f1 score (average=micro):  0.6267923526287839
	f1 score (average=weighted):  0.6195640965610109
	f1 score (average=None):  [0.41721854 0.61437908 0.58468677 0.59681698 0.66666667 0.6674938
 0.7290401  0.52869238 0.68965517 0.72966507 0.81481481 0.73829201
 0.44978784 0.69282815 0.68286445 0.66384778 0.56470588 0.75504323
 0.42213115 0.11875   ]

compute the F-beta score
	f beta score (average=macro):  0.6110259190905107
	f beta score (average=micro):  0.6267923526287839
	f beta score (average=weighted):  0.6221252370856075
	f beta score (average=None):  [0.4318026  0.62070787 0.5560459  0.61141304 0.67089276 0.65963708
 0.7068803  0.45855091 0.68159688 0.70831398 0.80763583 0.78088578
 0.48211037 0.7239819  0.68602261 0.60617761 0.54878049 0.79490291
 0.50391389 0.18026565]

compute the average Hamming loss
	hamming loss:  0.37320764737121614

jaccard similarity coefficient score
	jaccard score (average=macro):  0.4509352414278219
	jaccard score (average=None):  [0.26359833 0.44339623 0.41311475 0.42533081 0.5        0.5009311
 0.57361377 0.35933504 0.52631579 0.57438795 0.6875     0.58515284
 0.29014599 0.5300207  0.5184466  0.49683544 0.39344262 0.60648148
 0.26753247 0.06312292]


================================================================================
Classifier.RIDGE_CLASSIFIER
________________________________________________________________________________
Training: 
RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, random_state=0, solver='auto',
                tol=0.001)
train time: 2.377s
test time:  0.023s
accuracy:   0.704


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
	accuracy: 5-fold cross validation: [0.77551922 0.7565179  0.76756518 0.78479894 0.75773652]
	test accuracy: 5-fold cross validation accuracy: 0.77 (+/- 0.02)
dimensionality: 101322
density: 1.000000



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.7035315985130112
	accuracy score (normalize=False):  5299

compute the precision
	precision score (average=macro):  0.7001727985077145
	precision score (average=micro):  0.7035315985130112
	precision score (average=weighted):  0.7078752100517143
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.6s finished
	precision score (average=None):  [0.55714286 0.68171021 0.64735516 0.66336634 0.73243243 0.83333333
 0.74514563 0.76203209 0.81432361 0.54340836 0.87281796 0.85714286
 0.624      0.7893401  0.75252525 0.64484127 0.60144928 0.85373134
 0.57480315 0.45255474]
	precision score (average=None, zero_division=1):  [0.55714286 0.68171021 0.64735516 0.66336634 0.73243243 0.83333333
 0.74514563 0.76203209 0.81432361 0.54340836 0.87281796 0.85714286
 0.624      0.7893401  0.75252525 0.64484127 0.60144928 0.85373134
 0.57480315 0.45255474]

compute the precision
	recall score (average=macro):  0.6905425119887065
	recall score (average=micro):  0.7035315985130112
	recall score (average=weighted):  0.7035315985130112
	recall score (average=None):  [0.48902821 0.7377892  0.65228426 0.68367347 0.7038961  0.70886076
 0.78717949 0.71969697 0.77135678 0.85138539 0.87719298 0.71212121
 0.59541985 0.78535354 0.75634518 0.81658291 0.68406593 0.7606383
 0.47096774 0.24701195]
	recall score (average=None, zero_division=1):  [0.48902821 0.7377892  0.65228426 0.68367347 0.7038961  0.70886076
 0.78717949 0.71969697 0.77135678 0.85138539 0.87719298 0.71212121
 0.59541985 0.78535354 0.75634518 0.81658291 0.68406593 0.7606383
 0.47096774 0.24701195]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.6902380983234169
	f1 score (average=micro):  0.7035315985130112
	f1 score (average=weighted):  0.7009967959860923
	f1 score (average=None):  [0.52086811 0.70864198 0.64981037 0.67336683 0.71788079 0.76607387
 0.76558603 0.74025974 0.79225806 0.66339549 0.875      0.77793103
 0.609375   0.78734177 0.75443038 0.72062084 0.64010283 0.8045007
 0.5177305  0.31958763]

compute the F-beta score
	f beta score (average=macro):  0.6948782109756747
	f beta score (average=micro):  0.7035315985130112
	f beta score (average=weighted):  0.7039804530860729
	f beta score (average=None):  [0.54204309 0.69223348 0.64833502 0.66733068 0.72654155 0.80506038
 0.7531894  0.75317125 0.80535152 0.58578856 0.87368947 0.82359813
 0.61806656 0.78853955 0.75328615 0.67315659 0.61633663 0.83333333
 0.5505279  0.38798498]

compute the average Hamming loss
	hamming loss:  0.29646840148698883

jaccard similarity coefficient score
	jaccard score (average=macro):  0.5392263941095169
	jaccard score (average=None):  [0.35214447 0.54875717 0.48127341 0.50757576 0.55991736 0.62084257
 0.62020202 0.58762887 0.65598291 0.49632893 0.77777778 0.63656885
 0.43820225 0.64926931 0.60569106 0.56325823 0.47069943 0.67294118
 0.3492823  0.19018405]


================================================================================
Classifier.RIDGE_CLASSIFIERCV
________________________________________________________________________________
Training: 
RidgeClassifierCV(alphas=array([ 0.1,  1. , 10. ]), class_weight=None, cv=None,
                  fit_intercept=True, normalize=False, scoring=None,
                  store_cv_values=False)
train time: 172.960s
test time:  0.018s
accuracy:   0.704


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[LibSVM]	accuracy: 5-fold cross validation: [0.77551922 0.7565179  0.76756518 0.78479894 0.75773652]
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  5.5min finished
	test accuracy: 5-fold cross validation accuracy: 0.77 (+/- 0.02)
dimensionality: 101322
density: 1.000000



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.7036643653744026
	accuracy score (normalize=False):  5300

compute the precision
	precision score (average=macro):  0.7003156337207433
	precision score (average=micro):  0.7036643653744026
	precision score (average=weighted):  0.7080212412905894
	precision score (average=None):  [0.55714286 0.68171021 0.64735516 0.66336634 0.73441734 0.83333333
 0.74514563 0.76407507 0.81432361 0.54340836 0.87064677 0.85714286
 0.625      0.7893401  0.75252525 0.64484127 0.60144928 0.85373134
 0.57480315 0.45255474]
	precision score (average=None, zero_division=1):  [0.55714286 0.68171021 0.64735516 0.66336634 0.73441734 0.83333333
 0.74514563 0.76407507 0.81432361 0.54340836 0.87064677 0.85714286
 0.625      0.7893401  0.75252525 0.64484127 0.60144928 0.85373134
 0.57480315 0.45255474]

compute the precision
	recall score (average=macro):  0.6906697384518108
	recall score (average=micro):  0.7036643653744026
	recall score (average=weighted):  0.7036643653744026
	recall score (average=None):  [0.48902821 0.7377892  0.65228426 0.68367347 0.7038961  0.70886076
 0.78717949 0.71969697 0.77135678 0.85138539 0.87719298 0.71212121
 0.59796438 0.78535354 0.75634518 0.81658291 0.68406593 0.7606383
 0.47096774 0.24701195]
	recall score (average=None, zero_division=1):  [0.48902821 0.7377892  0.65228426 0.68367347 0.7038961  0.70886076
 0.78717949 0.71969697 0.77135678 0.85138539 0.87719298 0.71212121
 0.59796438 0.78535354 0.75634518 0.81658291 0.68406593 0.7606383
 0.47096774 0.24701195]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.690369632997523
	f1 score (average=micro):  0.7036643653744026
	f1 score (average=weighted):  0.7011325606353689
	f1 score (average=None):  [0.52086811 0.70864198 0.64981037 0.67336683 0.71883289 0.76607387
 0.76558603 0.74122237 0.79225806 0.66339549 0.87390762 0.77793103
 0.61118336 0.78734177 0.75443038 0.72062084 0.64010283 0.8045007
 0.5177305  0.31958763]

compute the F-beta score
	f beta score (average=macro):  0.6950156371185258
	f beta score (average=micro):  0.7036643653744026
	f beta score (average=weighted):  0.704121453611637
	f beta score (average=None):  [0.54204309 0.69223348 0.64833502 0.66733068 0.72810317 0.80506038
 0.7531894  0.75476695 0.80535152 0.58578856 0.87194818 0.82359813
 0.61939905 0.78853955 0.75328615 0.67315659 0.61633663 0.83333333
 0.5505279  0.38798498]

compute the average Hamming loss
	hamming loss:  0.2963356346255975

jaccard similarity coefficient score
	jaccard score (average=macro):  0.5393524668538194
	jaccard score (average=None):  [0.35214447 0.54875717 0.48127341 0.50757576 0.5610766  0.62084257
 0.62020202 0.58884298 0.65598291 0.49632893 0.77605322 0.63656885
 0.44007491 0.64926931 0.60569106 0.56325823 0.47069943 0.67294118
 0.3492823  0.19018405]


================================================================================
Classifier.SGD_CLASSIFIER
________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=0, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=True, warm_start=False)
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 27.82, NNZs: 14837, Bias: -1.258507, T: 11314, Avg. loss: 0.066281
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 27.88, NNZs: 11211, Bias: -1.186235, T: 11314, Avg. loss: 0.071502
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 24.77, NNZs: 14476, Bias: -1.096449, T: 22628, Avg. loss: 0.042898
Total training time: 0.05 seconds.
-- Epoch 3
Norm: 25.44, NNZs: 13387, Bias: -1.287355, T: 11314, Avg. loss: 0.078009
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 24.31, NNZs: 17691, Bias: -1.184354, T: 22628, Avg. loss: 0.035969
Total training time: 0.05 seconds.
Norm: 26.77, NNZs: 12873, Bias: -1.178053, T: 11314, Avg. loss: 0.048313
Total training time: 0.04 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 1
-- Epoch 1
Norm: 23.47, NNZs: 16606, Bias: -1.186201, T: 22628, Avg. loss: 0.047258
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 27.86, NNZs: 11707, Bias: -1.186255, T: 11314, Avg. loss: 0.082575
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 22.51, NNZs: 18213, Bias: -1.115144, T: 33942, Avg. loss: 0.039901
Norm: 29.03, NNZs: 10565, Bias: -1.207232, T: 11314, Avg. loss: 0.063908
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 23.15, NNZs: 15670, Bias: -1.074247, T: 22628, Avg. loss: 0.026795
Total training time: 0.06 seconds.
-- Epoch 1
Norm: 28.33, NNZs: 14055, Bias: -1.276419, T: 11314, Avg. loss: 0.090048
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 26.28, NNZs: 12182, Bias: -1.384905, T: 11314, Avg. loss: 0.094156
Total training time: 0.10 seconds.
-- Epoch 2
Norm: 26.70, NNZs: 10627, Bias: -1.272591, T: 11314, Avg. loss: 0.075749
Total training time: 0.07 seconds.
Total training time: 0.09 seconds.
-- Epoch 2
-- Epoch 4
Norm: 22.23, NNZs: 19279, Bias: -1.110268, T: 45256, Avg. loss: 0.037730
Total training time: 0.09 seconds.
Norm: 23.42, NNZs: 20599, Bias: -1.103202, T: 33942, Avg. loss: 0.030228
Total training time: 0.09 seconds.
-- Epoch 5
-- Epoch 4
Norm: 23.85, NNZs: 16315, Bias: -1.028794, T: 33942, Avg. loss: 0.037313
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 22.08, NNZs: 19909, Bias: -1.086478, T: 56570, Avg. loss: 0.035557
Total training time: 0.10 seconds.
Norm: 25.17, NNZs: 16987, Bias: -1.081764, T: 22628, Avg. loss: 0.035400
Total training time: 0.08 seconds.
-- Epoch 3
-- Epoch 3
Norm: 23.17, NNZs: 26480, Bias: -1.081974, T: 45256, Avg. loss: 0.027827
Total training time: 0.11 seconds.
Norm: 24.08, NNZs: 12910, Bias: -1.213154, T: 22628, Avg. loss: 0.045494
Total training time: 0.09 seconds.
Norm: 26.84, NNZs: 40423, Bias: -1.277664, T: 11314, Avg. loss: 0.085164
Total training time: 0.09 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 1
Norm: 24.06, NNZs: 45055, Bias: -1.253349, T: 22628, Avg. loss: 0.052142
Total training time: 0.09 seconds.
-- Epoch 5
-- Epoch 3
Norm: 23.53, NNZs: 47199, Bias: -1.198766, T: 33942, Avg. loss: 0.045206
Total training time: 0.10 seconds.
-- Epoch 4
-- Epoch 6
Norm: 23.59, NNZs: 14430, Bias: -1.277384, T: 22628, Avg. loss: 0.059869
Total training time: 0.14 seconds.
-- Epoch 3
Norm: 23.39, NNZs: 22748, Bias: -1.021593, T: 45256, Avg. loss: 0.033899
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 23.08, NNZs: 14478, Bias: -1.160264, T: 33942, Avg. loss: 0.039257
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 25.81, NNZs: 15364, Bias: -1.334667, T: 11314, Avg. loss: 0.056573
Total training time: 0.08 seconds.
-- Epoch 2
Norm: 27.71, NNZs: 14292, Bias: -1.279018, T: 11314, Avg. loss: 0.070411
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 22.14, NNZs: 17380, Bias: -1.041265, T: 33942, Avg. loss: 0.023627
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 23.01, NNZs: 27652, Bias: -1.066084, T: 56570, Avg. loss: 0.026159
Total training time: 0.14 seconds.
Norm: 22.64, NNZs: 18848, Bias: -1.252101, T: 22628, Avg. loss: 0.032760
Total training time: 0.09 seconds.
-- Epoch 6
-- Epoch 3
Norm: 23.06, NNZs: 23752, Bias: -1.001494, T: 56570, Avg. loss: 0.032292
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 22.98, NNZs: 25107, Bias: -1.009215, T: 67884, Avg. loss: 0.031401
Total training time: 0.16 seconds.
Norm: 24.61, NNZs: 16772, Bias: -1.160026, T: 22628, Avg. loss: 0.049493
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 22.00, NNZs: 20772, Bias: -1.064987, T: 67884, Avg. loss: 0.034451
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 23.90, NNZs: 21142, Bias: -1.048302, T: 33942, Avg. loss: 0.030297
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 21.72, NNZs: 21076, Bias: -1.023475, T: 45256, Avg. loss: 0.021883
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 25.30, NNZs: 18472, Bias: -1.202169, T: 22628, Avg. loss: 0.053994
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 23.25, NNZs: 48068, Bias: -1.149664, T: 45256, Avg. loss: 0.041747
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 22.63, NNZs: 15749, Bias: -1.134775, T: 45256, Avg. loss: 0.036477
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 22.96, NNZs: 20661, Bias: -1.243450, T: 33942, Avg. loss: 0.052433
Total training time: 0.18 seconds.
Norm: 23.61, NNZs: 22329, Bias: -1.000810, T: 45256, Avg. loss: 0.027927
Total training time: 0.15 seconds.
-- Epoch 5
-- Epoch 4
Norm: 21.92, NNZs: 21193, Bias: -1.059068, T: 79198, Avg. loss: 0.033577
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 22.72, NNZs: 22060, Bias: -1.214591, T: 45256, Avg. loss: 0.048642
Total training time: 0.18 seconds.
Norm: 23.90, NNZs: 19409, Bias: -1.208973, T: 22628, Avg. loss: 0.040264
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 23.77, NNZs: 21317, Bias: -1.096530, T: 33942, Avg. loss: 0.043571
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 22.88, NNZs: 28253, Bias: -1.050624, T: 67884, Avg. loss: 0.025451
Total training time: 0.17 seconds.
-- Epoch 7
-- Epoch 7
Norm: 21.45, NNZs: 21899, Bias: -1.016229, T: 56570, Avg. loss: 0.020493
Total training time: 0.17 seconds.
-- Epoch 6
Norm: 23.14, NNZs: 48717, Bias: -1.118282, T: 56570, Avg. loss: 0.039776
Total training time: 0.15 seconds.
Norm: 24.31, NNZs: 22279, Bias: -1.122861, T: 33942, Avg. loss: 0.046081
Total training time: 0.11 seconds.
-- Epoch 6
-- Epoch 4
Norm: 21.59, NNZs: 21772, Bias: -1.172095, T: 33942, Avg. loss: 0.028616
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 21.32, NNZs: 22466, Bias: -1.003006, T: 67884, Avg. loss: 0.019631
Total training time: 0.17 seconds.
-- Epoch 7
Norm: 23.98, NNZs: 23445, Bias: -1.111383, T: 45256, Avg. loss: 0.042898
Total training time: 0.12 seconds.
Norm: 22.34, NNZs: 19024, Bias: -1.108675, T: 56570, Avg. loss: 0.034666
Total training time: 0.16 seconds.
-- Epoch 5
-- Epoch 6
Norm: 21.92, NNZs: 21675, Bias: -1.050793, T: 90512, Avg. loss: 0.032961
Total training time: 0.19 seconds.
Norm: 22.86, NNZs: 25829, Bias: -1.003314, T: 79198, Avg. loss: 0.030869
Total training time: 0.20 seconds.
-- Epoch 9
-- Epoch 8
Norm: 23.37, NNZs: 23056, Bias: -1.002038, T: 56570, Avg. loss: 0.026483
Total training time: 0.17 seconds.
-- Epoch 6
-- Epoch 5
Norm: 21.07, NNZs: 23492, Bias: -1.137277, T: 45256, Avg. loss: 0.026665
Norm: 23.00, NNZs: 49417, Bias: -1.123236, T: 67884, Avg. loss: 0.038499
Total training time: 0.16 seconds.
-- Epoch 7
Norm: 23.53, NNZs: 22897, Bias: -1.054314, T: 45256, Avg. loss: 0.040522
Total training time: 0.15 seconds.
-- Epoch 5
Norm: 21.22, NNZs: 23459, Bias: -1.003020, T: 79198, Avg. loss: 0.019108
Total training time: 0.18 seconds.
-- Epoch 8
Norm: 23.79, NNZs: 25426, Bias: -1.077750, T: 56570, Avg. loss: 0.041147
Total training time: 0.12 seconds.
Total training time: 0.14 seconds.
Norm: 22.25, NNZs: 19637, Bias: -1.097694, T: 67884, Avg. loss: 0.033631
Total training time: 0.17 seconds.
-- Epoch 6
-- Epoch 5
-- Epoch 7
Norm: 22.76, NNZs: 31367, Bias: -1.048311, T: 79198, Avg. loss: 0.024541
Total training time: 0.19 seconds.
Norm: 21.86, NNZs: 21978, Bias: -1.046325, T: 101826, Avg. loss: 0.032246
Total training time: 0.19 seconds.
-- Epoch 8
-- Epoch 10
Norm: 23.25, NNZs: 28115, Bias: -1.003884, T: 67884, Avg. loss: 0.025418
Total training time: 0.18 seconds.
-- Epoch 7
Norm: 22.55, NNZs: 22767, Bias: -1.169325, T: 56570, Avg. loss: 0.046463
Total training time: 0.20 seconds.
-- Epoch 6
Norm: 22.91, NNZs: 49733, Bias: -1.110357, T: 79198, Avg. loss: 0.037547
Total training time: 0.17 seconds.
Norm: 22.79, NNZs: 26357, Bias: -1.003260, T: 90512, Avg. loss: 0.030089
Total training time: 0.21 seconds.
-- Epoch 8
-- Epoch 9
Norm: 23.24, NNZs: 23726, Bias: -1.046860, T: 56570, Avg. loss: 0.038096
Total training time: 0.16 seconds.
Norm: 21.12, NNZs: 23796, Bias: -1.004060, T: 90512, Avg. loss: 0.018710
Total training time: 0.18 seconds.
Norm: 20.98, NNZs: 24855, Bias: -1.147236, T: 56570, Avg. loss: 0.025618
Total training time: 0.15 seconds.
Norm: 22.97, NNZs: 21529, Bias: -1.161275, T: 33942, Avg. loss: 0.034973
Total training time: 0.08 seconds.
Norm: 23.69, NNZs: 25956, Bias: -1.051355, T: 67884, Avg. loss: 0.039925
Total training time: 0.13 seconds.
-- Epoch 6
-- Epoch 9
-- Epoch 4
-- Epoch 6
Norm: 22.14, NNZs: 20127, Bias: -1.082775, T: 79198, Avg. loss: 0.032969
-- Epoch 7
Total training time: 0.18 seconds.
-- Epoch 8
Norm: 22.75, NNZs: 34146, Bias: -1.033172, T: 90512, Avg. loss: 0.024083
Total training time: 0.20 seconds.
Norm: 21.86, NNZs: 22203, Bias: -1.037723, T: 113140, Avg. loss: 0.031986
Total training time: 0.20 seconds.
-- Epoch 9
-- Epoch 11
Norm: 23.13, NNZs: 28802, Bias: -1.005145, T: 79198, Avg. loss: 0.024848
Total training time: 0.18 seconds.
-- Epoch 8
Norm: 22.53, NNZs: 29270, Bias: -1.169886, T: 67884, Avg. loss: 0.044934
Total training time: 0.21 seconds.
-- Epoch 7
Norm: 22.68, NNZs: 31683, Bias: -1.007975, T: 101826, Avg. loss: 0.029663
Total training time: 0.21 seconds.
Norm: 22.84, NNZs: 50373, Bias: -1.090030, T: 90512, Avg. loss: 0.036913
Total training time: 0.18 seconds.
-- Epoch 10
-- Epoch 9
Norm: 23.19, NNZs: 29134, Bias: -1.019011, T: 67884, Avg. loss: 0.037495
Total training time: 0.17 seconds.
-- Epoch 7
Norm: 22.57, NNZs: 22707, Bias: -1.105501, T: 45256, Avg. loss: 0.032151
Norm: 20.72, NNZs: 26375, Bias: -1.124065, T: 67884, Avg. loss: 0.024537
Total training time: 0.15 seconds.
Total training time: 0.09 seconds.
-- Epoch 7
-- Epoch 5
Norm: 23.59, NNZs: 27527, Bias: -1.054993, T: 79198, Avg. loss: 0.038749
Total training time: 0.14 seconds.
Norm: 22.06, NNZs: 20521, Bias: -1.071964, T: 90512, Avg. loss: 0.032439
Total training time: 0.18 seconds.
-- Epoch 8
-- Epoch 9
Norm: 22.64, NNZs: 38443, Bias: -1.028025, T: 101826, Avg. loss: 0.023507
Total training time: 0.20 seconds.
Norm: 21.85, NNZs: 22387, Bias: -1.040755, T: 124454, Avg. loss: 0.031535
Total training time: 0.20 seconds.
-- Epoch 10
Convergence after 11 epochs took 0.21 seconds
-- Epoch 1
Norm: 22.03, NNZs: 21016, Bias: -1.076129, T: 101826, Avg. loss: 0.032103
Total training time: 0.20 seconds.
Norm: 22.48, NNZs: 29709, Bias: -1.152985, T: 79198, Avg. loss: 0.044032
Total training time: 0.23 seconds.
Norm: 23.13, NNZs: 31611, Bias: -1.011432, T: 79198, Avg. loss: 0.036395
Total training time: 0.18 seconds.
-- Epoch 10
-- Epoch 8
-- Epoch 8
Norm: 23.01, NNZs: 30968, Bias: -1.004788, T: 90512, Avg. loss: 0.024165
Total training time: 0.21 seconds.
Norm: 28.52, NNZs: 13342, Bias: -1.273055, T: 11314, Avg. loss: 0.094472
Norm: 20.71, NNZs: 26982, Bias: -1.104531, T: 79198, Avg. loss: 0.024024
Total training time: 0.17 seconds.
Norm: 22.81, NNZs: 50990, Bias: -1.091381, T: 101826, Avg. loss: 0.036307
Total training time: 0.20 seconds.
Norm: 22.65, NNZs: 38605, Bias: -1.017262, T: 113140, Avg. loss: 0.023331
Total training time: 0.22 seconds.
-- Epoch 8
-- Epoch 10
Convergence after 10 epochs took 0.22 seconds
-- Epoch 9
-- Epoch 1
Norm: 21.99, NNZs: 21242, Bias: -1.060543, T: 113140, Avg. loss: 0.031562
Total training time: 0.21 seconds.
Total training time: 0.02 seconds.
Norm: 21.09, NNZs: 24140, Bias: -1.011675, T: 101826, Avg. loss: 0.018335
Total training time: 0.22 seconds.
-- Epoch 2
-- Epoch 10
Norm: 22.38, NNZs: 29962, Bias: -1.140181, T: 90512, Avg. loss: 0.043173
Total training time: 0.24 seconds.
-- Epoch 9
Norm: 20.63, NNZs: 27572, Bias: -1.108488, T: 90512, Avg. loss: 0.023345
Total training time: 0.18 seconds.
-- Epoch 9
Norm: 23.52, NNZs: 27913, Bias: -1.048146, T: 90512, Avg. loss: 0.038081
Norm: 22.66, NNZs: 31947, Bias: -1.001244, T: 113140, Avg. loss: 0.029387
Total training time: 0.25 seconds.
Convergence after 10 epochs took 0.25 seconds
Norm: 22.90, NNZs: 51212, Bias: -1.088616, T: 113140, Avg. loss: 0.035924
Total training time: 0.21 seconds.
-- Epoch 11
Norm: 23.11, NNZs: 32103, Bias: -1.012453, T: 90512, Avg. loss: 0.035845
Total training time: 0.21 seconds.
-- Epoch 9
Norm: 21.06, NNZs: 29339, Bias: -1.001927, T: 113140, Avg. loss: 0.018123
Total training time: 0.23 seconds.
Norm: 22.37, NNZs: 23366, Bias: -1.107080, T: 56570, Avg. loss: 0.030725
Convergence after 10 epochs took 0.23 seconds
Norm: 22.39, NNZs: 30158, Bias: -1.137182, T: 101826, Avg. loss: 0.042841
-- Epoch 11
Total training time: 0.13 seconds.
-- Epoch 6
Total training time: 0.18 seconds.
-- Epoch 9
Norm: 23.07, NNZs: 32330, Bias: -1.004153, T: 101826, Avg. loss: 0.035238
Total training time: 0.21 seconds.
-- Epoch 10
-- Epoch 1
Total training time: 0.27 seconds.
Norm: 20.57, NNZs: 30473, Bias: -1.093924, T: 101826, Avg. loss: 0.023081
Total training time: 0.22 seconds.
Norm: 22.03, NNZs: 24111, Bias: -1.056155, T: 124454, Avg. loss: 0.031325
Total training time: 0.25 seconds.
-- Epoch 1
-- Epoch 10
Norm: 28.26, NNZs: 15823, Bias: -1.235868, T: 11314, Avg. loss: 0.065592
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 10
Norm: 28.71, NNZs: 16846, Bias: -1.163916, T: 11314, Avg. loss: 0.071072
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 22.38, NNZs: 32961, Bias: -1.129800, T: 113140, Avg. loss: 0.042423
Total training time: 0.29 seconds.
Norm: 22.82, NNZs: 51300, Bias: -1.081112, T: 124454, Avg. loss: 0.035434
Total training time: 0.26 seconds.
-- Epoch 11
Convergence after 11 epochs took 0.26 seconds
Convergence after 11 epochs took 0.26 seconds
Norm: 23.06, NNZs: 32763, Bias: -0.998945, T: 113140, Avg. loss: 0.034796
Total training time: 0.26 seconds.
Norm: 25.09, NNZs: 19792, Bias: -1.134833, T: 22628, Avg. loss: 0.035957
-- Epoch 1
Norm: 22.21, NNZs: 23910, Bias: -1.096599, T: 67884, Avg. loss: 0.029606
Total training time: 0.18 seconds.
Norm: 20.59, NNZs: 30663, Bias: -1.085331, T: 113140, Avg. loss: 0.022776
Total training time: 0.25 seconds.
-- Epoch 11
Norm: 22.36, NNZs: 37474, Bias: -1.131293, T: 124454, Avg. loss: 0.041914
Total training time: 0.32 seconds.
Convergence after 11 epochs took 0.32 seconds
-- Epoch 1
Total training time: 0.08 seconds.
Norm: 25.62, NNZs: 16949, Bias: -1.140498, T: 22628, Avg. loss: 0.056893
Total training time: 0.10 seconds.
-- Epoch 3
-- Epoch 7
Norm: 23.54, NNZs: 28223, Bias: -1.039054, T: 101826, Avg. loss: 0.037498
Total training time: 0.25 seconds.
-- Epoch 10
-- Epoch 3
Norm: 25.19, NNZs: 19734, Bias: -1.132730, T: 22628, Avg. loss: 0.040752
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 22.97, NNZs: 31185, Bias: -1.008436, T: 101826, Avg. loss: 0.023886
Total training time: 0.31 seconds.
Norm: 22.21, NNZs: 25007, Bias: -1.084528, T: 79198, Avg. loss: 0.028947
Norm: 20.62, NNZs: 31015, Bias: -1.080236, T: 124454, Avg. loss: 0.022597
Norm: 25.42, NNZs: 14742, Bias: -1.227796, T: 11314, Avg. loss: 0.054779
Total training time: 0.03 seconds.
-- Epoch 1
-- Epoch 10
Norm: 24.91, NNZs: 24306, Bias: -1.124754, T: 33942, Avg. loss: 0.048811
Norm: 26.50, NNZs: 17760, Bias: -1.388290, T: 11314, Avg. loss: 0.081462
Total training time: 0.06 seconds.
-- Epoch 2
Norm: 23.49, NNZs: 28472, Bias: -1.031342, T: 113140, Avg. loss: 0.036840
Total training time: 0.26 seconds.
-- Epoch 11
Total training time: 0.22 seconds.
-- Epoch 8
Norm: 25.90, NNZs: 17821, Bias: -1.331532, T: 11314, Avg. loss: 0.077669
Total training time: 0.01 seconds.
Norm: 22.94, NNZs: 31980, Bias: -1.001652, T: 113140, Avg. loss: 0.023719
Total training time: 0.32 seconds.
-- Epoch 11
Norm: 27.82, NNZs: 18012, Bias: -1.323153, T: 11314, Avg. loss: 0.072002
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 22.93, NNZs: 33384, Bias: -0.998478, T: 124454, Avg. loss: 0.023609
Total training time: 0.32 seconds.
Convergence after 11 epochs took 0.32 seconds
Total training time: 0.28 seconds.
-- Epoch 11
-- Epoch 2
Norm: 22.19, NNZs: 30172, Bias: -1.071987, T: 90512, Avg. loss: 0.028208
Total training time: 0.23 seconds.
-- Epoch 9
Norm: 24.21, NNZs: 26405, Bias: -1.180986, T: 22628, Avg. loss: 0.041738
Total training time: 0.03 seconds.
-- Epoch 3
Convergence after 11 epochs took 0.29 seconds
Norm: 23.03, NNZs: 33078, Bias: -1.007344, T: 124454, Avg. loss: 0.034344
-- Epoch 1
Norm: 24.36, NNZs: 20560, Bias: -1.218327, T: 22628, Avg. loss: 0.048013
Total training time: 0.08 seconds.
Total training time: 0.31 seconds.
-- Epoch 12
Norm: 24.36, NNZs: 22091, Bias: -1.065058, T: 33942, Avg. loss: 0.035087
Total training time: 0.09 seconds.
Norm: 24.09, NNZs: 22511, Bias: -1.095418, T: 33942, Avg. loss: 0.031088
Total training time: 0.12 seconds.
-- Epoch 4
-- Epoch 4
Norm: 22.99, NNZs: 33271, Bias: -1.005587, T: 135768, Avg. loss: 0.034130
Total training time: 0.32 seconds.
-- Epoch 3
Convergence after 12 epochs took 0.32 seconds
-- Epoch 2
Norm: 23.85, NNZs: 25355, Bias: -1.058386, T: 45256, Avg. loss: 0.032096
Total training time: 0.10 seconds.
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 22.34, NNZs: 18082, Bias: -1.148855, T: 22628, Avg. loss: 0.031477
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 23.59, NNZs: 22146, Bias: -1.195948, T: 33942, Avg. loss: 0.041451
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 24.61, NNZs: 26264, Bias: -1.104438, T: 45256, Avg. loss: 0.044854
Total training time: 0.15 seconds.
-- Epoch 5
Norm: 22.20, NNZs: 30721, Bias: -1.072982, T: 101826, Avg. loss: 0.027685
Total training time: 0.25 seconds.
-- Epoch 10
Norm: 23.34, NNZs: 20985, Bias: -1.281570, T: 22628, Avg. loss: 0.045441
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 21.01, NNZs: 13213, Bias: -1.343747, T: 11314, Avg. loss: 0.076112
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 23.55, NNZs: 24021, Bias: -1.073831, T: 45256, Avg. loss: 0.028115
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 18.72, NNZs: 16790, Bias: -1.178518, T: 22628, Avg. loss: 0.051335
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 23.50, NNZs: 29369, Bias: -1.027461, T: 124454, Avg. loss: 0.036826
Total training time: 0.30 seconds.
-- Epoch 12
-- Epoch 5
Norm: 23.28, NNZs: 23053, Bias: -1.161686, T: 45256, Avg. loss: 0.038365
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 23.48, NNZs: 33259, Bias: -1.129037, T: 33942, Avg. loss: 0.035986
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 23.51, NNZs: 24959, Bias: -1.044996, T: 56570, Avg. loss: 0.026791
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 22.67, NNZs: 23046, Bias: -1.206437, T: 33942, Avg. loss: 0.039099
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 23.13, NNZs: 24105, Bias: -1.142217, T: 56570, Avg. loss: 0.036402
Total training time: 0.11 seconds.
-- Epoch 6
Norm: 23.02, NNZs: 34892, Bias: -1.118993, T: 45256, Avg. loss: 0.033266
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 22.18, NNZs: 31387, Bias: -1.061484, T: 113140, Avg. loss: 0.027304
Total training time: 0.26 seconds.
-- Epoch 11
Norm: 22.42, NNZs: 24503, Bias: -1.172633, T: 45256, Avg. loss: 0.036095
Total training time: 0.05 seconds.
Norm: 23.16, NNZs: 24509, Bias: -1.118543, T: 67884, Avg. loss: 0.035166
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 22.85, NNZs: 39811, Bias: -1.098012, T: 56570, Avg. loss: 0.031316
Total training time: 0.07 seconds.
Norm: 21.42, NNZs: 20020, Bias: -1.070641, T: 33942, Avg. loss: 0.026980
Total training time: 0.08 seconds.
Norm: 24.35, NNZs: 31884, Bias: -1.092531, T: 56570, Avg. loss: 0.042197
-- Epoch 4
Norm: 22.18, NNZs: 31755, Bias: -1.060057, T: 124454, Avg. loss: 0.026962
Total training time: 0.27 seconds.
Convergence after 11 epochs took 0.27 seconds
Norm: 23.47, NNZs: 29547, Bias: -1.022957, T: 135768, Avg. loss: 0.036339
Total training time: 0.32 seconds.
Convergence after 12 epochs took 0.32 seconds
-- Epoch 5
Norm: 21.14, NNZs: 21426, Bias: -1.049096, T: 45256, Avg. loss: 0.025343
Norm: 23.61, NNZs: 26687, Bias: -1.023504, T: 56570, Avg. loss: 0.030491
Norm: 23.33, NNZs: 28949, Bias: -1.031227, T: 67884, Avg. loss: 0.025754
Total training time: 0.15 seconds.
Total training time: 0.13 seconds.
-- Epoch 6
-- Epoch 7
Norm: 22.30, NNZs: 25366, Bias: -1.152519, T: 56570, Avg. loss: 0.034187
Total training time: 0.06 seconds.
-- Epoch 6
Norm: 23.49, NNZs: 27580, Bias: -1.021080, T: 67884, Avg. loss: 0.029103
Total training time: 0.13 seconds.
-- Epoch 7
Norm: 23.24, NNZs: 29321, Bias: -1.017932, T: 79198, Avg. loss: 0.024824
Total training time: 0.16 seconds.
Total training time: 0.18 seconds.
-- Epoch 6
-- Epoch 8
Norm: 22.19, NNZs: 25795, Bias: -1.139880, T: 67884, Avg. loss: 0.032926
Total training time: 0.07 seconds.
-- Epoch 7
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 23.28, NNZs: 29763, Bias: -1.009089, T: 90512, Avg. loss: 0.024281
Total training time: 0.16 seconds.
-- Epoch 9
Norm: 22.15, NNZs: 26268, Bias: -1.119168, T: 79198, Avg. loss: 0.031964
Total training time: 0.07 seconds.
-- Epoch 8
Norm: 24.26, NNZs: 34892, Bias: -1.061660, T: 67884, Avg. loss: 0.041745
Total training time: 0.19 seconds.
-- Epoch 7
-- Epoch 6
Norm: 22.13, NNZs: 26603, Bias: -1.124429, T: 90512, Avg. loss: 0.031383
Total training time: 0.07 seconds.
-- Epoch 9
Norm: 18.29, NNZs: 18775, Bias: -1.158044, T: 33942, Avg. loss: 0.045310
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 22.69, NNZs: 41903, Bias: -1.074075, T: 67884, Avg. loss: 0.030415
Total training time: 0.09 seconds.
-- Epoch 7
Norm: 23.12, NNZs: 30074, Bias: -0.998911, T: 101826, Avg. loss: 0.023771
Total training time: 0.18 seconds.
Norm: 23.03, NNZs: 24838, Bias: -1.108001, T: 79198, Avg. loss: 0.034107
Total training time: 0.14 seconds.
-- Epoch 10
-- Epoch 8
Norm: 22.11, NNZs: 26924, Bias: -1.112293, T: 101826, Avg. loss: 0.030911
Total training time: 0.08 seconds.
-- Epoch 10
Norm: 24.20, NNZs: 35270, Bias: -1.061495, T: 79198, Avg. loss: 0.040139
Total training time: 0.20 seconds.
-- Epoch 8
Norm: 23.11, NNZs: 30886, Bias: -1.004737, T: 113140, Avg. loss: 0.023683
Total training time: 0.18 seconds.
Norm: 22.08, NNZs: 27601, Bias: -1.106991, T: 113140, Avg. loss: 0.030379
Total training time: 0.09 seconds.
-- Epoch 11
-- Epoch 11
Norm: 23.41, NNZs: 28100, Bias: -1.013246, T: 79198, Avg. loss: 0.028213
Total training time: 0.16 seconds.
-- Epoch 8
Norm: 20.77, NNZs: 22770, Bias: -1.037532, T: 56570, Avg. loss: 0.024107
Total training time: 0.12 seconds.
-- Epoch 6
Norm: 18.08, NNZs: 20144, Bias: -1.123876, T: 45256, Avg. loss: 0.042231
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 23.10, NNZs: 31074, Bias: -1.003084, T: 124454, Avg. loss: 0.023294
Total training time: 0.19 seconds.
Convergence after 11 epochs took 0.19 seconds
Norm: 24.19, NNZs: 35826, Bias: -1.047333, T: 90512, Avg. loss: 0.039445
Total training time: 0.21 seconds.
Norm: 17.98, NNZs: 20998, Bias: -1.094565, T: 56570, Avg. loss: 0.040456
Total training time: 0.07 seconds.
-- Epoch 9
-- Epoch 6
Norm: 23.00, NNZs: 25165, Bias: -1.097361, T: 90512, Avg. loss: 0.033625
Total training time: 0.15 seconds.
-- Epoch 9
Norm: 22.54, NNZs: 42499, Bias: -1.068645, T: 79198, Avg. loss: 0.029719
Total training time: 0.12 seconds.
-- Epoch 8
Norm: 20.62, NNZs: 23611, Bias: -1.022526, T: 67884, Avg. loss: 0.023288
Total training time: 0.13 seconds.
-- Epoch 7
Norm: 22.10, NNZs: 27787, Bias: -1.098450, T: 124454, Avg. loss: 0.030216
Total training time: 0.10 seconds.
Convergence after 11 epochs took 0.10 seconds
Norm: 23.03, NNZs: 25414, Bias: -1.105382, T: 101826, Avg. loss: 0.032925
Total training time: 0.16 seconds.
-- Epoch 10
Norm: 23.33, NNZs: 30270, Bias: -1.005720, T: 90512, Avg. loss: 0.027929
Total training time: 0.17 seconds.
Norm: 22.55, NNZs: 43450, Bias: -1.063771, T: 90512, Avg. loss: 0.029210
Total training time: 0.12 seconds.
-- Epoch 9
-- Epoch 9
Norm: 20.60, NNZs: 24158, Bias: -1.014532, T: 79198, Avg. loss: 0.022786
Total training time: 0.14 seconds.
Norm: 22.94, NNZs: 25762, Bias: -1.089460, T: 113140, Avg. loss: 0.032426
Total training time: 0.16 seconds.
Norm: 17.95, NNZs: 21868, Bias: -1.086560, T: 67884, Avg. loss: 0.039278
-- Epoch 8
Total training time: 0.09 seconds.
-- Epoch 11
-- Epoch 7
Norm: 24.16, NNZs: 38819, Bias: -1.040026, T: 101826, Avg. loss: 0.038917
Total training time: 0.22 seconds.
Norm: 23.26, NNZs: 30618, Bias: -1.003762, T: 101826, Avg. loss: 0.027369
Total training time: 0.18 seconds.
Norm: 22.50, NNZs: 43671, Bias: -1.060922, T: 101826, Avg. loss: 0.028503
Total training time: 0.13 seconds.
-- Epoch 10
-- Epoch 10
-- Epoch 10
Norm: 20.54, NNZs: 24844, Bias: -1.005494, T: 90512, Avg. loss: 0.022182
Total training time: 0.14 seconds.
-- Epoch 9
Norm: 17.93, NNZs: 22441, Bias: -1.083432, T: 79198, Avg. loss: 0.038476
Total training time: 0.09 seconds.
Norm: 22.97, NNZs: 25982, Bias: -1.091569, T: 124454, Avg. loss: 0.032122
Total training time: 0.17 seconds.
-- Epoch 8
-- Epoch 12
Norm: 24.16, NNZs: 39040, Bias: -1.035378, T: 113140, Avg. loss: 0.038264
Total training time: 0.23 seconds.
Norm: 23.23, NNZs: 30804, Bias: -1.008138, T: 113140, Avg. loss: 0.027114
Total training time: 0.18 seconds.
-- Epoch 11
Norm: 22.56, NNZs: 43924, Bias: -1.046023, T: 113140, Avg. loss: 0.028098
-- Epoch 11
Total training time: 0.13 seconds.
Convergence after 10 epochs took 0.13 seconds
Norm: 20.40, NNZs: 25245, Bias: -1.011320, T: 101826, Avg. loss: 0.021803
Norm: 17.91, NNZs: 22835, Bias: -1.085403, T: 90512, Avg. loss: 0.037843
Total training time: 0.14 seconds.
Total training time: 0.09 seconds.
-- Epoch 9
-- Epoch 10
Norm: 22.93, NNZs: 26136, Bias: -1.091904, T: 135768, Avg. loss: 0.031822
Total training time: 0.17 seconds.
Convergence after 12 epochs took 0.17 seconds
Norm: 23.22, NNZs: 31075, Bias: -1.006857, T: 124454, Avg. loss: 0.026853
Total training time: 0.18 seconds.
Norm: 24.15, NNZs: 39275, Bias: -1.029636, T: 124454, Avg. loss: 0.037918
Total training time: 0.23 seconds.
Convergence after 11 epochs took 0.18 seconds
-- Epoch 12
Norm: 17.92, NNZs: 23091, Bias: -1.080652, T: 101826, Avg. loss: 0.037346
Total training time: 0.10 seconds.
-- Epoch 10
Norm: 20.36, NNZs: 25583, Bias: -1.002859, T: 113140, Avg. loss: 0.021533
Total training time: 0.15 seconds.
Convergence after 10 epochs took 0.15 seconds
Norm: 24.13, NNZs: 42075, Bias: -1.016739, T: 135768, Avg. loss: 0.037604
Total training time: 0.23 seconds.
Norm: 17.91, NNZs: 23615, Bias: -1.062481, T: 113140, Avg. loss: 0.037074
Total training time: 0.10 seconds.
Convergence after 12 epochs took 0.23 seconds
-- Epoch 11
Norm: 17.92, NNZs: 24121, Bias: -1.057422, T: 124454, Avg. loss: 0.036924
Total training time: 0.10 seconds.
Convergence after 11 epochs took 0.10 seconds
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.5s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished
train time: 0.514s
test time:  0.012s
accuracy:   0.701


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 27.44, NNZs: 11981, Bias: -1.242817, T: 9051, Avg. loss: 0.080532
Total training time: 0.01 seconds.
Norm: 29.87, NNZs: 12774, Bias: -1.304411, T: 9051, Avg. loss: 0.091979
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 2
Norm: 27.85, NNZs: 9019, Bias: -1.149573, T: 9051, Avg. loss: 0.053240
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 28.37, NNZs: 10644, Bias: -1.413125, T: 9051, Avg. loss: 0.097934
Total training time: 0.01 seconds.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
Norm: 29.82, NNZs: 12345, Bias: -1.334658, T: 9051, Avg. loss: 0.071524
Norm: 29.96, NNZs: 12519, Bias: -1.238443, T: 9051, Avg. loss: 0.076471
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 29.32, NNZs: 11304, Bias: -1.276363, T: 9051, Avg. loss: 0.065344
Total training time: 0.04 seconds.
-- Epoch 2
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 1
Norm: 25.95, NNZs: 15418, Bias: -1.145889, T: 18102, Avg. loss: 0.040770
Norm: 25.64, NNZs: 14214, Bias: -1.134076, T: 18102, Avg. loss: 0.032633
Total training time: 0.04 seconds.
Norm: 24.08, NNZs: 11840, Bias: -1.090839, T: 18102, Avg. loss: 0.026885
Total training time: 0.04 seconds.
Total training time: 0.05 seconds.
-- Epoch 1
-- Epoch 2
Norm: 26.42, NNZs: 16023, Bias: -1.215061, T: 18102, Avg. loss: 0.051233
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 28.84, NNZs: 9396, Bias: -1.315114, T: 9051, Avg. loss: 0.078360
Total training time: 0.04 seconds.
Norm: 30.02, NNZs: 16029, Bias: -1.283984, T: 9051, Avg. loss: 0.085502
Total training time: 0.04 seconds.
-- Epoch 3
-- Epoch 2
Norm: 30.53, NNZs: 10860, Bias: -1.202743, T: 9051, Avg. loss: 0.066729
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 29.03, NNZs: 32138, Bias: -1.390709, T: 9051, Avg. loss: 0.091430
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 2
-- Epoch 3
Norm: 25.83, NNZs: 35483, Bias: -1.248029, T: 18102, Avg. loss: 0.051122
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 25.47, NNZs: 11889, Bias: -1.227205, T: 18102, Avg. loss: 0.041504
Total training time: 0.05 seconds.
Norm: 24.98, NNZs: 17666, Bias: -1.036223, T: 27153, Avg. loss: 0.034457
Total training time: 0.06 seconds.
-- Epoch 3
-- Epoch 4
Norm: 25.28, NNZs: 17724, Bias: -1.148168, T: 27153, Avg. loss: 0.042646
Total training time: 0.06 seconds.
Norm: 26.08, NNZs: 13978, Bias: -1.068173, T: 18102, Avg. loss: 0.032541
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 4
Norm: 27.07, NNZs: 13802, Bias: -1.422007, T: 9051, Avg. loss: 0.060658
Norm: 24.78, NNZs: 15036, Bias: -1.193453, T: 18102, Avg. loss: 0.044035
Total training time: 0.02 seconds.
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 24.63, NNZs: 37889, Bias: -1.219883, T: 27153, Avg. loss: 0.042181
Total training time: 0.02 seconds.
Norm: 24.28, NNZs: 20009, Bias: -1.017080, T: 36204, Avg. loss: 0.031060
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 5
Norm: 25.14, NNZs: 16069, Bias: -1.250279, T: 18102, Avg. loss: 0.037945
Total training time: 0.05 seconds.
Norm: 24.93, NNZs: 19386, Bias: -1.051554, T: 27153, Avg. loss: 0.027220
Total training time: 0.06 seconds.
-- Epoch 3
-- Epoch 4
Norm: 23.63, NNZs: 16476, Bias: -1.094329, T: 27153, Avg. loss: 0.036546
Total training time: 0.07 seconds.
-- Epoch 4
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
Norm: 24.37, NNZs: 39033, Bias: -1.183250, T: 36204, Avg. loss: 0.038046
Total training time: 0.03 seconds.
-- Epoch 5
Norm: 23.99, NNZs: 20856, Bias: -1.013049, T: 45255, Avg. loss: 0.029124
Total training time: 0.07 seconds.
-- Epoch 6
Norm: 24.19, NNZs: 18865, Bias: -1.159768, T: 27153, Avg. loss: 0.031331
Total training time: 0.06 seconds.
Norm: 24.36, NNZs: 20456, Bias: -1.012649, T: 36204, Avg. loss: 0.024084
Total training time: 0.06 seconds.
-- Epoch 4
-- Epoch 5
Norm: 23.17, NNZs: 17678, Bias: -1.113808, T: 36204, Avg. loss: 0.032781
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 23.82, NNZs: 21709, Bias: -1.015570, T: 54306, Avg. loss: 0.027662
Norm: 24.00, NNZs: 39958, Bias: -1.120429, T: 45255, Avg. loss: 0.035671
Total training time: 0.08 seconds.
Norm: 22.66, NNZs: 13618, Bias: -1.052258, T: 27153, Avg. loss: 0.021791
Total training time: 0.07 seconds.
Norm: 23.60, NNZs: 25130, Bias: -1.138752, T: 36204, Avg. loss: 0.027601
Total training time: 0.06 seconds.
Total training time: 0.04 seconds.
Norm: 26.10, NNZs: 18480, Bias: -1.149026, T: 18102, Avg. loss: 0.046510
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 24.26, NNZs: 20730, Bias: -1.159301, T: 27153, Avg. loss: 0.026607
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 24.74, NNZs: 20784, Bias: -1.096517, T: 36204, Avg. loss: 0.039015
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 5
Norm: 24.20, NNZs: 13298, Bias: -1.161827, T: 27153, Avg. loss: 0.034909
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 23.39, NNZs: 17098, Bias: -1.281466, T: 18102, Avg. loss: 0.031636
Total training time: 0.03 seconds.
Norm: 25.30, NNZs: 22763, Bias: -1.103357, T: 27153, Avg. loss: 0.038927
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 23.79, NNZs: 22241, Bias: -1.091749, T: 36204, Avg. loss: 0.023767
Total training time: 0.08 seconds.
-- Epoch 4
-- Epoch 5
Norm: 24.60, NNZs: 21517, Bias: -1.079827, T: 45255, Avg. loss: 0.036587
Total training time: 0.08 seconds.
-- Epoch 6
Norm: 25.32, NNZs: 12620, Bias: -1.310817, T: 18102, Avg. loss: 0.057052
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 23.73, NNZs: 14669, Bias: -1.147046, T: 36204, Avg. loss: 0.031835
Total training time: 0.08 seconds.
-- Epoch 5
Norm: 22.15, NNZs: 19514, Bias: -1.203733, T: 27153, Avg. loss: 0.026987
Total training time: 0.04 seconds.
-- Epoch 4
Norm: 23.59, NNZs: 26343, Bias: -1.085897, T: 45255, Avg. loss: 0.021829
Total training time: 0.08 seconds.
Norm: 24.49, NNZs: 22223, Bias: -1.063845, T: 54306, Avg. loss: 0.034936
Total training time: 0.08 seconds.
-- Epoch 6
-- Epoch 7
-- Epoch 5
Norm: 24.39, NNZs: 14216, Bias: -1.249559, T: 27153, Avg. loss: 0.047232
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 23.47, NNZs: 15772, Bias: -1.115132, T: 45255, Avg. loss: 0.029762
Total training time: 0.08 seconds.
Norm: 21.75, NNZs: 20920, Bias: -1.164609, T: 36204, Avg. loss: 0.024157
Total training time: 0.04 seconds.
-- Epoch 6
-- Epoch 5
Norm: 23.53, NNZs: 27679, Bias: -1.055675, T: 54306, Avg. loss: 0.020802
Total training time: 0.09 seconds.
-- Epoch 7
-- Epoch 6
-- Epoch 4
Norm: 23.30, NNZs: 26006, Bias: -1.109744, T: 45255, Avg. loss: 0.025673
Total training time: 0.08 seconds.
Norm: 23.09, NNZs: 18501, Bias: -1.069780, T: 45255, Avg. loss: 0.031000
Total training time: 0.09 seconds.
-- Epoch 6
-- Epoch 6
Norm: 23.16, NNZs: 18119, Bias: -1.102610, T: 54306, Avg. loss: 0.028678
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 23.38, NNZs: 28088, Bias: -1.045559, T: 63357, Avg. loss: 0.019804
Total training time: 0.09 seconds.
-- Epoch 8
Norm: 22.27, NNZs: 16324, Bias: -1.025553, T: 36204, Avg. loss: 0.019666
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 22.93, NNZs: 19141, Bias: -1.059684, T: 54306, Avg. loss: 0.029371
Total training time: 0.09 seconds.
Norm: 23.21, NNZs: 26495, Bias: -1.069960, T: 54306, Avg. loss: 0.024406
Total training time: 0.08 seconds.
-- Epoch 7
-- Epoch 7
Norm: 23.27, NNZs: 28388, Bias: -1.034063, T: 72408, Avg. loss: 0.019298
Total training time: 0.10 seconds.
Norm: 22.97, NNZs: 18505, Bias: -1.078022, T: 63357, Avg. loss: 0.027614
Total training time: 0.09 seconds.
-- Epoch 9
-- Epoch 8
-- Epoch 7
Norm: 23.94, NNZs: 16884, Bias: -1.209224, T: 36204, Avg. loss: 0.043349
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 23.79, NNZs: 40494, Bias: -1.123762, T: 54306, Avg. loss: 0.034558
Total training time: 0.06 seconds.
Norm: 23.00, NNZs: 18995, Bias: -1.076017, T: 72408, Avg. loss: 0.027063
Total training time: 0.09 seconds.
-- Epoch 9
Norm: 23.20, NNZs: 31277, Bias: -1.035041, T: 81459, Avg. loss: 0.018859
Total training time: 0.10 seconds.
Norm: 23.69, NNZs: 22589, Bias: -1.006383, T: 63357, Avg. loss: 0.026948
Total training time: 0.10 seconds.
-- Epoch 10
-- Epoch 8
Norm: 23.65, NNZs: 17862, Bias: -1.208853, T: 45255, Avg. loss: 0.040946
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 21.43, NNZs: 22921, Bias: -1.151959, T: 45255, Avg. loss: 0.022550
Total training time: 0.06 seconds.
-- Epoch 6
Norm: 23.21, NNZs: 31662, Bias: -1.018440, T: 90510, Avg. loss: 0.018361
Total training time: 0.11 seconds.
-- Epoch 11
Norm: 23.53, NNZs: 22910, Bias: -1.174845, T: 54306, Avg. loss: 0.039197
Total training time: 0.09 seconds.
-- Epoch 7
Norm: 24.49, NNZs: 23377, Bias: -1.058283, T: 63357, Avg. loss: 0.033836
Norm: 21.26, NNZs: 23672, Bias: -1.116248, T: 54306, Avg. loss: 0.021573
Total training time: 0.11 seconds.
Total training time: 0.06 seconds.
Norm: 21.94, NNZs: 17158, Bias: -1.013592, T: 45255, Avg. loss: 0.017985
Total training time: 0.10 seconds.
Norm: 24.04, NNZs: 22034, Bias: -1.003372, T: 45255, Avg. loss: 0.022668
Total training time: 0.10 seconds.
-- Epoch 6
Norm: 24.47, NNZs: 23925, Bias: -1.076183, T: 36204, Avg. loss: 0.034884
Total training time: 0.10 seconds.
-- Epoch 8
-- Epoch 6
-- Epoch 5
Norm: 23.58, NNZs: 23366, Bias: -1.007389, T: 72408, Avg. loss: 0.026086
Total training time: 0.11 seconds.
-- Epoch 9
Norm: 21.78, NNZs: 17912, Bias: -1.017545, T: 54306, Avg. loss: 0.017392
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 23.13, NNZs: 32155, Bias: -1.024500, T: 99561, Avg. loss: 0.017944
-- Epoch 7
Norm: 24.21, NNZs: 26470, Bias: -1.055826, T: 45255, Avg. loss: 0.033000
Total training time: 0.11 seconds.
Norm: 24.35, NNZs: 23907, Bias: -1.055627, T: 72408, Avg. loss: 0.032790
Total training time: 0.12 seconds.
-- Epoch 7
Total training time: 0.12 seconds.
Convergence after 11 epochs took 0.12 seconds
-- Epoch 6
Norm: 23.54, NNZs: 23362, Bias: -1.162425, T: 63357, Avg. loss: 0.037955
Norm: 23.53, NNZs: 23847, Bias: -1.004529, T: 81459, Avg. loss: 0.025675
Norm: 23.05, NNZs: 19406, Bias: -1.064215, T: 81459, Avg. loss: 0.026345
Norm: 23.81, NNZs: 22940, Bias: -1.007225, T: 54306, Avg. loss: 0.021490
Total training time: 0.11 seconds.
Total training time: 0.12 seconds.
Total training time: 0.11 seconds.
Total training time: 0.11 seconds.
-- Epoch 10
-- Epoch 10
Norm: 22.92, NNZs: 19636, Bias: -1.044748, T: 63357, Avg. loss: 0.028308
Total training time: 0.12 seconds.
-- Epoch 8
Norm: 21.16, NNZs: 27493, Bias: -1.123569, T: 63357, Avg. loss: 0.021001
Total training time: 0.08 seconds.
-- Epoch 8
Norm: 24.04, NNZs: 27251, Bias: -1.035643, T: 54306, Avg. loss: 0.031572
Total training time: 0.11 seconds.
-- Epoch 7
-- Epoch 8
Norm: 23.52, NNZs: 24245, Bias: -1.003228, T: 90510, Avg. loss: 0.025395
Total training time: 0.13 seconds.
-- Epoch 11
Norm: 22.84, NNZs: 20061, Bias: -1.045109, T: 72408, Avg. loss: 0.027408
Total training time: 0.13 seconds.
-- Epoch 9
Norm: 22.97, NNZs: 20322, Bias: -1.056039, T: 90510, Avg. loss: 0.025952
Total training time: 0.12 seconds.
-- Epoch 11
Norm: 21.13, NNZs: 27988, Bias: -1.079784, T: 72408, Avg. loss: 0.020301
Total training time: 0.08 seconds.
-- Epoch 9
Norm: 23.42, NNZs: 23786, Bias: -1.159791, T: 72408, Avg. loss: 0.037090
Total training time: 0.12 seconds.
-- Epoch 7
-- Epoch 9
Norm: 22.88, NNZs: 20394, Bias: -1.032102, T: 81459, Avg. loss: 0.026832
Total training time: 0.13 seconds.
Norm: 23.45, NNZs: 29182, Bias: -1.004082, T: 99561, Avg. loss: 0.024802
-- Epoch 1
Norm: 21.70, NNZs: 21043, Bias: -0.998575, T: 63357, Avg. loss: 0.016617
Total training time: 0.12 seconds.
Norm: 21.01, NNZs: 33417, Bias: -1.095678, T: 81459, Avg. loss: 0.019835
Total training time: 0.09 seconds.
-- Epoch 8
-- Epoch 10
-- Epoch 9
Norm: 23.77, NNZs: 40882, Bias: -1.103618, T: 63357, Avg. loss: 0.033075
Total training time: 0.09 seconds.
Norm: 23.53, NNZs: 23398, Bias: -1.009883, T: 63357, Avg. loss: 0.020772
Total training time: 0.12 seconds.
-- Epoch 8
-- Epoch 8
-- Epoch 10
Norm: 30.15, NNZs: 12744, Bias: -1.358841, T: 9051, Avg. loss: 0.097647
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 23.04, NNZs: 27713, Bias: -1.084856, T: 63357, Avg. loss: 0.023431
Total training time: 0.12 seconds.
Norm: 21.01, NNZs: 36444, Bias: -1.086862, T: 90510, Avg. loss: 0.019472
Total training time: 0.09 seconds.
-- Epoch 8
Convergence after 10 epochs took 0.09 seconds
Norm: 24.07, NNZs: 30018, Bias: -1.016687, T: 63357, Avg. loss: 0.030701
Norm: 24.32, NNZs: 24193, Bias: -1.043924, T: 81459, Avg. loss: 0.032287
Total training time: 0.14 seconds.
-- Epoch 10
Norm: 21.58, NNZs: 21735, Bias: -1.001702, T: 72408, Avg. loss: 0.016280
Total training time: 0.13 seconds.
Total training time: 0.13 seconds.
-- Epoch 9
-- Epoch 8
Total training time: 0.14 seconds.
Convergence after 11 epochs took 0.14 seconds
Norm: 23.48, NNZs: 25152, Bias: -1.145057, T: 81459, Avg. loss: 0.036361
Norm: 23.09, NNZs: 28458, Bias: -1.060319, T: 72408, Avg. loss: 0.023038
Total training time: 0.13 seconds.
-- Epoch 1
-- Epoch 1
Norm: 22.97, NNZs: 23036, Bias: -1.062871, T: 99561, Avg. loss: 0.025510
Total training time: 0.14 seconds.
Norm: 27.29, NNZs: 16126, Bias: -1.234022, T: 18102, Avg. loss: 0.054840
Total training time: 0.01 seconds.
-- Epoch 1
-- Epoch 1
-- Epoch 3
-- Epoch 1
[LibSVM]-- Epoch 1
Total training time: 0.14 seconds.
-- Epoch 10
Norm: 26.13, NNZs: 23118, Bias: -1.164608, T: 27153, Avg. loss: 0.044640
Total training time: 0.02 seconds.
-- Epoch 1
-- Epoch 4
Norm: 23.75, NNZs: 41603, Bias: -1.084842, T: 72408, Avg. loss: 0.032462
Total training time: 0.12 seconds.
Norm: 30.06, NNZs: 14943, Bias: -1.340795, T: 9051, Avg. loss: 0.067955
Total training time: 0.01 seconds.
-- Epoch 1
Norm: 23.41, NNZs: 25433, Bias: -1.147471, T: 90510, Avg. loss: 0.035921
Total training time: 0.14 seconds.
-- Epoch 11
-- Epoch 1
-- Epoch 9
-- Epoch 1
Norm: 30.20, NNZs: 13269, Bias: -1.321815, T: 9051, Avg. loss: 0.093256
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 12
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 23.99, NNZs: 30409, Bias: -1.002589, T: 72408, Avg. loss: 0.029663
Total training time: 0.15 seconds.
-- Epoch 9
-- Epoch 9
Norm: 22.98, NNZs: 23170, Bias: -1.051031, T: 108612, Avg. loss: 0.025238
Total training time: 0.16 seconds.
Convergence after 12 epochs took 0.16 seconds
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 22.92, NNZs: 31174, Bias: -1.058446, T: 81459, Avg. loss: 0.022393
Total training time: 0.15 seconds.
Norm: 22.81, NNZs: 20741, Bias: -1.012358, T: 90510, Avg. loss: 0.026382
Total training time: 0.17 seconds.
-- Epoch 10
-- Epoch 11
-- Epoch 1
-- Epoch 1
Norm: 23.70, NNZs: 41844, Bias: -1.093513, T: 81459, Avg. loss: 0.031829
Total training time: 0.13 seconds.
-- Epoch 10
Norm: 28.75, NNZs: 16008, Bias: -1.347528, T: 9051, Avg. loss: 0.085656
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 22.82, NNZs: 21037, Bias: -1.013356, T: 99561, Avg. loss: 0.026092
Total training time: 0.17 seconds.
-- Epoch 12
Norm: 23.92, NNZs: 30740, Bias: -1.009268, T: 81459, Avg. loss: 0.029045
Total training time: 0.16 seconds.
-- Epoch 10
Norm: 25.63, NNZs: 24452, Bias: -1.109025, T: 36204, Avg. loss: 0.040208
Total training time: 0.04 seconds.
-- Epoch 1
-- Epoch 5
Norm: 26.35, NNZs: 15700, Bias: -1.195648, T: 18102, Avg. loss: 0.050642
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 29.04, NNZs: 12408, Bias: -1.337854, T: 9051, Avg. loss: 0.071107
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 23.76, NNZs: 42050, Bias: -1.079753, T: 90510, Avg. loss: 0.031035
Total training time: 0.14 seconds.
Norm: 23.45, NNZs: 30148, Bias: -1.142494, T: 99561, Avg. loss: 0.035429
Total training time: 0.16 seconds.
-- Epoch 11
Norm: 25.82, NNZs: 18357, Bias: -1.300821, T: 18102, Avg. loss: 0.044626
Total training time: 0.01 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 12
-- Epoch 1
-- Epoch 1
Norm: 25.30, NNZs: 25759, Bias: -1.115720, T: 45255, Avg. loss: 0.037349
Total training time: 0.05 seconds.
-- Epoch 6
Norm: 22.94, NNZs: 31476, Bias: -1.047680, T: 90510, Avg. loss: 0.022165
Total training time: 0.17 seconds.
Norm: 24.29, NNZs: 24418, Bias: -1.038563, T: 90510, Avg. loss: 0.031906
Total training time: 0.18 seconds.
-- Epoch 11
-- Epoch 1
-- Epoch 11
-- Epoch 1
-- Epoch 1
Norm: 29.92, NNZs: 13148, Bias: -1.190042, T: 9051, Avg. loss: 0.074558
Total training time: 0.04 seconds.
Norm: 24.87, NNZs: 19500, Bias: -1.226440, T: 27153, Avg. loss: 0.036395
Total training time: 0.02 seconds.
-- Epoch 1
Norm: 29.33, NNZs: 33573, Bias: -1.272398, T: 9051, Avg. loss: 0.091341
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 4
-- Epoch 1
-- Epoch 2
-- Epoch 1
Norm: 23.98, NNZs: 33624, Bias: -1.007783, T: 90510, Avg. loss: 0.028519
Total training time: 0.18 seconds.
-- Epoch 1
-- Epoch 11
Norm: 26.34, NNZs: 17082, Bias: -1.103666, T: 18102, Avg. loss: 0.039475
Total training time: 0.05 seconds.
-- Epoch 1
-- Epoch 1
Norm: 24.46, NNZs: 20416, Bias: -1.170743, T: 36204, Avg. loss: 0.033359
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 1
-- Epoch 5
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 24.23, NNZs: 24654, Bias: -1.032668, T: 99561, Avg. loss: 0.031092
Total training time: 0.19 seconds.
Norm: 25.98, NNZs: 17707, Bias: -1.158629, T: 18102, Avg. loss: 0.034126
Total training time: 0.05 seconds.
-- Epoch 12
-- Epoch 3
Norm: 24.10, NNZs: 21097, Bias: -1.173291, T: 45255, Avg. loss: 0.030758
Total training time: 0.03 seconds.
-- Epoch 6
Norm: 22.79, NNZs: 21142, Bias: -1.013141, T: 108612, Avg. loss: 0.025762
Total training time: 0.20 seconds.
-- Epoch 1
Convergence after 12 epochs took 0.20 seconds
-- Epoch 1
Norm: 30.24, NNZs: 14259, Bias: -1.337974, T: 9051, Avg. loss: 0.092025
Total training time: 0.03 seconds.
-- Epoch 1
-- Epoch 2
Norm: 22.93, NNZs: 31704, Bias: -1.043924, T: 99561, Avg. loss: 0.021672
Total training time: 0.19 seconds.
Convergence after 11 epochs took 0.19 seconds
Norm: 27.96, NNZs: 10338, Bias: -1.441933, T: 9051, Avg. loss: 0.097501
Total training time: 0.05 seconds.
-- Epoch 1
-- Epoch 2
Norm: 27.07, NNZs: 13812, Bias: -1.359401, T: 9052, Avg. loss: 0.057211
Total training time: 0.02 seconds.
Norm: 30.35, NNZs: 8888, Bias: -1.248533, T: 9052, Avg. loss: 0.065324
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 1
Norm: 30.41, NNZs: 9992, Bias: -1.181280, T: 9052, Avg. loss: 0.074491
Norm: 28.82, NNZs: 32042, Bias: -1.413782, T: 9052, Avg. loss: 0.091023
Total training time: 0.01 seconds.
Norm: 29.52, NNZs: 13896, Bias: -1.292889, T: 9052, Avg. loss: 0.067499
Norm: 29.29, NNZs: 10888, Bias: -1.242763, T: 9051, Avg. loss: 0.086344
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 28.47, NNZs: 8335, Bias: -1.325210, T: 9051, Avg. loss: 0.079022
Total training time: 0.06 seconds.
-- Epoch 1
-- Epoch 2
-- Epoch 1
Norm: 28.65, NNZs: 10662, Bias: -1.442400, T: 9051, Avg. loss: 0.100275
Total training time: 0.04 seconds.
Total training time: 0.02 seconds.
-- Epoch 1
-- Epoch 2
-- Epoch 2
Norm: 30.19, NNZs: 12853, Bias: -1.264413, T: 9052, Avg. loss: 0.093486
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 25.62, NNZs: 17471, Bias: -1.236212, T: 18102, Avg. loss: 0.037673
Total training time: 0.05 seconds.
-- Epoch 3
Norm: 26.71, NNZs: 13357, Bias: -1.409315, T: 9051, Avg. loss: 0.059928
Norm: 25.15, NNZs: 11054, Bias: -1.233218, T: 18102, Avg. loss: 0.043287
Total training time: 0.07 seconds.
-- Epoch 3
-- Epoch 1
-- Epoch 1
Norm: 29.00, NNZs: 11342, Bias: -1.328725, T: 9051, Avg. loss: 0.069973
Total training time: 0.07 seconds.
Norm: 27.83, NNZs: 11852, Bias: -1.280615, T: 9051, Avg. loss: 0.081782
Total training time: 0.05 seconds.
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 2
Norm: 28.50, NNZs: 9587, Bias: -1.194495, T: 9051, Avg. loss: 0.050591
-- Epoch 2
Total training time: 0.02 seconds.
Norm: 27.45, NNZs: 11336, Bias: -1.414632, T: 9052, Avg. loss: 0.077580
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 28.40, NNZs: 8828, Bias: -1.163361, T: 9051, Avg. loss: 0.052194
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 29.83, NNZs: 8886, Bias: -1.224215, T: 9051, Avg. loss: 0.068236
Total training time: 0.01 seconds.
Total training time: 0.05 seconds.
-- Epoch 1
-- Epoch 2
Norm: 29.25, NNZs: 11171, Bias: -1.329358, T: 9051, Avg. loss: 0.068749
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 2
Norm: 28.51, NNZs: 9299, Bias: -1.427756, T: 9051, Avg. loss: 0.100874
Total training time: 0.05 seconds.
Norm: 29.29, NNZs: 32415, Bias: -1.350031, T: 9051, Avg. loss: 0.090284
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 2
-- Epoch 2
Norm: 29.08, NNZs: 13130, Bias: -1.279853, T: 9051, Avg. loss: 0.066102
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 30.44, NNZs: 9329, Bias: -1.225418, T: 9051, Avg. loss: 0.074306
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 29.50, NNZs: 10481, Bias: -1.201668, T: 9051, Avg. loss: 0.085852
Total training time: 0.05 seconds.
-- Epoch 2
-- Epoch 2
Norm: 26.51, NNZs: 13586, Bias: -1.395896, T: 9051, Avg. loss: 0.059756
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 24.88, NNZs: 14629, Bias: -1.197282, T: 18102, Avg. loss: 0.044533
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 23.43, NNZs: 16688, Bias: -1.293708, T: 18102, Avg. loss: 0.032304
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 30.50, NNZs: 14009, Bias: -1.237161, T: 9051, Avg. loss: 0.064854
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 30.37, NNZs: 9906, Bias: -1.241545, T: 9051, Avg. loss: 0.073571
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 25.24, NNZs: 15533, Bias: -1.256389, T: 18102, Avg. loss: 0.038842
Total training time: 0.08 seconds.
Norm: 23.96, NNZs: 12755, Bias: -1.172764, T: 27153, Avg. loss: 0.035686
Total training time: 0.09 seconds.
-- Epoch 3
-- Epoch 4
Norm: 25.71, NNZs: 14675, Bias: -1.211711, T: 18102, Avg. loss: 0.033938
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 30.37, NNZs: 9722, Bias: -1.260814, T: 9051, Avg. loss: 0.073544
Total training time: 0.04 seconds.
Norm: 26.89, NNZs: 13075, Bias: -1.428725, T: 9051, Avg. loss: 0.057890
Total training time: 0.04 seconds.
Norm: 24.17, NNZs: 19550, Bias: -1.197502, T: 27153, Avg. loss: 0.030989
Total training time: 0.07 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 4
Norm: 25.61, NNZs: 12374, Bias: -1.311135, T: 18102, Avg. loss: 0.057136
Norm: 29.49, NNZs: 11176, Bias: -1.200685, T: 9051, Avg. loss: 0.085927
Total training time: 0.07 seconds.
Total training time: 0.02 seconds.
-- Epoch 3
-- Epoch 2
Norm: 27.67, NNZs: 11822, Bias: -1.276334, T: 9051, Avg. loss: 0.082773
Total training time: 0.09 seconds.
Norm: 25.76, NNZs: 16966, Bias: -1.107928, T: 18102, Avg. loss: 0.032624
Total training time: 0.08 seconds.
-- Epoch 2
Norm: 27.91, NNZs: 8978, Bias: -1.327390, T: 9051, Avg. loss: 0.079550
Total training time: 0.07 seconds.
Norm: 25.35, NNZs: 37696, Bias: -1.284053, T: 18102, Avg. loss: 0.048955
Total training time: 0.08 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 3
Norm: 23.45, NNZs: 23732, Bias: -0.999786, T: 72408, Avg. loss: 0.020449
Norm: 29.23, NNZs: 11912, Bias: -1.317709, T: 9051, Avg. loss: 0.071042
Total training time: 0.08 seconds.
Norm: 29.00, NNZs: 37437, Bias: -1.310504, T: 9051, Avg. loss: 0.090496
Total training time: 0.07 seconds.
Norm: 30.69, NNZs: 14084, Bias: -1.392375, T: 9051, Avg. loss: 0.093769
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 2
-- Epoch 2
Norm: 29.08, NNZs: 12247, Bias: -1.298498, T: 9051, Avg. loss: 0.066278
Total training time: 0.08 seconds.
-- Epoch 2
Norm: 24.55, NNZs: 13963, Bias: -1.230321, T: 18104, Avg. loss: 0.041964
Total training time: 0.06 seconds.
-- Epoch 1
Total training time: 0.23 seconds.
Norm: 26.12, NNZs: 13561, Bias: -1.137846, T: 18102, Avg. loss: 0.048758
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 3
-- Epoch 9
Norm: 25.46, NNZs: 17159, Bias: -1.213505, T: 18104, Avg. loss: 0.032022
Total training time: 0.06 seconds.
Norm: 25.30, NNZs: 35534, Bias: -1.278532, T: 18104, Avg. loss: 0.048305
Total training time: 0.07 seconds.
Norm: 28.03, NNZs: 13194, Bias: -1.351263, T: 9052, Avg. loss: 0.071779
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 29.64, NNZs: 10921, Bias: -1.204861, T: 9052, Avg. loss: 0.085959
Total training time: 0.04 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 2
Norm: 25.80, NNZs: 15292, Bias: -1.112727, T: 18102, Avg. loss: 0.035093
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 23.26, NNZs: 14426, Bias: -1.125185, T: 36204, Avg. loss: 0.032705
Norm: 28.38, NNZs: 9153, Bias: -1.363392, T: 9051, Avg. loss: 0.077368
Total training time: 0.10 seconds.
Total training time: 0.05 seconds.
-- Epoch 5
-- Epoch 2
Norm: 27.89, NNZs: 9468, Bias: -1.217142, T: 9051, Avg. loss: 0.052430
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 27.38, NNZs: 11366, Bias: -1.253315, T: 9051, Avg. loss: 0.083130
Total training time: 0.08 seconds.
-- Epoch 2
Norm: 25.28, NNZs: 19199, Bias: -1.083521, T: 27153, Avg. loss: 0.031788
Total training time: 0.10 seconds.
Norm: 23.64, NNZs: 42236, Bias: -1.085198, T: 99561, Avg. loss: 0.030517
Total training time: 0.21 seconds.
-- Epoch 4
Norm: 30.43, NNZs: 8649, Bias: -1.221059, T: 9051, Avg. loss: 0.068125
-- Epoch 12
Total training time: 0.06 seconds.
Norm: 23.43, NNZs: 26496, Bias: -1.002528, T: 81459, Avg. loss: 0.020004
Total training time: 0.24 seconds.
-- Epoch 2
-- Epoch 10
Norm: 24.05, NNZs: 17717, Bias: -1.180058, T: 27153, Avg. loss: 0.031728
Total training time: 0.10 seconds.
-- Epoch 4
Norm: 23.50, NNZs: 16548, Bias: -1.289577, T: 18102, Avg. loss: 0.030752
Total training time: 0.06 seconds.
Norm: 21.52, NNZs: 22056, Bias: -1.001693, T: 81459, Avg. loss: 0.015797
Total training time: 0.24 seconds.
-- Epoch 3
Norm: 25.21, NNZs: 26537, Bias: -1.072717, T: 54306, Avg. loss: 0.036165
Total training time: 0.12 seconds.
-- Epoch 7
Norm: 24.19, NNZs: 21790, Bias: -1.130060, T: 54306, Avg. loss: 0.029384
Norm: 25.53, NNZs: 36657, Bias: -1.214884, T: 18102, Avg. loss: 0.049491
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 23.93, NNZs: 12881, Bias: -1.144456, T: 18102, Avg. loss: 0.024570
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 25.58, NNZs: 16297, Bias: -1.169934, T: 18102, Avg. loss: 0.048163
Total training time: 0.04 seconds.
Norm: 25.47, NNZs: 16666, Bias: -1.161357, T: 18102, Avg. loss: 0.033811
Total training time: 0.05 seconds.
-- Epoch 3
Norm: 22.49, NNZs: 19368, Bias: -1.201899, T: 27153, Avg. loss: 0.026091
Total training time: 0.06 seconds.
Total training time: 0.09 seconds.
Norm: 25.15, NNZs: 12401, Bias: -1.321163, T: 18102, Avg. loss: 0.055427
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 7
Norm: 24.77, NNZs: 14698, Bias: -1.223136, T: 18102, Avg. loss: 0.044671
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 24.74, NNZs: 20564, Bias: -1.046037, T: 36204, Avg. loss: 0.028831
Total training time: 0.12 seconds.
Norm: 24.26, NNZs: 25250, Bias: -1.028048, T: 108612, Avg. loss: 0.031133
Total training time: 0.26 seconds.
-- Epoch 5
-- Epoch 13
Norm: 24.65, NNZs: 19163, Bias: -1.239978, T: 27153, Avg. loss: 0.047540
Total training time: 0.09 seconds.
Norm: 23.37, NNZs: 28081, Bias: -1.001304, T: 90510, Avg. loss: 0.019727
Total training time: 0.25 seconds.
Norm: 23.41, NNZs: 30298, Bias: -1.125746, T: 108612, Avg. loss: 0.034969
-- Epoch 4
Total training time: 0.25 seconds.
-- Epoch 11
Convergence after 12 epochs took 0.25 seconds
-- Epoch 1
Norm: 23.84, NNZs: 16089, Bias: -1.113842, T: 27153, Avg. loss: 0.036868
Total training time: 0.10 seconds.
-- Epoch 10
Norm: 23.85, NNZs: 33784, Bias: -1.008444, T: 99561, Avg. loss: 0.028002
Norm: 24.90, NNZs: 19830, Bias: -1.112653, T: 27153, Avg. loss: 0.028426
Norm: 25.36, NNZs: 17782, Bias: -1.115914, T: 27153, Avg. loss: 0.042489
Total training time: 0.12 seconds.
-- Epoch 4
-- Epoch 1
Total training time: 0.26 seconds.
-- Epoch 12
Norm: 25.38, NNZs: 16272, Bias: -1.249935, T: 18102, Avg. loss: 0.038299
Total training time: 0.10 seconds.
Norm: 28.57, NNZs: 16141, Bias: -1.307881, T: 9051, Avg. loss: 0.074651
Total training time: 0.03 seconds.
Norm: 24.00, NNZs: 22270, Bias: -1.134238, T: 63357, Avg. loss: 0.027982
Total training time: 0.10 seconds.
-- Epoch 3
-- Epoch 2
-- Epoch 8
Norm: 25.79, NNZs: 15782, Bias: -1.182688, T: 18102, Avg. loss: 0.033835
Total training time: 0.11 seconds.
Norm: 24.76, NNZs: 22085, Bias: -1.125871, T: 27153, Avg. loss: 0.027574
Total training time: 0.07 seconds.
Norm: 24.71, NNZs: 21276, Bias: -1.031454, T: 27153, Avg. loss: 0.027204
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 4
Norm: 25.15, NNZs: 31891, Bias: -1.063702, T: 63357, Avg. loss: 0.034776
Total training time: 0.14 seconds.
-- Epoch 8
-- Epoch 1
Norm: 26.16, NNZs: 12362, Bias: -1.085547, T: 18102, Avg. loss: 0.038732
Total training time: 0.08 seconds.
Norm: 26.11, NNZs: 11637, Bias: -1.085785, T: 18104, Avg. loss: 0.032922
Total training time: 0.09 seconds.
Norm: 24.22, NNZs: 18067, Bias: -1.114037, T: 27153, Avg. loss: 0.028009
Total training time: 0.06 seconds.
-- Epoch 3
-- Epoch 3
Norm: 26.63, NNZs: 17848, Bias: -1.188397, T: 18102, Avg. loss: 0.049668
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 3
Norm: 24.07, NNZs: 12015, Bias: -1.111220, T: 18102, Avg. loss: 0.026280
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 27.35, NNZs: 13220, Bias: -1.193665, T: 9051, Avg. loss: 0.056589
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 23.70, NNZs: 17852, Bias: -1.263875, T: 18104, Avg. loss: 0.031545
Total training time: 0.09 seconds.
Norm: 24.08, NNZs: 22971, Bias: -1.099213, T: 36204, Avg. loss: 0.024300
Total training time: 0.08 seconds.
Norm: 22.53, NNZs: 19755, Bias: -1.194231, T: 27153, Avg. loss: 0.027034
Total training time: 0.11 seconds.
Norm: 23.76, NNZs: 15721, Bias: -1.161665, T: 27156, Avg. loss: 0.034829
Total training time: 0.10 seconds.
-- Epoch 4
-- Epoch 4
Norm: 26.18, NNZs: 12665, Bias: -1.123036, T: 18104, Avg. loss: 0.039885
Total training time: 0.09 seconds.
Norm: 26.09, NNZs: 13570, Bias: -1.134265, T: 18102, Avg. loss: 0.039618
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 24.61, NNZs: 40068, Bias: -1.213844, T: 27153, Avg. loss: 0.040904
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 24.71, NNZs: 38872, Bias: -1.201200, T: 27153, Avg. loss: 0.041324
Total training time: 0.10 seconds.
-- Epoch 4
-- Epoch 1
Norm: 21.46, NNZs: 25559, Bias: -1.006065, T: 90510, Avg. loss: 0.015631
Total training time: 0.27 seconds.
Norm: 24.28, NNZs: 18465, Bias: -1.148816, T: 27156, Avg. loss: 0.026875
Total training time: 0.10 seconds.
Convergence after 10 epochs took 0.27 seconds
Total training time: 0.14 seconds.
-- Epoch 4
-- Epoch 4
Norm: 25.18, NNZs: 11127, Bias: -1.211594, T: 18102, Avg. loss: 0.044581
Total training time: 0.11 seconds.
-- Epoch 3
-- Epoch 5
Norm: 25.19, NNZs: 11704, Bias: -1.241342, T: 18102, Avg. loss: 0.043042
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 23.08, NNZs: 15291, Bias: -1.100831, T: 45255, Avg. loss: 0.031236
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 24.90, NNZs: 17297, Bias: -1.128873, T: 27153, Avg. loss: 0.040417
Total training time: 0.09 seconds.
Norm: 26.23, NNZs: 15855, Bias: -1.145513, T: 18102, Avg. loss: 0.046869
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 3
Norm: 25.20, NNZs: 23912, Bias: -1.201293, T: 18102, Avg. loss: 0.039326
-- Epoch 3
Norm: 24.65, NNZs: 17206, Bias: -1.046464, T: 27153, Avg. loss: 0.028870
Total training time: 0.08 seconds.
-- Epoch 4
Norm: 23.99, NNZs: 22569, Bias: -1.127907, T: 72408, Avg. loss: 0.027541
Total training time: 0.12 seconds.
-- Epoch 9
Norm: 24.33, NNZs: 21589, Bias: -1.064978, T: 36204, Avg. loss: 0.025506
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 25.89, NNZs: 12428, Bias: -1.131264, T: 18102, Avg. loss: 0.039313
Total training time: 0.09 seconds.
Norm: 23.38, NNZs: 16511, Bias: -1.299209, T: 18102, Avg. loss: 0.031052
Total training time: 0.08 seconds.
-- Epoch 4
-- Epoch 3
Norm: 25.44, NNZs: 40457, Bias: -1.240594, T: 18102, Avg. loss: 0.047813
Total training time: 0.12 seconds.
-- Epoch 3
-- Epoch 3
Norm: 22.18, NNZs: 21169, Bias: -1.191384, T: 36204, Avg. loss: 0.024367
Total training time: 0.12 seconds.
Norm: 24.78, NNZs: 18126, Bias: -1.121034, T: 27153, Avg. loss: 0.041386
Total training time: 0.07 seconds.
Norm: 23.86, NNZs: 12555, Bias: -1.179986, T: 27153, Avg. loss: 0.035891
Total training time: 0.13 seconds.
-- Epoch 5
-- Epoch 4
-- Epoch 4
Norm: 24.19, NNZs: 23299, Bias: -1.020300, T: 36204, Avg. loss: 0.025670
Total training time: 0.08 seconds.
Norm: 23.67, NNZs: 42398, Bias: -1.071016, T: 108612, Avg. loss: 0.030284
Total training time: 0.26 seconds.
Convergence after 12 epochs took 0.26 seconds
Total training time: 0.06 seconds.
-- Epoch 1
Norm: 23.69, NNZs: 20837, Bias: -1.142319, T: 36204, Avg. loss: 0.028001
Total training time: 0.14 seconds.
Norm: 23.67, NNZs: 24458, Bias: -1.094856, T: 36208, Avg. loss: 0.024089
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 24.63, NNZs: 17671, Bias: -1.234185, T: 18104, Avg. loss: 0.039582
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 23.89, NNZs: 33995, Bias: -1.007129, T: 108612, Avg. loss: 0.027768
Total training time: 0.29 seconds.
-- Epoch 3
Norm: 29.05, NNZs: 11612, Bias: -1.290821, T: 9052, Avg. loss: 0.077507
Norm: 26.44, NNZs: 13692, Bias: -1.125052, T: 18104, Avg. loss: 0.047160
-- Epoch 3
-- Epoch 13
Norm: 24.92, NNZs: 21608, Bias: -1.111214, T: 36204, Avg. loss: 0.037884
Total training time: 0.15 seconds.
-- Epoch 3
-- Epoch 5
Total training time: 0.12 seconds.
-- Epoch 3
Norm: 22.34, NNZs: 11546, Bias: -1.276839, T: 9051, Avg. loss: 0.079135
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 24.98, NNZs: 32288, Bias: -1.051430, T: 72408, Avg. loss: 0.033732
Total training time: 0.17 seconds.
Norm: 24.03, NNZs: 12395, Bias: -1.099878, T: 18102, Avg. loss: 0.025142
Total training time: 0.10 seconds.
-- Epoch 9
-- Epoch 3
Norm: 23.90, NNZs: 13133, Bias: -1.157511, T: 27153, Avg. loss: 0.035879
Total training time: 0.11 seconds.
Norm: 27.21, NNZs: 15659, Bias: -1.394646, T: 9051, Avg. loss: 0.080533
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 24.82, NNZs: 14491, Bias: -1.027648, T: 27153, Avg. loss: 0.033427
Total training time: 0.11 seconds.
-- Epoch 2
Norm: 24.12, NNZs: 30125, Bias: -1.179359, T: 27153, Avg. loss: 0.032745
Total training time: 0.07 seconds.
Norm: 23.83, NNZs: 34362, Bias: -1.004315, T: 117663, Avg. loss: 0.027560
Total training time: 0.30 seconds.
-- Epoch 4
-- Epoch 4
Convergence after 13 epochs took 0.30 seconds
-- Epoch 2
Norm: 26.67, NNZs: 17021, Bias: -1.201119, T: 18102, Avg. loss: 0.048277
Total training time: 0.14 seconds.
-- Epoch 3
Norm: 24.12, NNZs: 20253, Bias: -1.227010, T: 36204, Avg. loss: 0.043172
Total training time: 0.14 seconds.
Norm: 23.22, NNZs: 16932, Bias: -1.118125, T: 36208, Avg. loss: 0.031160
Total training time: 0.13 seconds.
Norm: 24.61, NNZs: 38382, Bias: -1.200911, T: 27156, Avg. loss: 0.040790
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 22.41, NNZs: 20227, Bias: -1.186378, T: 27156, Avg. loss: 0.025869
Total training time: 0.13 seconds.
Norm: 25.10, NNZs: 14023, Bias: -1.241634, T: 18104, Avg. loss: 0.043592
Total training time: 0.13 seconds.
-- Epoch 4
-- Epoch 3
Norm: 25.98, NNZs: 12341, Bias: -1.087550, T: 18102, Avg. loss: 0.033554
Total training time: 0.13 seconds.
-- Epoch 3
-- Epoch 5
Norm: 23.89, NNZs: 19876, Bias: -1.164048, T: 27156, Avg. loss: 0.032747
Total training time: 0.10 seconds.
Norm: 24.21, NNZs: 39773, Bias: -1.157508, T: 36204, Avg. loss: 0.037093
Total training time: 0.13 seconds.
Total training time: 0.12 seconds.
-- Epoch 5
-- Epoch 3
Norm: 24.90, NNZs: 15239, Bias: -1.021255, T: 27156, Avg. loss: 0.027129
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 24.49, NNZs: 22078, Bias: -1.147940, T: 27153, Avg. loss: 0.027149
Total training time: 0.16 seconds.
-- Epoch 4
Norm: 27.67, NNZs: 9710, Bias: -1.247461, T: 9052, Avg. loss: 0.049581
Total training time: 0.06 seconds.
-- Epoch 2
-- Epoch 5
Norm: 23.27, NNZs: 20755, Bias: -1.144328, T: 36204, Avg. loss: 0.028412
Total training time: 0.18 seconds.
Norm: 24.64, NNZs: 14332, Bias: -1.182991, T: 18102, Avg. loss: 0.046607
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 22.45, NNZs: 19426, Bias: -1.195672, T: 27153, Avg. loss: 0.026307
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 4
-- Epoch 5
Norm: 24.98, NNZs: 15474, Bias: -1.045414, T: 27153, Avg. loss: 0.033482
Total training time: 0.14 seconds.
Norm: 23.79, NNZs: 16375, Bias: -1.129915, T: 27153, Avg. loss: 0.036766
Total training time: 0.18 seconds.
-- Epoch 4
-- Epoch 4
Norm: 23.13, NNZs: 17810, Bias: -1.108037, T: 45260, Avg. loss: 0.029148
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 26.39, NNZs: 15512, Bias: -1.204588, T: 18104, Avg. loss: 0.052036
Total training time: 0.14 seconds.
-- Epoch 3
Norm: 23.98, NNZs: 18328, Bias: -1.176850, T: 27153, Avg. loss: 0.030920
Total training time: 0.17 seconds.
-- Epoch 4
Norm: 23.44, NNZs: 26508, Bias: -1.128687, T: 45255, Avg. loss: 0.026127
Total training time: 0.17 seconds.
Norm: 23.21, NNZs: 21100, Bias: -1.138520, T: 36208, Avg. loss: 0.029122
Total training time: 0.12 seconds.
Norm: 21.86, NNZs: 21810, Bias: -1.139064, T: 36208, Avg. loss: 0.023544
Total training time: 0.15 seconds.
-- Epoch 6
-- Epoch 5
Norm: 23.92, NNZs: 25020, Bias: -1.196469, T: 45255, Avg. loss: 0.041096
Total training time: 0.17 seconds.
-- Epoch 5
-- Epoch 6
Norm: 23.28, NNZs: 17738, Bias: -1.150706, T: 36204, Avg. loss: 0.033082
Total training time: 0.19 seconds.
Norm: 23.97, NNZs: 12191, Bias: -1.135175, T: 18104, Avg. loss: 0.026320
Total training time: 0.07 seconds.
Norm: 24.93, NNZs: 14355, Bias: -1.063169, T: 27153, Avg. loss: 0.032403
-- Epoch 3
Norm: 24.18, NNZs: 18933, Bias: -1.233736, T: 27153, Avg. loss: 0.045777
Total training time: 0.18 seconds.
-- Epoch 3
-- Epoch 4
Norm: 24.56, NNZs: 22283, Bias: -1.028620, T: 45255, Avg. loss: 0.026677
Total training time: 0.19 seconds.
Norm: 22.70, NNZs: 13885, Bias: -1.066746, T: 27153, Avg. loss: 0.020883
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 24.17, NNZs: 39949, Bias: -1.183037, T: 36208, Avg. loss: 0.037183
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 25.02, NNZs: 18246, Bias: -1.066475, T: 27156, Avg. loss: 0.039703
Total training time: 0.14 seconds.
Norm: 24.64, NNZs: 14097, Bias: -1.041711, T: 27153, Avg. loss: 0.028719
Total training time: 0.15 seconds.
Norm: 24.02, NNZs: 24692, Bias: -1.098657, T: 36204, Avg. loss: 0.024526
Total training time: 0.18 seconds.
-- Epoch 4
-- Epoch 4
-- Epoch 5
-- Epoch 5
Norm: 23.89, NNZs: 40782, Bias: -1.126661, T: 45255, Avg. loss: 0.034873
Total training time: 0.16 seconds.
Norm: 21.68, NNZs: 22862, Bias: -1.143336, T: 45260, Avg. loss: 0.022044
Total training time: 0.16 seconds.
-- Epoch 6
-- Epoch 6
Norm: 23.53, NNZs: 19770, Bias: -1.218252, T: 36204, Avg. loss: 0.041967
Total training time: 0.19 seconds.
Norm: 22.10, NNZs: 17933, Bias: -1.053128, T: 36204, Avg. loss: 0.018619
Total training time: 0.13 seconds.
-- Epoch 5
-- Epoch 5
Norm: 28.23, NNZs: 9825, Bias: -1.447191, T: 9052, Avg. loss: 0.101296
Total training time: 0.06 seconds.
-- Epoch 2
Norm: 25.53, NNZs: 18534, Bias: -1.148049, T: 27153, Avg. loss: 0.040133
Total training time: 0.18 seconds.
Norm: 23.69, NNZs: 25262, Bias: -1.083156, T: 45255, Avg. loss: 0.022813
Total training time: 0.19 seconds.
Norm: 24.50, NNZs: 21150, Bias: -1.091102, T: 36204, Avg. loss: 0.036699
Total training time: 0.15 seconds.
Norm: 24.20, NNZs: 23140, Bias: -1.000054, T: 36204, Avg. loss: 0.024186
Total training time: 0.19 seconds.
-- Epoch 4
-- Epoch 6
Norm: 23.90, NNZs: 26350, Bias: -1.069220, T: 45255, Avg. loss: 0.022601
Total training time: 0.14 seconds.
Norm: 25.41, NNZs: 17556, Bias: -1.148002, T: 27156, Avg. loss: 0.043286
Total training time: 0.15 seconds.
-- Epoch 6
-- Epoch 4
Norm: 24.34, NNZs: 16894, Bias: -1.007233, T: 36204, Avg. loss: 0.029910
Total training time: 0.15 seconds.
Norm: 22.69, NNZs: 14292, Bias: -1.068278, T: 27153, Avg. loss: 0.020847
Total training time: 0.15 seconds.
Norm: 24.15, NNZs: 40807, Bias: -1.164257, T: 36204, Avg. loss: 0.036815
Total training time: 0.19 seconds.
-- Epoch 5
-- Epoch 4
-- Epoch 5
-- Epoch 5
Norm: 23.47, NNZs: 19879, Bias: -1.117296, T: 36204, Avg. loss: 0.028079
Total training time: 0.18 seconds.
Norm: 22.96, NNZs: 15968, Bias: -1.092466, T: 54306, Avg. loss: 0.029841
Total training time: 0.20 seconds.
-- Epoch 5
-- Epoch 7
Norm: 25.09, NNZs: 14708, Bias: -1.041125, T: 27156, Avg. loss: 0.033073
Total training time: 0.16 seconds.
-- Epoch 6
Total training time: 0.16 seconds.
-- Epoch 4
Norm: 23.65, NNZs: 16398, Bias: -1.171804, T: 18102, Avg. loss: 0.028378
Total training time: 0.08 seconds.
Norm: 24.29, NNZs: 21179, Bias: -1.028129, T: 36204, Avg. loss: 0.030370
Total training time: 0.15 seconds.
-- Epoch 5
Norm: 23.31, NNZs: 13553, Bias: -1.119305, T: 36204, Avg. loss: 0.033465
Total training time: 0.18 seconds.
Norm: 24.49, NNZs: 19456, Bias: -1.029249, T: 36208, Avg. loss: 0.035961
Total training time: 0.15 seconds.
-- Epoch 5
-- Epoch 5
Norm: 23.62, NNZs: 28520, Bias: -1.045372, T: 54306, Avg. loss: 0.021704
Total training time: 0.19 seconds.
-- Epoch 7
Norm: 24.73, NNZs: 22148, Bias: -1.080123, T: 45255, Avg. loss: 0.035865
-- Epoch 5
Norm: 23.80, NNZs: 19091, Bias: -1.082742, T: 36204, Avg. loss: 0.025520
Total training time: 0.21 seconds.
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 23.40, NNZs: 17378, Bias: -1.107630, T: 36204, Avg. loss: 0.032589
Total training time: 0.19 seconds.
-- Epoch 6
-- Epoch 5
Norm: 23.52, NNZs: 14306, Bias: -1.122747, T: 36204, Avg. loss: 0.032642
Total training time: 0.16 seconds.
Norm: 23.90, NNZs: 24491, Bias: -0.998916, T: 45255, Avg. loss: 0.022320
Total training time: 0.20 seconds.
Norm: 25.18, NNZs: 20746, Bias: -1.092065, T: 36208, Avg. loss: 0.039390
Total training time: 0.16 seconds.
-- Epoch 5
-- Epoch 6
Norm: 25.15, NNZs: 11327, Bias: -1.300526, T: 18102, Avg. loss: 0.056325
Norm: 25.36, NNZs: 19916, Bias: -1.141630, T: 27153, Avg. loss: 0.042163
Total training time: 0.16 seconds.
-- Epoch 4
-- Epoch 4
Norm: 24.23, NNZs: 22060, Bias: -1.053137, T: 45255, Avg. loss: 0.034191
Total training time: 0.16 seconds.
Norm: 22.85, NNZs: 16684, Bias: -1.087584, T: 63357, Avg. loss: 0.028577
Total training time: 0.21 seconds.
Norm: 25.11, NNZs: 19570, Bias: -1.102915, T: 27153, Avg. loss: 0.039482
Total training time: 0.18 seconds.
-- Epoch 6
-- Epoch 8
Norm: 24.08, NNZs: 22484, Bias: -1.071238, T: 36204, Avg. loss: 0.037030
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 23.20, NNZs: 19096, Bias: -1.088580, T: 45255, Avg. loss: 0.031307
Total training time: 0.21 seconds.
-- Epoch 6
-- Epoch 4
Norm: 24.61, NNZs: 42447, Bias: -1.206643, T: 27153, Avg. loss: 0.040455
Total training time: 0.19 seconds.
Total training time: 0.19 seconds.
Norm: 23.22, NNZs: 27775, Bias: -1.108173, T: 54306, Avg. loss: 0.024767
Total training time: 0.20 seconds.
Norm: 22.85, NNZs: 13893, Bias: -1.087520, T: 27156, Avg. loss: 0.021785
Total training time: 0.10 seconds.
-- Epoch 4
-- Epoch 7
-- Epoch 4
Norm: 23.65, NNZs: 15824, Bias: -1.142984, T: 27153, Avg. loss: 0.038585
-- Epoch 5
Norm: 23.39, NNZs: 25082, Bias: -1.083172, T: 45260, Avg. loss: 0.022288
Total training time: 0.18 seconds.
Norm: 23.62, NNZs: 27551, Bias: -1.006764, T: 54306, Avg. loss: 0.021261
Total training time: 0.21 seconds.
Norm: 23.78, NNZs: 25580, Bias: -1.174923, T: 54306, Avg. loss: 0.039116
Total training time: 0.19 seconds.
-- Epoch 6
-- Epoch 7
-- Epoch 7
Norm: 25.00, NNZs: 21522, Bias: -1.111207, T: 36204, Avg. loss: 0.036217
Total training time: 0.19 seconds.
Norm: 23.40, NNZs: 23464, Bias: -1.048888, T: 45255, Avg. loss: 0.022964
Total training time: 0.15 seconds.
-- Epoch 5
-- Epoch 6
-- Epoch 3
Norm: 24.27, NNZs: 25404, Bias: -1.014041, T: 117663, Avg. loss: 0.030602
Total training time: 0.36 seconds.
Norm: 22.91, NNZs: 19809, Bias: -1.079973, T: 54306, Avg. loss: 0.029649
Total training time: 0.22 seconds.
Convergence after 13 epochs took 0.36 seconds
Total training time: 0.20 seconds.
-- Epoch 7
Norm: 24.15, NNZs: 18306, Bias: -1.025113, T: 36204, Avg. loss: 0.025736
Total training time: 0.18 seconds.
-- Epoch 3
-- Epoch 4
Norm: 24.26, NNZs: 43491, Bias: -1.159036, T: 36204, Avg. loss: 0.036158
Total training time: 0.19 seconds.
-- Epoch 5
-- Epoch 5
Norm: 22.97, NNZs: 18509, Bias: -1.083579, T: 54312, Avg. loss: 0.027633
Total training time: 0.19 seconds.
Norm: 24.85, NNZs: 22651, Bias: -1.112083, T: 36204, Avg. loss: 0.038369
Total training time: 0.17 seconds.
-- Epoch 7
-- Epoch 5
Norm: 23.57, NNZs: 22725, Bias: -1.203375, T: 45255, Avg. loss: 0.039824
Total training time: 0.22 seconds.
Norm: 25.08, NNZs: 11852, Bias: -1.337775, T: 18104, Avg. loss: 0.056855
Total training time: 0.09 seconds.
Norm: 23.95, NNZs: 22115, Bias: -1.000378, T: 45255, Avg. loss: 0.028624
Total training time: 0.17 seconds.
-- Epoch 6
-- Epoch 3
-- Epoch 6
Norm: 22.44, NNZs: 18235, Bias: -1.096388, T: 27153, Avg. loss: 0.024155
Norm: 22.86, NNZs: 13475, Bias: -1.028342, T: 27153, Avg. loss: 0.021923
Total training time: 0.17 seconds.
-- Epoch 4
Norm: 23.36, NNZs: 28317, Bias: -1.000309, T: 99561, Avg. loss: 0.019244
Total training time: 0.36 seconds.
Norm: 23.16, NNZs: 21843, Bias: -1.125844, T: 45255, Avg. loss: 0.026588
Total training time: 0.23 seconds.
Norm: 24.29, NNZs: 12662, Bias: -1.259824, T: 27153, Avg. loss: 0.047273
Total training time: 0.20 seconds.
Norm: 23.18, NNZs: 17680, Bias: -1.127880, T: 36204, Avg. loss: 0.034288
Norm: 22.93, NNZs: 14476, Bias: -1.102384, T: 45255, Avg. loss: 0.031563
Norm: 23.43, NNZs: 28988, Bias: -1.035621, T: 63357, Avg. loss: 0.020775
Total training time: 0.22 seconds.
Convergence after 11 epochs took 0.36 seconds
Norm: 21.94, NNZs: 20653, Bias: -1.182671, T: 36204, Avg. loss: 0.023447
Total training time: 0.16 seconds.
Total training time: 0.21 seconds.
-- Epoch 6
Norm: 20.54, NNZs: 14203, Bias: -1.275040, T: 18102, Avg. loss: 0.049793
-- Epoch 8
Total training time: 0.08 seconds.
-- Epoch 5
-- Epoch 3
Norm: 24.80, NNZs: 23208, Bias: -1.085194, T: 45255, Avg. loss: 0.036144
Total training time: 0.18 seconds.
-- Epoch 5
Norm: 21.50, NNZs: 24245, Bias: -1.125459, T: 54312, Avg. loss: 0.020877
Total training time: 0.19 seconds.
-- Epoch 6
-- Epoch 7
Norm: 23.88, NNZs: 41680, Bias: -1.145348, T: 45255, Avg. loss: 0.034831
Total training time: 0.22 seconds.
Norm: 21.70, NNZs: 22975, Bias: -1.153319, T: 45255, Avg. loss: 0.022824
-- Epoch 6
Total training time: 0.21 seconds.
Norm: 23.75, NNZs: 23160, Bias: -1.010097, T: 54306, Avg. loss: 0.027144
Total training time: 0.18 seconds.
Norm: 24.65, NNZs: 27430, Bias: -1.063680, T: 54306, Avg. loss: 0.034137
Total training time: 0.23 seconds.
-- Epoch 7
-- Epoch 7
Norm: 24.00, NNZs: 18970, Bias: -1.000241, T: 45255, Avg. loss: 0.023883
Total training time: 0.19 seconds.
-- Epoch 6
Norm: 24.54, NNZs: 18887, Bias: -1.305709, T: 18102, Avg. loss: 0.044374
Norm: 24.12, NNZs: 23466, Bias: -1.029468, T: 45255, Avg. loss: 0.023597
-- Epoch 5
Total training time: 0.23 seconds.
Total training time: 0.11 seconds.
Norm: 23.78, NNZs: 41418, Bias: -1.103670, T: 54306, Avg. loss: 0.033816
Total training time: 0.20 seconds.
-- Epoch 6
-- Epoch 3
Norm: 24.47, NNZs: 16041, Bias: -1.016366, T: 36208, Avg. loss: 0.030288
Total training time: 0.19 seconds.
Norm: 22.14, NNZs: 21292, Bias: -1.184000, T: 36204, Avg. loss: 0.023228
-- Epoch 6
Total training time: 0.18 seconds.
-- Epoch 7
-- Epoch 5
-- Epoch 5
Norm: 24.28, NNZs: 25039, Bias: -1.022329, T: 54306, Avg. loss: 0.024962
Total training time: 0.24 seconds.
-- Epoch 7
Norm: 23.39, NNZs: 29454, Bias: -1.024271, T: 72408, Avg. loss: 0.020227
Total training time: 0.22 seconds.
Norm: 23.78, NNZs: 23960, Bias: -1.007070, T: 45255, Avg. loss: 0.024173
Total training time: 0.17 seconds.
-- Epoch 9
-- Epoch 6
-- Epoch 4
Norm: 24.03, NNZs: 41044, Bias: -1.154515, T: 45260, Avg. loss: 0.034722
Total training time: 0.21 seconds.
-- Epoch 6
Norm: 24.67, NNZs: 22011, Bias: -1.073947, T: 45260, Avg. loss: 0.035984
Total training time: 0.19 seconds.
Norm: 23.68, NNZs: 41789, Bias: -1.084465, T: 63357, Avg. loss: 0.032249
Total training time: 0.20 seconds.
Total training time: 0.22 seconds.
Norm: 23.84, NNZs: 24512, Bias: -1.018896, T: 54306, Avg. loss: 0.022319
Total training time: 0.24 seconds.
Norm: 23.24, NNZs: 21614, Bias: -1.115778, T: 45255, Avg. loss: 0.026271
Total training time: 0.22 seconds.
-- Epoch 6
-- Epoch 7
-- Epoch 8
Norm: 25.04, NNZs: 35442, Bias: -1.027804, T: 81459, Avg. loss: 0.033290
Total training time: 0.26 seconds.
-- Epoch 6
Norm: 23.91, NNZs: 22918, Bias: -1.117192, T: 81459, Avg. loss: 0.026506
Total training time: 0.22 seconds.
-- Epoch 10
-- Epoch 10
Norm: 23.63, NNZs: 26152, Bias: -1.017355, T: 54306, Avg. loss: 0.023288
Norm: 22.89, NNZs: 20302, Bias: -1.077245, T: 63357, Avg. loss: 0.028807
Total training time: 0.18 seconds.
Total training time: 0.24 seconds.
-- Epoch 7
-- Epoch 8
Norm: 22.27, NNZs: 17723, Bias: -1.026255, T: 36204, Avg. loss: 0.019192
Total training time: 0.19 seconds.
Norm: 23.69, NNZs: 15778, Bias: -1.159182, T: 27156, Avg. loss: 0.036144
Total training time: 0.21 seconds.
Norm: 23.80, NNZs: 15138, Bias: -1.232122, T: 36204, Avg. loss: 0.043169
Total training time: 0.22 seconds.
-- Epoch 5
-- Epoch 4
-- Epoch 5
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 23.30, NNZs: 28275, Bias: -1.061534, T: 54312, Avg. loss: 0.020972
Total training time: 0.21 seconds.
Norm: 23.73, NNZs: 21009, Bias: -1.229176, T: 27153, Avg. loss: 0.035751
Total training time: 0.12 seconds.
Norm: 23.81, NNZs: 26057, Bias: -1.163623, T: 63357, Avg. loss: 0.038072
Total training time: 0.23 seconds.
-- Epoch 7
-- Epoch 4
-- Epoch 8
Norm: 22.99, NNZs: 22394, Bias: -1.112449, T: 54306, Avg. loss: 0.025354
Total training time: 0.25 seconds.
-- Epoch 7
Norm: 21.90, NNZs: 22556, Bias: -1.148169, T: 45255, Avg. loss: 0.021957
Norm: 24.78, NNZs: 22654, Bias: -1.085942, T: 45255, Avg. loss: 0.033945
Total training time: 0.20 seconds.
Total training time: 0.23 seconds.
-- Epoch 6
-- Epoch 6
Norm: 23.76, NNZs: 31118, Bias: -1.128941, T: 36204, Avg. loss: 0.029264
Total training time: 0.16 seconds.
Norm: 22.95, NNZs: 20721, Bias: -1.065733, T: 72408, Avg. loss: 0.027879
Total training time: 0.25 seconds.
Norm: 24.27, NNZs: 22039, Bias: -1.024145, T: 45260, Avg. loss: 0.033836
-- Epoch 5
Total training time: 0.20 seconds.
-- Epoch 9
-- Epoch 6
Norm: 21.55, NNZs: 22215, Bias: -1.144714, T: 45255, Avg. loss: 0.022042
Total training time: 0.18 seconds.
-- Epoch 6
Norm: 24.12, NNZs: 25556, Bias: -1.002879, T: 63357, Avg. loss: 0.024350
Total training time: 0.26 seconds.
Norm: 23.77, NNZs: 30625, Bias: -1.152851, T: 72408, Avg. loss: 0.036745
-- Epoch 8
Total training time: 0.23 seconds.
Norm: 24.29, NNZs: 18524, Bias: -1.275787, T: 27156, Avg. loss: 0.048249
Total training time: 0.12 seconds.
-- Epoch 9
-- Epoch 4
Norm: 23.96, NNZs: 23048, Bias: -1.102377, T: 90510, Avg. loss: 0.026198
Total training time: 0.24 seconds.
Norm: 22.49, NNZs: 15308, Bias: -1.033211, T: 36208, Avg. loss: 0.018948
Total training time: 0.14 seconds.
-- Epoch 11
Norm: 23.81, NNZs: 27661, Bias: -1.069532, T: 54306, Avg. loss: 0.021349
Total training time: 0.20 seconds.
-- Epoch 5
-- Epoch 7
Norm: 24.29, NNZs: 21948, Bias: -1.024596, T: 36208, Avg. loss: 0.024397
Total training time: 0.22 seconds.
-- Epoch 5
Norm: 22.09, NNZs: 18791, Bias: -1.051053, T: 36204, Avg. loss: 0.018371
Norm: 23.88, NNZs: 25755, Bias: -1.050294, T: 45255, Avg. loss: 0.035028
Total training time: 0.21 seconds.
Total training time: 0.19 seconds.
Norm: 23.32, NNZs: 33950, Bias: -1.035028, T: 81459, Avg. loss: 0.019813
Total training time: 0.25 seconds.
-- Epoch 5
-- Epoch 6
-- Epoch 10
Norm: 19.98, NNZs: 16378, Bias: -1.139365, T: 27153, Avg. loss: 0.041940
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 23.22, NNZs: 15699, Bias: -1.112512, T: 45255, Avg. loss: 0.030868
Total training time: 0.21 seconds.
-- Epoch 6
Norm: 22.92, NNZs: 18878, Bias: -1.069222, T: 63364, Avg. loss: 0.026572
Total training time: 0.23 seconds.
-- Epoch 8
Norm: 23.80, NNZs: 27836, Bias: -1.002097, T: 63357, Avg. loss: 0.021473
Total training time: 0.27 seconds.
Norm: 23.37, NNZs: 23360, Bias: -1.186357, T: 54306, Avg. loss: 0.037878
Total training time: 0.26 seconds.
Norm: 23.61, NNZs: 42487, Bias: -1.068719, T: 72408, Avg. loss: 0.031705
Total training time: 0.23 seconds.
-- Epoch 8
-- Epoch 7
Norm: 23.30, NNZs: 18118, Bias: -1.076882, T: 45255, Avg. loss: 0.030783
Total training time: 0.24 seconds.
Norm: 24.36, NNZs: 15851, Bias: -1.024058, T: 36204, Avg. loss: 0.029143
Total training time: 0.22 seconds.
-- Epoch 9
Norm: 23.26, NNZs: 24055, Bias: -1.032539, T: 54306, Avg. loss: 0.022396
Total training time: 0.20 seconds.
-- Epoch 6
-- Epoch 5
Norm: 21.91, NNZs: 18413, Bias: -1.017606, T: 45255, Avg. loss: 0.018287
Total training time: 0.21 seconds.
Norm: 23.35, NNZs: 34094, Bias: -1.014583, T: 90510, Avg. loss: 0.019286
Total training time: 0.25 seconds.
-- Epoch 7
Norm: 23.04, NNZs: 18592, Bias: -1.097949, T: 45255, Avg. loss: 0.032461
Total training time: 0.25 seconds.
-- Epoch 6
-- Epoch 11
-- Epoch 6
Norm: 21.93, NNZs: 19775, Bias: -1.078723, T: 36204, Avg. loss: 0.021594
Total training time: 0.15 seconds.
Norm: 24.68, NNZs: 23800, Bias: -1.069622, T: 54306, Avg. loss: 0.034208
Total training time: 0.21 seconds.
-- Epoch 5
-- Epoch 7
Norm: 23.83, NNZs: 42278, Bias: -1.131653, T: 54306, Avg. loss: 0.033060
Total training time: 0.26 seconds.
Norm: 23.36, NNZs: 22160, Bias: -1.174842, T: 36204, Avg. loss: 0.032513
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 23.91, NNZs: 41498, Bias: -1.135496, T: 54312, Avg. loss: 0.032869
Total training time: 0.24 seconds.
-- Epoch 5
Norm: 23.13, NNZs: 27378, Bias: -1.088111, T: 54306, Avg. loss: 0.024610
Total training time: 0.25 seconds.
Norm: 21.94, NNZs: 20714, Bias: -1.027908, T: 45260, Avg. loss: 0.017713
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 21.62, NNZs: 28269, Bias: -1.113999, T: 54306, Avg. loss: 0.021909
Total training time: 0.25 seconds.
-- Epoch 7
Norm: 23.15, NNZs: 28099, Bias: -1.096266, T: 63357, Avg. loss: 0.024256
Total training time: 0.26 seconds.
-- Epoch 6
Norm: 23.55, NNZs: 28371, Bias: -1.001822, T: 63357, Avg. loss: 0.020687
Total training time: 0.26 seconds.
Norm: 24.16, NNZs: 17291, Bias: -1.004121, T: 45260, Avg. loss: 0.027816
Total training time: 0.23 seconds.
-- Epoch 8
-- Epoch 8
Norm: 23.48, NNZs: 33770, Bias: -1.114073, T: 45255, Avg. loss: 0.027342
Total training time: 0.18 seconds.
Norm: 22.88, NNZs: 21126, Bias: -1.064901, T: 81459, Avg. loss: 0.027159
Total training time: 0.27 seconds.
Norm: 23.69, NNZs: 23802, Bias: -1.001300, T: 63357, Avg. loss: 0.026336
Total training time: 0.22 seconds.
-- Epoch 6
-- Epoch 10
-- Epoch 8
Norm: 24.56, NNZs: 21153, Bias: -1.072314, T: 36204, Avg. loss: 0.035492
Total training time: 0.24 seconds.
Norm: 22.72, NNZs: 17043, Bias: -1.067196, T: 72408, Avg. loss: 0.027943
Total training time: 0.28 seconds.
-- Epoch 5
-- Epoch 9
-- Epoch 7
Norm: 23.86, NNZs: 16217, Bias: -1.194527, T: 45255, Avg. loss: 0.040696
Total training time: 0.25 seconds.
-- Epoch 6
Norm: 22.96, NNZs: 19308, Bias: -1.070703, T: 54306, Avg. loss: 0.030800
Total training time: 0.26 seconds.
Norm: 24.71, NNZs: 22918, Bias: -1.067009, T: 54312, Avg. loss: 0.034889
Total training time: 0.23 seconds.
Norm: 23.79, NNZs: 42399, Bias: -1.117284, T: 63364, Avg. loss: 0.031941
Total training time: 0.25 seconds.
-- Epoch 7
-- Epoch 7
-- Epoch 6
Norm: 24.10, NNZs: 45376, Bias: -1.139544, T: 45255, Avg. loss: 0.033670
Total training time: 0.26 seconds.
Norm: 22.80, NNZs: 21337, Bias: -1.049208, T: 90510, Avg. loss: 0.026499
Total training time: 0.28 seconds.
-- Epoch 6
Norm: 23.55, NNZs: 24784, Bias: -1.002987, T: 72408, Avg. loss: 0.025474
Total training time: 0.23 seconds.
-- Epoch 11
-- Epoch 9
Norm: 23.40, NNZs: 17146, Bias: -1.114491, T: 36208, Avg. loss: 0.033704
Total training time: 0.25 seconds.
Norm: 23.66, NNZs: 19483, Bias: -1.011722, T: 54306, Avg. loss: 0.022549
Total training time: 0.24 seconds.
-- Epoch 5
-- Epoch 7
Norm: 21.82, NNZs: 19045, Bias: -1.012908, T: 45255, Avg. loss: 0.017642
Total training time: 0.22 seconds.
Norm: 23.99, NNZs: 22792, Bias: -1.039986, T: 54306, Avg. loss: 0.032685
Total training time: 0.24 seconds.
-- Epoch 7
Norm: 21.30, NNZs: 23278, Bias: -1.119759, T: 54306, Avg. loss: 0.020983
Total training time: 0.22 seconds.
Norm: 23.99, NNZs: 23153, Bias: -1.000466, T: 54312, Avg. loss: 0.026617
Total training time: 0.24 seconds.
-- Epoch 7
Norm: 22.87, NNZs: 15249, Bias: -1.089431, T: 54306, Avg. loss: 0.030302
Total training time: 0.27 seconds.
-- Epoch 7
-- Epoch 7
Norm: 24.71, NNZs: 23761, Bias: -1.071308, T: 54306, Avg. loss: 0.032402
Total training time: 0.27 seconds.
Norm: 24.56, NNZs: 28224, Bias: -1.063674, T: 63357, Avg. loss: 0.033040
Total training time: 0.29 seconds.
-- Epoch 7
Norm: 23.92, NNZs: 21184, Bias: -0.996463, T: 45255, Avg. loss: 0.028066
Total training time: 0.24 seconds.
-- Epoch 8
Norm: 24.12, NNZs: 22788, Bias: -1.009321, T: 54312, Avg. loss: 0.032390
Total training time: 0.24 seconds.
Norm: 23.04, NNZs: 22262, Bias: -1.115852, T: 45260, Avg. loss: 0.027271
Total training time: 0.22 seconds.
-- Epoch 6
-- Epoch 6
Norm: 24.10, NNZs: 24710, Bias: -0.996467, T: 45260, Avg. loss: 0.022748
Total training time: 0.25 seconds.
-- Epoch 6
Norm: 23.05, NNZs: 18988, Bias: -1.095929, T: 54306, Avg. loss: 0.029359
Total training time: 0.25 seconds.
Norm: 23.16, NNZs: 18795, Bias: -1.068347, T: 54306, Avg. loss: 0.029115
Total training time: 0.27 seconds.
-- Epoch 7
-- Epoch 7
Norm: 23.78, NNZs: 32118, Bias: -1.138015, T: 81459, Avg. loss: 0.036256
Total training time: 0.27 seconds.
Norm: 21.41, NNZs: 28928, Bias: -1.121811, T: 63357, Avg. loss: 0.021023
Total training time: 0.27 seconds.
-- Epoch 10
-- Epoch 8
Norm: 22.87, NNZs: 21596, Bias: -1.045820, T: 99561, Avg. loss: 0.026453
Total training time: 0.29 seconds.
Convergence after 11 epochs took 0.29 seconds
-- Epoch 7
Norm: 23.10, NNZs: 29068, Bias: -1.055291, T: 63364, Avg. loss: 0.020093
Total training time: 0.26 seconds.
-- Epoch 8
Norm: 23.24, NNZs: 37082, Bias: -1.013379, T: 99561, Avg. loss: 0.018904
Total training time: 0.28 seconds.
Norm: 24.51, NNZs: 28660, Bias: -1.043911, T: 72408, Avg. loss: 0.032231
Total training time: 0.30 seconds.
Convergence after 11 epochs took 0.29 seconds
Norm: 24.99, NNZs: 37085, Bias: -1.023666, T: 90510, Avg. loss: 0.032706
Total training time: 0.31 seconds.
-- Epoch 11
-- Epoch 1
Norm: 21.59, NNZs: 24037, Bias: -1.132760, T: 54306, Avg. loss: 0.020936
Total training time: 0.25 seconds.
Norm: 23.16, NNZs: 22869, Bias: -1.159276, T: 45255, Avg. loss: 0.030233
Total training time: 0.17 seconds.
Norm: 23.82, NNZs: 32348, Bias: -1.135258, T: 90510, Avg. loss: 0.035770
Total training time: 0.28 seconds.
-- Epoch 7
-- Epoch 6
-- Epoch 11
Norm: 22.80, NNZs: 17311, Bias: -1.061717, T: 81459, Avg. loss: 0.027548
Total training time: 0.30 seconds.
-- Epoch 6
Norm: 21.94, NNZs: 19788, Bias: -1.028493, T: 45255, Avg. loss: 0.017375
Total training time: 0.26 seconds.
-- Epoch 6
Norm: 21.43, NNZs: 29896, Bias: -1.106154, T: 63364, Avg. loss: 0.020103
Total training time: 0.27 seconds.
Norm: 23.77, NNZs: 19330, Bias: -1.234033, T: 36208, Avg. loss: 0.043644
Total training time: 0.17 seconds.
-- Epoch 8
-- Epoch 5
Norm: 24.55, NNZs: 25600, Bias: -1.067916, T: 63357, Avg. loss: 0.033099
Total training time: 0.25 seconds.
Norm: 23.62, NNZs: 42777, Bias: -1.077336, T: 81459, Avg. loss: 0.031121
Total training time: 0.27 seconds.
-- Epoch 8
Norm: 23.15, NNZs: 28873, Bias: -1.070684, T: 72408, Avg. loss: 0.023285
Total training time: 0.29 seconds.
-- Epoch 6
-- Epoch 10
-- Epoch 9
Norm: 30.14, NNZs: 14879, Bias: -1.338876, T: 9051, Avg. loss: 0.067852
Total training time: 0.01 seconds.
Norm: 23.44, NNZs: 26846, Bias: -0.999956, T: 63357, Avg. loss: 0.022218
Total training time: 0.24 seconds.
-- Epoch 2
-- Epoch 8
Norm: 21.71, NNZs: 21510, Bias: -1.007010, T: 54312, Avg. loss: 0.016851
Total training time: 0.19 seconds.
-- Epoch 7
Norm: 23.75, NNZs: 28291, Bias: -1.003930, T: 72408, Avg. loss: 0.020620
Total training time: 0.31 seconds.
Norm: 21.14, NNZs: 24150, Bias: -1.131827, T: 63357, Avg. loss: 0.020368
Total training time: 0.24 seconds.
-- Epoch 9
-- Epoch 8
-- Epoch 8
Norm: 23.91, NNZs: 23401, Bias: -1.028675, T: 63357, Avg. loss: 0.031684
Total training time: 0.26 seconds.
-- Epoch 8
Norm: 23.27, NNZs: 26975, Bias: -1.031859, T: 63357, Avg. loss: 0.021548
Total training time: 0.25 seconds.
Norm: 23.91, NNZs: 26407, Bias: -0.999313, T: 54312, Avg. loss: 0.021652
Total training time: 0.27 seconds.
-- Epoch 8
Norm: 23.97, NNZs: 23337, Bias: -1.101423, T: 99561, Avg. loss: 0.025713
Total training time: 0.29 seconds.
Norm: 23.41, NNZs: 30443, Bias: -1.005987, T: 72408, Avg. loss: 0.019903
Total training time: 0.30 seconds.
-- Epoch 7
-- Epoch 12
-- Epoch 9
Norm: 21.82, NNZs: 22359, Bias: -1.011991, T: 54306, Avg. loss: 0.017494
Total training time: 0.26 seconds.
-- Epoch 1
-- Epoch 7
Norm: 24.11, NNZs: 26121, Bias: -1.009920, T: 72408, Avg. loss: 0.023584
Total training time: 0.32 seconds.
Norm: 23.66, NNZs: 28198, Bias: -1.059447, T: 63357, Avg. loss: 0.020455
-- Epoch 9
-- Epoch 10
Total training time: 0.26 seconds.
-- Epoch 8
Norm: 22.80, NNZs: 19235, Bias: -1.062561, T: 72416, Avg. loss: 0.025963
Total training time: 0.28 seconds.
-- Epoch 9
Norm: 23.83, NNZs: 24002, Bias: -1.010770, T: 72408, Avg. loss: 0.031006
Total training time: 0.27 seconds.
-- Epoch 9
Norm: 21.19, NNZs: 25162, Bias: -1.104589, T: 72408, Avg. loss: 0.019928
Total training time: 0.25 seconds.
Norm: 22.72, NNZs: 22793, Bias: -1.097969, T: 54312, Avg. loss: 0.025815
Total training time: 0.25 seconds.
Norm: 23.01, NNZs: 22889, Bias: -1.086948, T: 63357, Avg. loss: 0.024467
Total training time: 0.32 seconds.
-- Epoch 7
-- Epoch 8
Norm: 23.41, NNZs: 26518, Bias: -1.173982, T: 63357, Avg. loss: 0.036766
Total training time: 0.32 seconds.
Norm: 23.63, NNZs: 26357, Bias: -1.034894, T: 54306, Avg. loss: 0.033517
Total training time: 0.25 seconds.
-- Epoch 8
-- Epoch 7
Norm: 24.09, NNZs: 16772, Bias: -1.003496, T: 45255, Avg. loss: 0.027698
Total training time: 0.28 seconds.
Norm: 21.49, NNZs: 27402, Bias: -1.127356, T: 63357, Avg. loss: 0.020070
Total training time: 0.27 seconds.
Norm: 23.63, NNZs: 21354, Bias: -1.001313, T: 63357, Avg. loss: 0.021816
Total training time: 0.28 seconds.
-- Epoch 6
-- Epoch 8
-- Epoch 8
Norm: 21.76, NNZs: 20405, Bias: -1.018432, T: 54306, Avg. loss: 0.016448
Total training time: 0.28 seconds.
Norm: 24.00, NNZs: 23390, Bias: -1.001227, T: 63364, Avg. loss: 0.031231
Total training time: 0.27 seconds.
-- Epoch 7
Norm: 22.71, NNZs: 17902, Bias: -1.054625, T: 90510, Avg. loss: 0.027025
Total training time: 0.32 seconds.
-- Epoch 8
-- Epoch 11
Norm: 24.99, NNZs: 37392, Bias: -1.021505, T: 99561, Avg. loss: 0.032261
Total training time: 0.34 seconds.
Norm: 23.92, NNZs: 45871, Bias: -1.134502, T: 54306, Avg. loss: 0.032409
Total training time: 0.30 seconds.
-- Epoch 12
Norm: 19.79, NNZs: 17440, Bias: -1.133318, T: 36204, Avg. loss: 0.038120
Total training time: 0.18 seconds.
Norm: 23.00, NNZs: 27833, Bias: -1.081982, T: 63357, Avg. loss: 0.023979
Total training time: 0.31 seconds.
Norm: 23.65, NNZs: 42595, Bias: -1.111605, T: 63357, Avg. loss: 0.031783
Total training time: 0.32 seconds.
-- Epoch 7
-- Epoch 5
-- Epoch 8
-- Epoch 8
Norm: 23.60, NNZs: 43321, Bias: -1.066078, T: 90510, Avg. loss: 0.030475
Total training time: 0.29 seconds.
Norm: 23.13, NNZs: 19149, Bias: -1.052461, T: 63357, Avg. loss: 0.027852
Total training time: 0.31 seconds.
Norm: 22.71, NNZs: 23125, Bias: -1.062328, T: 63364, Avg. loss: 0.025308
Total training time: 0.26 seconds.
-- Epoch 11
-- Epoch 8
-- Epoch 8
Norm: 21.28, NNZs: 30191, Bias: -1.108131, T: 72416, Avg. loss: 0.019507
Total training time: 0.29 seconds.
-- Epoch 9
-- Epoch 9
Norm: 23.18, NNZs: 18448, Bias: -1.092279, T: 45260, Avg. loss: 0.031140
Total training time: 0.29 seconds.
-- Epoch 6
Norm: 21.39, NNZs: 29678, Bias: -1.089715, T: 72408, Avg. loss: 0.020588
Total training time: 0.31 seconds.
Norm: 21.71, NNZs: 20751, Bias: -1.063945, T: 45255, Avg. loss: 0.020130
Total training time: 0.21 seconds.
-- Epoch 9
-- Epoch 6
Norm: 22.86, NNZs: 19875, Bias: -1.058805, T: 63357, Avg. loss: 0.029736
Total training time: 0.32 seconds.
Norm: 23.82, NNZs: 24150, Bias: -0.996191, T: 63364, Avg. loss: 0.025608
Total training time: 0.29 seconds.
Norm: 23.48, NNZs: 25088, Bias: -1.002986, T: 81459, Avg. loss: 0.024957
Total training time: 0.28 seconds.
-- Epoch 8
-- Epoch 8
-- Epoch 10
Norm: 23.78, NNZs: 32604, Bias: -1.125334, T: 99561, Avg. loss: 0.035237
Total training time: 0.31 seconds.
-- Epoch 12
Norm: 23.29, NNZs: 26766, Bias: -1.151364, T: 72408, Avg. loss: 0.035657
Total training time: 0.33 seconds.
Norm: 22.96, NNZs: 19095, Bias: -1.086325, T: 54312, Avg. loss: 0.029967
Total training time: 0.30 seconds.
-- Epoch 9
Norm: 23.74, NNZs: 28712, Bias: -1.001569, T: 81459, Avg. loss: 0.020274
Total training time: 0.34 seconds.
Norm: 24.35, NNZs: 22175, Bias: -1.013903, T: 45255, Avg. loss: 0.033601
Total training time: 0.30 seconds.
Norm: 30.69, NNZs: 12354, Bias: -1.329268, T: 9051, Avg. loss: 0.098957
Total training time: 0.02 seconds.
-- Epoch 10
-- Epoch 2
Norm: 23.76, NNZs: 24878, Bias: -1.192998, T: 45260, Avg. loss: 0.041500
Total training time: 0.20 seconds.
Norm: 23.91, NNZs: 23509, Bias: -1.090309, T: 108612, Avg. loss: 0.025266
Total training time: 0.32 seconds.
-- Epoch 6
-- Epoch 13
Norm: 24.67, NNZs: 23549, Bias: -1.020644, T: 63364, Avg. loss: 0.033937
Total training time: 0.29 seconds.
-- Epoch 8
-- Epoch 6
Norm: 23.35, NNZs: 30660, Bias: -1.004334, T: 81459, Avg. loss: 0.019473
Total training time: 0.33 seconds.
Norm: 23.07, NNZs: 33625, Bias: -1.044056, T: 72416, Avg. loss: 0.019851
Total training time: 0.30 seconds.
-- Epoch 10
-- Epoch 9
Norm: 23.34, NNZs: 35176, Bias: -1.084935, T: 54306, Avg. loss: 0.025901
Total training time: 0.25 seconds.
-- Epoch 7
Norm: 23.70, NNZs: 16812, Bias: -1.182799, T: 54306, Avg. loss: 0.038617
Total training time: 0.32 seconds.
-- Epoch 7
Norm: 22.88, NNZs: 19479, Bias: -1.088106, T: 63357, Avg. loss: 0.028427
Total training time: 0.30 seconds.
-- Epoch 8
Norm: 23.03, NNZs: 35214, Bias: -1.089576, T: 81459, Avg. loss: 0.022639
Total training time: 0.33 seconds.
Norm: 23.60, NNZs: 28267, Bias: -1.023062, T: 63357, Avg. loss: 0.032689
Total training time: 0.27 seconds.
-- Epoch 10
-- Epoch 8
Norm: 23.82, NNZs: 24357, Bias: -1.007980, T: 81459, Avg. loss: 0.030343
Total training time: 0.30 seconds.
-- Epoch 10
Norm: 24.55, NNZs: 26055, Bias: -1.054658, T: 72408, Avg. loss: 0.032207
Total training time: 0.30 seconds.
-- Epoch 9
Norm: 23.71, NNZs: 21873, Bias: -1.017467, T: 54306, Avg. loss: 0.027002
Total training time: 0.30 seconds.
Norm: 21.59, NNZs: 22866, Bias: -1.000183, T: 63357, Avg. loss: 0.016477
Total training time: 0.29 seconds.
-- Epoch 7
-- Epoch 8
Norm: 23.65, NNZs: 30998, Bias: -1.043138, T: 72408, Avg. loss: 0.019782
Total training time: 0.30 seconds.
Norm: 23.09, NNZs: 23760, Bias: -1.149836, T: 54306, Avg. loss: 0.028498
Total training time: 0.23 seconds.
-- Epoch 7
Norm: 23.18, NNZs: 27282, Bias: -1.016469, T: 72408, Avg. loss: 0.020567
Total training time: 0.29 seconds.
Norm: 23.81, NNZs: 42849, Bias: -1.114124, T: 72416, Avg. loss: 0.031236
Total training time: 0.32 seconds.
Norm: 24.61, NNZs: 24075, Bias: -1.065112, T: 63357, Avg. loss: 0.031116
Total training time: 0.33 seconds.
-- Epoch 9
Norm: 23.57, NNZs: 29443, Bias: -1.015053, T: 72408, Avg. loss: 0.031841
Total training time: 0.28 seconds.
-- Epoch 9
-- Epoch 8
Norm: 23.35, NNZs: 27281, Bias: -1.000249, T: 72408, Avg. loss: 0.021589
Total training time: 0.29 seconds.
Norm: 23.06, NNZs: 35720, Bias: -1.069618, T: 90510, Avg. loss: 0.022285
Total training time: 0.34 seconds.
-- Epoch 9
Norm: 23.43, NNZs: 25421, Bias: -1.004037, T: 90510, Avg. loss: 0.024622
Total training time: 0.30 seconds.
Norm: 26.30, NNZs: 18415, Bias: -1.174975, T: 18102, Avg. loss: 0.034256
Total training time: 0.06 seconds.
Norm: 23.84, NNZs: 46303, Bias: -1.125860, T: 63357, Avg. loss: 0.030903
Total training time: 0.33 seconds.
Norm: 22.97, NNZs: 28719, Bias: -1.057076, T: 72408, Avg. loss: 0.023230
Total training time: 0.34 seconds.
Norm: 22.64, NNZs: 15739, Bias: -1.079836, T: 63357, Avg. loss: 0.029205
Total training time: 0.33 seconds.
-- Epoch 11
Norm: 21.78, NNZs: 21262, Bias: -1.009828, T: 54306, Avg. loss: 0.016737
Total training time: 0.29 seconds.
-- Epoch 8
-- Epoch 8
-- Epoch 7
Norm: 23.74, NNZs: 26853, Bias: -1.007672, T: 63364, Avg. loss: 0.020742
Total training time: 0.32 seconds.
Norm: 22.84, NNZs: 19511, Bias: -1.042874, T: 81468, Avg. loss: 0.025682
Total training time: 0.32 seconds.
-- Epoch 8
Norm: 24.46, NNZs: 28975, Bias: -1.033202, T: 81459, Avg. loss: 0.031347
Total training time: 0.36 seconds.
-- Epoch 10
-- Epoch 10
Norm: 23.50, NNZs: 21653, Bias: -1.005751, T: 72408, Avg. loss: 0.021088
Norm: 19.62, NNZs: 18250, Bias: -1.096018, T: 45255, Avg. loss: 0.035841
Total training time: 0.21 seconds.
Norm: 23.96, NNZs: 26519, Bias: -1.012922, T: 81459, Avg. loss: 0.022851
Total training time: 0.36 seconds.
-- Epoch 6
-- Epoch 10
Norm: 24.08, NNZs: 23776, Bias: -1.008663, T: 72416, Avg. loss: 0.030717
Total training time: 0.31 seconds.
Norm: 23.71, NNZs: 35440, Bias: -1.120134, T: 108612, Avg. loss: 0.034757
Total training time: 0.34 seconds.
-- Epoch 9
-- Epoch 13
Norm: 23.41, NNZs: 26405, Bias: -1.000867, T: 99561, Avg. loss: 0.024318
Total training time: 0.31 seconds.
-- Epoch 3
-- Epoch 9
Norm: 21.42, NNZs: 30718, Bias: -1.096223, T: 72408, Avg. loss: 0.019551
Total training time: 0.32 seconds.
-- Epoch 7
-- Epoch 9
Norm: 23.68, NNZs: 28887, Bias: -1.003831, T: 90510, Avg. loss: 0.019714
Total training time: 0.37 seconds.
Norm: 23.58, NNZs: 43847, Bias: -1.065466, T: 99561, Avg. loss: 0.029922
Total training time: 0.33 seconds.
Norm: 21.18, NNZs: 33305, Bias: -1.100058, T: 81459, Avg. loss: 0.020206
Total training time: 0.35 seconds.
-- Epoch 11
-- Epoch 12
-- Epoch 10
Norm: 22.95, NNZs: 33879, Bias: -1.045213, T: 81468, Avg. loss: 0.019232
Total training time: 0.33 seconds.
-- Epoch 10
Norm: 23.68, NNZs: 42948, Bias: -1.096478, T: 72408, Avg. loss: 0.031299
Total training time: 0.36 seconds.
Norm: 23.13, NNZs: 19673, Bias: -1.054381, T: 72408, Avg. loss: 0.027213
Total training time: 0.35 seconds.
Norm: 24.96, NNZs: 40182, Bias: -1.020283, T: 108612, Avg. loss: 0.031624
Total training time: 0.39 seconds.
-- Epoch 9
-- Epoch 13
Norm: 27.20, NNZs: 16071, Bias: -1.175343, T: 18102, Avg. loss: 0.054287
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 21.68, NNZs: 26884, Bias: -0.997363, T: 63364, Avg. loss: 0.016359
Total training time: 0.26 seconds.
-- Epoch 8
Norm: 22.85, NNZs: 19643, Bias: -1.082806, T: 63364, Avg. loss: 0.029102
Total training time: 0.34 seconds.
-- Epoch 8
Norm: 21.49, NNZs: 21779, Bias: -1.032174, T: 54306, Avg. loss: 0.019267
Total training time: 0.26 seconds.
Norm: 23.58, NNZs: 44045, Bias: -1.052630, T: 108612, Avg. loss: 0.029630
Total training time: 0.34 seconds.
-- Epoch 7
Total training time: 0.34 seconds.
-- Epoch 9
Convergence after 12 epochs took 0.34 seconds
Norm: 23.86, NNZs: 17517, Bias: -1.005645, T: 54306, Avg. loss: 0.025976
Total training time: 0.34 seconds.
-- Epoch 11
-- Epoch 9
-- Epoch 9
Norm: 22.81, NNZs: 28019, Bias: -1.082661, T: 72408, Avg. loss: 0.023619
Total training time: 0.38 seconds.
Norm: 22.89, NNZs: 19812, Bias: -1.073937, T: 72408, Avg. loss: 0.027733
Total training time: 0.33 seconds.
Norm: 24.62, NNZs: 24996, Bias: -1.040181, T: 72416, Avg. loss: 0.032662
Total training time: 0.33 seconds.
-- Epoch 9
Norm: 26.31, NNZs: 23077, Bias: -1.140715, T: 27153, Avg. loss: 0.044283
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 7
Norm: 23.62, NNZs: 25598, Bias: -1.199416, T: 54312, Avg. loss: 0.039563
Norm: 22.87, NNZs: 20384, Bias: -1.057869, T: 72408, Avg. loss: 0.029257
Total training time: 0.25 seconds.
Total training time: 0.37 seconds.
Norm: 23.42, NNZs: 31291, Bias: -1.146624, T: 81459, Avg. loss: 0.035075
Total training time: 0.38 seconds.
Norm: 21.17, NNZs: 31289, Bias: -1.087247, T: 81468, Avg. loss: 0.019114
Total training time: 0.35 seconds.
Norm: 23.74, NNZs: 22081, Bias: -1.161415, T: 63357, Avg. loss: 0.037402
Total training time: 0.36 seconds.
Norm: 21.56, NNZs: 20843, Bias: -1.015510, T: 63357, Avg. loss: 0.015528
Total training time: 0.34 seconds.
-- Epoch 7
-- Epoch 10
Norm: 23.29, NNZs: 36175, Bias: -1.061447, T: 63357, Avg. loss: 0.025111
Total training time: 0.29 seconds.
-- Epoch 10
-- Epoch 8
-- Epoch 8
-- Epoch 8
-- Epoch 9
-- Epoch 1
Norm: 22.71, NNZs: 18082, Bias: -1.050856, T: 99561, Avg. loss: 0.026680
Total training time: 0.39 seconds.
-- Epoch 12
Norm: 23.93, NNZs: 23672, Bias: -1.089868, T: 117663, Avg. loss: 0.025042
Total training time: 0.37 seconds.
-- Epoch 9
-- Epoch 14
Norm: 24.49, NNZs: 26639, Bias: -1.039708, T: 81459, Avg. loss: 0.031478
Total training time: 0.33 seconds.
-- Epoch 10
Norm: 23.72, NNZs: 35599, Bias: -1.115301, T: 117663, Avg. loss: 0.034438
Total training time: 0.37 seconds.
Norm: 23.32, NNZs: 35065, Bias: -0.998660, T: 90510, Avg. loss: 0.019171
Total training time: 0.38 seconds.
Norm: 23.61, NNZs: 22597, Bias: -1.006244, T: 63357, Avg. loss: 0.026118
Total training time: 0.34 seconds.
-- Epoch 9
Convergence after 13 epochs took 0.37 seconds
Norm: 23.82, NNZs: 43067, Bias: -1.095947, T: 81468, Avg. loss: 0.030396
Total training time: 0.36 seconds.
-- Epoch 9
-- Epoch 8
Norm: 25.09, NNZs: 20975, Bias: -1.089498, T: 27153, Avg. loss: 0.027997
Total training time: 0.09 seconds.
-- Epoch 10
-- Epoch 4
Norm: 24.08, NNZs: 24888, Bias: -1.026986, T: 54306, Avg. loss: 0.031849
Total training time: 0.36 seconds.
Norm: 23.49, NNZs: 31516, Bias: -1.006834, T: 81459, Avg. loss: 0.031136
Total training time: 0.32 seconds.
-- Epoch 7
Norm: 19.61, NNZs: 19010, Bias: -1.087906, T: 54306, Avg. loss: 0.034561
Total training time: 0.25 seconds.
-- Epoch 7
-- Epoch 10
Norm: 21.49, NNZs: 21669, Bias: -0.999728, T: 63357, Avg. loss: 0.015959
Total training time: 0.34 seconds.
Norm: 23.73, NNZs: 24666, Bias: -1.006438, T: 72416, Avg. loss: 0.025042
Total training time: 0.35 seconds.
-- Epoch 8
-- Epoch 9
Norm: 22.72, NNZs: 19780, Bias: -1.040420, T: 90520, Avg. loss: 0.024851
Total training time: 0.37 seconds.
Norm: 24.74, NNZs: 23064, Bias: -1.039818, T: 36204, Avg. loss: 0.024761
Total training time: 0.10 seconds.
-- Epoch 11
-- Epoch 5
Norm: 30.40, NNZs: 14057, Bias: -1.188061, T: 9051, Avg. loss: 0.071058
Total training time: 0.01 seconds.
Norm: 23.90, NNZs: 24259, Bias: -1.008106, T: 81468, Avg. loss: 0.029711
-- Epoch 2
-- Epoch 9
Norm: 21.49, NNZs: 27253, Bias: -1.011833, T: 72416, Avg. loss: 0.015608
Total training time: 0.29 seconds.
-- Epoch 9
Norm: 23.58, NNZs: 29196, Bias: -1.000689, T: 99561, Avg. loss: 0.019389
Total training time: 0.40 seconds.
Convergence after 11 epochs took 0.41 seconds
Norm: 23.63, NNZs: 27232, Bias: -1.000128, T: 72416, Avg. loss: 0.020284
Total training time: 0.37 seconds.
Norm: 23.47, NNZs: 23413, Bias: -1.005343, T: 81459, Avg. loss: 0.020749
Total training time: 0.37 seconds.
-- Epoch 9
Norm: 22.91, NNZs: 34168, Bias: -1.031236, T: 90520, Avg. loss: 0.018884
Total training time: 0.37 seconds.
Total training time: 0.35 seconds.
-- Epoch 10
Norm: 23.32, NNZs: 29745, Bias: -1.002994, T: 81459, Avg. loss: 0.021420
Total training time: 0.34 seconds.
-- Epoch 10
Norm: 22.66, NNZs: 24044, Bias: -1.062361, T: 72416, Avg. loss: 0.024542
Total training time: 0.34 seconds.
-- Epoch 9
Norm: 25.87, NNZs: 24877, Bias: -1.092241, T: 36204, Avg. loss: 0.039992
Total training time: 0.09 seconds.
Norm: 26.23, NNZs: 18542, Bias: -1.142861, T: 18102, Avg. loss: 0.037458
Total training time: 0.02 seconds.
-- Epoch 5
-- Epoch 3
-- Epoch 10
Norm: 22.99, NNZs: 24194, Bias: -1.128600, T: 63357, Avg. loss: 0.027766
Total training time: 0.29 seconds.
-- Epoch 8
Norm: 23.54, NNZs: 26344, Bias: -1.170826, T: 63364, Avg. loss: 0.038361
Total training time: 0.28 seconds.
-- Epoch 8
Convergence after 11 epochs took 0.36 seconds
Norm: 22.83, NNZs: 20169, Bias: -1.064584, T: 72416, Avg. loss: 0.028376
Total training time: 0.38 seconds.
-- Epoch 11
Norm: 21.45, NNZs: 23566, Bias: -0.997708, T: 72408, Avg. loss: 0.016390
Total training time: 0.36 seconds.
-- Epoch 1
Norm: 22.91, NNZs: 35829, Bias: -1.066111, T: 99561, Avg. loss: 0.021779
Total training time: 0.40 seconds.
-- Epoch 9
Norm: 23.79, NNZs: 24571, Bias: -1.018711, T: 90510, Avg. loss: 0.029968
Norm: 23.42, NNZs: 36065, Bias: -1.002369, T: 90510, Avg. loss: 0.030669
Total training time: 0.35 seconds.
Convergence after 11 epochs took 0.40 seconds
Norm: 23.65, NNZs: 22476, Bias: -1.162863, T: 72408, Avg. loss: 0.036608
Total training time: 0.39 seconds.
-- Epoch 11
-- Epoch 9
Norm: 24.47, NNZs: 26974, Bias: -1.034489, T: 90510, Avg. loss: 0.031108
Total training time: 0.37 seconds.
Norm: 19.58, NNZs: 19592, Bias: -1.077380, T: 63357, Avg. loss: 0.033430
Total training time: 0.27 seconds.
-- Epoch 11
-- Epoch 8
Norm: 21.25, NNZs: 36310, Bias: -1.091149, T: 90510, Avg. loss: 0.019815
Total training time: 0.40 seconds.
Norm: 21.50, NNZs: 21124, Bias: -1.000542, T: 72408, Avg. loss: 0.015186
Total training time: 0.38 seconds.
Norm: 23.17, NNZs: 36619, Bias: -1.055660, T: 72408, Avg. loss: 0.024344
Total training time: 0.33 seconds.
-- Epoch 9
-- Epoch 9
Norm: 23.05, NNZs: 27564, Bias: -1.015806, T: 81459, Avg. loss: 0.020155
Norm: 22.60, NNZs: 17795, Bias: -1.072069, T: 72408, Avg. loss: 0.028651
Total training time: 0.40 seconds.
Norm: 21.18, NNZs: 33918, Bias: -1.080439, T: 90520, Avg. loss: 0.018926
Total training time: 0.39 seconds.
Norm: 23.97, NNZs: 26711, Bias: -1.006493, T: 90510, Avg. loss: 0.022695
Total training time: 0.43 seconds.
-- Epoch 11
Norm: 24.54, NNZs: 25811, Bias: -1.032860, T: 81468, Avg. loss: 0.032060
Total training time: 0.38 seconds.
Norm: 23.71, NNZs: 24983, Bias: -0.999864, T: 81468, Avg. loss: 0.024811
Total training time: 0.38 seconds.
-- Epoch 11
-- Epoch 10
-- Epoch 10
Total training time: 0.38 seconds.
Norm: 21.35, NNZs: 31288, Bias: -1.099971, T: 81459, Avg. loss: 0.019094
Total training time: 0.38 seconds.
Norm: 21.39, NNZs: 27795, Bias: -1.006012, T: 81468, Avg. loss: 0.015468
Total training time: 0.31 seconds.
-- Epoch 10
-- Epoch 10
-- Epoch 9
Norm: 19.57, NNZs: 20222, Bias: -1.066633, T: 72408, Avg. loss: 0.032804
Total training time: 0.28 seconds.
Norm: 23.91, NNZs: 29147, Bias: -1.012078, T: 90520, Avg. loss: 0.029431
Total training time: 0.38 seconds.
-- Epoch 9
-- Epoch 1
-- Epoch 11
Norm: 23.94, NNZs: 23769, Bias: -1.087588, T: 126714, Avg. loss: 0.024897
Total training time: 0.41 seconds.
Norm: 22.87, NNZs: 28995, Bias: -1.055734, T: 81459, Avg. loss: 0.022741
Total training time: 0.42 seconds.
Norm: 23.42, NNZs: 23604, Bias: -1.000661, T: 90510, Avg. loss: 0.020387
Total training time: 0.40 seconds.
Norm: 22.61, NNZs: 29358, Bias: -1.064126, T: 81468, Avg. loss: 0.023788
Total training time: 0.36 seconds.
Convergence after 14 epochs took 0.41 seconds
-- Epoch 10
-- Epoch 11
-- Epoch 10
Norm: 23.55, NNZs: 27654, Bias: -1.003859, T: 81468, Avg. loss: 0.019907
Total training time: 0.40 seconds.
-- Epoch 10
Norm: 23.01, NNZs: 25004, Bias: -1.111767, T: 72408, Avg. loss: 0.026988
Total training time: 0.31 seconds.
Norm: 24.99, NNZs: 20803, Bias: -1.111809, T: 27153, Avg. loss: 0.030597
Total training time: 0.05 seconds.
Norm: 24.93, NNZs: 40925, Bias: -1.015429, T: 117663, Avg. loss: 0.031412
Total training time: 0.45 seconds.
-- Epoch 9
-- Epoch 4
Convergence after 13 epochs took 0.45 seconds
Norm: 23.73, NNZs: 43529, Bias: -1.104491, T: 90520, Avg. loss: 0.029953
Total training time: 0.41 seconds.
Norm: 23.72, NNZs: 18203, Bias: -1.003725, T: 63357, Avg. loss: 0.025206
Total training time: 0.40 seconds.
-- Epoch 11
Norm: 25.52, NNZs: 26123, Bias: -1.084965, T: 45255, Avg. loss: 0.036999
Total training time: 0.13 seconds.
Norm: 23.27, NNZs: 30006, Bias: -1.002929, T: 90510, Avg. loss: 0.020980
Total training time: 0.38 seconds.
-- Epoch 8
-- Epoch 11
-- Epoch 11
Norm: 21.06, NNZs: 34650, Bias: -1.082391, T: 99572, Avg. loss: 0.018440
Total training time: 0.40 seconds.
Norm: 23.42, NNZs: 23813, Bias: -1.005752, T: 99561, Avg. loss: 0.020073
Total training time: 0.40 seconds.
Convergence after 11 epochs took 0.40 seconds
Convergence after 11 epochs took 0.40 seconds
Norm: 23.59, NNZs: 25506, Bias: -1.142400, T: 81459, Avg. loss: 0.035735
Total training time: 0.42 seconds.
-- Epoch 10
Norm: 23.56, NNZs: 31203, Bias: -1.034934, T: 81459, Avg. loss: 0.019059
Total training time: 0.39 seconds.
Norm: 22.82, NNZs: 20752, Bias: -1.048131, T: 81459, Avg. loss: 0.028231
Total training time: 0.43 seconds.
Norm: 21.32, NNZs: 22414, Bias: -1.028498, T: 63357, Avg. loss: 0.018369
Total training time: 0.33 seconds.
-- Epoch 10
-- Epoch 8
Norm: 24.42, NNZs: 27179, Bias: -1.029921, T: 99561, Avg. loss: 0.030667
Total training time: 0.39 seconds.
-- Epoch 12
Norm: 23.80, NNZs: 46955, Bias: -1.101596, T: 72408, Avg. loss: 0.030420
Total training time: 0.42 seconds.
-- Epoch 9
Norm: 19.57, NNZs: 20807, Bias: -1.062125, T: 81459, Avg. loss: 0.032154
Total training time: 0.30 seconds.
Norm: 23.42, NNZs: 36329, Bias: -1.010455, T: 99561, Avg. loss: 0.030223
-- Epoch 10
Total training time: 0.39 seconds.
-- Epoch 10
Norm: 22.65, NNZs: 18156, Bias: -1.063094, T: 81459, Avg. loss: 0.028167
Total training time: 0.43 seconds.
-- Epoch 10
Total training time: 0.38 seconds.
Norm: 22.84, NNZs: 21693, Bias: -1.073041, T: 81459, Avg. loss: 0.027047
Norm: 24.05, NNZs: 25480, Bias: -1.015227, T: 63357, Avg. loss: 0.030749
Total training time: 0.42 seconds.
-- Epoch 8
Norm: 22.80, NNZs: 21073, Bias: -1.038457, T: 90510, Avg. loss: 0.027835
Total training time: 0.44 seconds.
-- Epoch 10
-- Epoch 1
Norm: 22.73, NNZs: 19890, Bias: -1.040995, T: 108612, Avg. loss: 0.026350
-- Epoch 1
Total training time: 0.46 seconds.
Norm: 24.47, NNZs: 27424, Bias: -1.022916, T: 108612, Avg. loss: 0.030271
Total training time: 0.40 seconds.
Norm: 23.29, NNZs: 34594, Bias: -1.002817, T: 99561, Avg. loss: 0.020645
Total training time: 0.39 seconds.
Norm: 24.26, NNZs: 26741, Bias: -1.034514, T: 45255, Avg. loss: 0.022508
Total training time: 0.16 seconds.
Convergence after 12 epochs took 0.40 seconds
-- Epoch 6
-- Epoch 12
Norm: 23.66, NNZs: 43460, Bias: -1.099503, T: 81459, Avg. loss: 0.030573
Total training time: 0.45 seconds.
-- Epoch 11
-- Epoch 10
Norm: 24.05, NNZs: 25829, Bias: -1.011433, T: 72408, Avg. loss: 0.029975
Total training time: 0.42 seconds.
Norm: 23.62, NNZs: 25776, Bias: -1.138348, T: 90510, Avg. loss: 0.035439
Total training time: 0.43 seconds.
-- Epoch 6
-- Epoch 9
Norm: 21.47, NNZs: 22174, Bias: -1.004376, T: 72408, Avg. loss: 0.015606
Total training time: 0.40 seconds.
-- Epoch 11
-- Epoch 9
Norm: 23.58, NNZs: 31358, Bias: -1.028512, T: 90510, Avg. loss: 0.018786
Total training time: 0.40 seconds.
-- Epoch 11
-- Epoch 9
Norm: 23.58, NNZs: 27596, Bias: -1.003102, T: 72408, Avg. loss: 0.025503
Total training time: 0.41 seconds.
-- Epoch 9
Norm: 24.54, NNZs: 23605, Bias: -1.065470, T: 36204, Avg. loss: 0.028144
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 22.93, NNZs: 34396, Bias: -1.015590, T: 99572, Avg. loss: 0.018555
Total training time: 0.42 seconds.
Norm: 22.99, NNZs: 28601, Bias: -1.007255, T: 90510, Avg. loss: 0.020050
Total training time: 0.40 seconds.
Norm: 21.44, NNZs: 21479, Bias: -1.005731, T: 81459, Avg. loss: 0.014753
Total training time: 0.42 seconds.
Convergence after 11 epochs took 0.42 seconds
Convergence after 10 epochs took 0.40 seconds
Norm: 29.41, NNZs: 15272, Bias: -1.366941, T: 9051, Avg. loss: 0.084909
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 22.89, NNZs: 25352, Bias: -1.117768, T: 81459, Avg. loss: 0.026166
Total training time: 0.34 seconds.
-- Epoch 10
Convergence after 9 epochs took 0.42 seconds
Norm: 22.80, NNZs: 29380, Bias: -1.079681, T: 81459, Avg. loss: 0.023117
-- Epoch 11
Total training time: 0.46 seconds.
-- Epoch 1
Convergence after 12 epochs took 0.47 seconds
-- Epoch 1
Norm: 21.45, NNZs: 25342, Bias: -1.010396, T: 81459, Avg. loss: 0.015948
Total training time: 0.41 seconds.
Norm: 23.95, NNZs: 27215, Bias: -1.006163, T: 99561, Avg. loss: 0.022284
Total training time: 0.47 seconds.
-- Epoch 10
Norm: 23.79, NNZs: 47346, Bias: -1.105507, T: 81459, Avg. loss: 0.029744
Total training time: 0.44 seconds.
Convergence after 11 epochs took 0.47 seconds
Norm: 24.60, NNZs: 25712, Bias: -1.048384, T: 72408, Avg. loss: 0.030462
Total training time: 0.42 seconds.
-- Epoch 10
Convergence after 11 epochs took 0.39 seconds
-- Epoch 10
-- Epoch 1
-- Epoch 1
Norm: 23.66, NNZs: 19272, Bias: -1.005031, T: 72408, Avg. loss: 0.024569
Total training time: 0.43 seconds.
-- Epoch 9
Norm: 23.50, NNZs: 28064, Bias: -1.004221, T: 81459, Avg. loss: 0.024833
Total training time: 0.43 seconds.
Norm: 22.55, NNZs: 18390, Bias: -1.055451, T: 90510, Avg. loss: 0.027638
Total training time: 0.45 seconds.
-- Epoch 10
Norm: 23.70, NNZs: 43617, Bias: -1.085057, T: 90510, Avg. loss: 0.030005
Total training time: 0.46 seconds.
-- Epoch 11
-- Epoch 11
Norm: 22.75, NNZs: 19981, Bias: -1.031008, T: 99572, Avg. loss: 0.024701
Total training time: 0.44 seconds.
-- Epoch 10
-- Epoch 12
Norm: 21.11, NNZs: 25683, Bias: -1.101727, T: 81459, Avg. loss: 0.019315
Total training time: 0.41 seconds.
Total training time: 0.45 seconds.
-- Epoch 10
-- Epoch 9
Norm: 21.34, NNZs: 31561, Bias: -1.097104, T: 90510, Avg. loss: 0.018783
Total training time: 0.43 seconds.
-- Epoch 1
Norm: 21.29, NNZs: 23203, Bias: -1.013589, T: 72408, Avg. loss: 0.018005
Total training time: 0.36 seconds.
-- Epoch 11
Norm: 23.45, NNZs: 30731, Bias: -1.155032, T: 72416, Avg. loss: 0.037425
-- Epoch 9
Norm: 23.64, NNZs: 25868, Bias: -1.007747, T: 90520, Avg. loss: 0.024272
Total training time: 0.43 seconds.
Norm: 29.20, NNZs: 16929, Bias: -1.354698, T: 9051, Avg. loss: 0.074458
Total training time: 0.01 seconds.
Norm: 23.61, NNZs: 19935, Bias: -1.004734, T: 81459, Avg. loss: 0.023975
Total training time: 0.44 seconds.
-- Epoch 11
-- Epoch 2
-- Epoch 10
Norm: 22.83, NNZs: 21316, Bias: -1.035817, T: 99561, Avg. loss: 0.027463
Total training time: 0.46 seconds.
-- Epoch 12
Norm: 23.43, NNZs: 31549, Bias: -1.020066, T: 99561, Avg. loss: 0.018352
Total training time: 0.42 seconds.
Total training time: 0.34 seconds.
Convergence after 11 epochs took 0.42 seconds
-- Epoch 9
Norm: 30.48, NNZs: 14310, Bias: -1.153288, T: 9051, Avg. loss: 0.071520
Norm: 22.74, NNZs: 20532, Bias: -1.060094, T: 81468, Avg. loss: 0.027611
Total training time: 0.44 seconds.
Norm: 23.18, NNZs: 37928, Bias: -1.062961, T: 81459, Avg. loss: 0.023698
Total training time: 0.39 seconds.
Norm: 30.09, NNZs: 15617, Bias: -1.278030, T: 9051, Avg. loss: 0.065874
Total training time: 0.03 seconds.
Norm: 30.46, NNZs: 11811, Bias: -1.374346, T: 9052, Avg. loss: 0.093893
Total training time: 0.01 seconds.
Norm: 23.32, NNZs: 31483, Bias: -1.134605, T: 90510, Avg. loss: 0.034442
Total training time: 0.47 seconds.
-- Epoch 10
-- Epoch 2
-- Epoch 11
Norm: 19.57, NNZs: 21012, Bias: -1.057022, T: 90510, Avg. loss: 0.031587
Total training time: 0.33 seconds.
Norm: 22.85, NNZs: 29248, Bias: -1.051830, T: 90510, Avg. loss: 0.022301
Total training time: 0.47 seconds.
-- Epoch 11
Norm: 21.04, NNZs: 25984, Bias: -1.096372, T: 90510, Avg. loss: 0.018843
Total training time: 0.41 seconds.
-- Epoch 11
-- Epoch 11
Norm: 25.50, NNZs: 27204, Bias: -1.044307, T: 54306, Avg. loss: 0.035655
Norm: 23.07, NNZs: 20084, Bias: -1.039033, T: 81459, Avg. loss: 0.026552
Total training time: 0.17 seconds.
Total training time: 0.46 seconds.
-- Epoch 10
-- Epoch 7
-- Epoch 10
Norm: 23.85, NNZs: 29479, Bias: -1.006296, T: 99572, Avg. loss: 0.029035
Total training time: 0.43 seconds.
Norm: 22.88, NNZs: 25669, Bias: -1.108738, T: 90510, Avg. loss: 0.026022
Total training time: 0.36 seconds.
-- Epoch 12
-- Epoch 11
Norm: 23.30, NNZs: 35272, Bias: -1.003941, T: 99561, Avg. loss: 0.018959
Total training time: 0.48 seconds.
Norm: 25.83, NNZs: 18621, Bias: -1.292459, T: 18102, Avg. loss: 0.043623
Total training time: 0.03 seconds.
Convergence after 11 epochs took 0.48 seconds
-- Epoch 3
Norm: 24.19, NNZs: 24792, Bias: -1.028119, T: 45255, Avg. loss: 0.025776
Total training time: 0.10 seconds.
Total training time: 0.05 seconds.
-- Epoch 6
Norm: 23.10, NNZs: 38150, Bias: -1.041970, T: 90510, Avg. loss: 0.023365
Total training time: 0.39 seconds.
-- Epoch 11
Norm: 19.54, NNZs: 21301, Bias: -1.046468, T: 99561, Avg. loss: 0.031288
Total training time: 0.34 seconds.
-- Epoch 12
Norm: 22.59, NNZs: 29619, Bias: -1.056365, T: 90520, Avg. loss: 0.023502
Norm: 24.48, NNZs: 26846, Bias: -1.020027, T: 90520, Avg. loss: 0.031588
Total training time: 0.44 seconds.
-- Epoch 1
-- Epoch 11
Norm: 22.69, NNZs: 20174, Bias: -1.030011, T: 108624, Avg. loss: 0.024285
Total training time: 0.46 seconds.
Norm: 23.06, NNZs: 20292, Bias: -1.019422, T: 90510, Avg. loss: 0.026123
Total training time: 0.47 seconds.
Convergence after 12 epochs took 0.46 seconds
Norm: 21.22, NNZs: 23486, Bias: -1.020400, T: 81459, Avg. loss: 0.017414
Norm: 22.84, NNZs: 21917, Bias: -1.057191, T: 90510, Avg. loss: 0.026615
Total training time: 0.37 seconds.
Total training time: 0.45 seconds.
-- Epoch 11
-- Epoch 11
-- Epoch 10
Norm: 25.38, NNZs: 27717, Bias: -1.042480, T: 63357, Avg. loss: 0.034259
Total training time: 0.18 seconds.
-- Epoch 8
Norm: 21.32, NNZs: 25594, Bias: -1.008565, T: 90510, Avg. loss: 0.015715
Total training time: 0.44 seconds.
-- Epoch 11
Norm: 23.34, NNZs: 31828, Bias: -1.136050, T: 99561, Avg. loss: 0.034055
Total training time: 0.49 seconds.
Convergence after 10 epochs took 0.47 seconds
Total training time: 0.42 seconds.
Norm: 21.40, NNZs: 22444, Bias: -1.002964, T: 81459, Avg. loss: 0.015118
Total training time: 0.44 seconds.
Norm: 22.89, NNZs: 25903, Bias: -1.100126, T: 99561, Avg. loss: 0.025374
Total training time: 0.37 seconds.
Convergence after 9 epochs took 0.44 seconds
Convergence after 11 epochs took 0.37 seconds
Norm: 24.50, NNZs: 26061, Bias: -1.040252, T: 81459, Avg. loss: 0.029945
Total training time: 0.48 seconds.
-- Epoch 1
Norm: 24.40, NNZs: 30110, Bias: -1.029021, T: 90510, Avg. loss: 0.030883
Norm: 25.85, NNZs: 24060, Bias: -1.255206, T: 18102, Avg. loss: 0.037119
Total training time: 0.03 seconds.
Norm: 24.48, NNZs: 31577, Bias: -1.012762, T: 99572, Avg. loss: 0.031252
Norm: 22.80, NNZs: 22144, Bias: -1.063759, T: 99561, Avg. loss: 0.026420
Total training time: 0.45 seconds.
Total training time: 0.45 seconds.
-- Epoch 3
Norm: 21.18, NNZs: 23854, Bias: -1.000574, T: 90510, Avg. loss: 0.017163
Total training time: 0.38 seconds.
Convergence after 11 epochs took 0.45 seconds
Convergence after 10 epochs took 0.38 seconds
-- Epoch 12
Norm: 30.56, NNZs: 13896, Bias: -1.291221, T: 9052, Avg. loss: 0.066935
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 22.78, NNZs: 29662, Bias: -1.072407, T: 90510, Avg. loss: 0.022721
Norm: 23.47, NNZs: 27861, Bias: -1.003682, T: 90520, Avg. loss: 0.019572
Total training time: 0.46 seconds.
Total training time: 0.50 seconds.
-- Epoch 1
Total training time: 0.50 seconds.
-- Epoch 11
-- Epoch 11
-- Epoch 11
-- Epoch 2
-- Epoch 1
Norm: 21.42, NNZs: 28070, Bias: -1.000194, T: 90520, Avg. loss: 0.015395
Norm: 23.46, NNZs: 32371, Bias: -1.154329, T: 81468, Avg. loss: 0.036965
Total training time: 0.37 seconds.
-- Epoch 11
-- Epoch 10
Total training time: 0.39 seconds.
Convergence after 10 epochs took 0.39 seconds
Norm: 23.53, NNZs: 25517, Bias: -1.004452, T: 90510, Avg. loss: 0.023638
Norm: 21.22, NNZs: 31814, Bias: -1.087164, T: 99561, Avg. loss: 0.018402
Total training time: 0.46 seconds.
Convergence after 11 epochs took 0.46 seconds
-- Epoch 1
-- Epoch 1
Norm: 22.73, NNZs: 23271, Bias: -1.042576, T: 90520, Avg. loss: 0.027347
Total training time: 0.47 seconds.
Total training time: 0.47 seconds.
Norm: 23.04, NNZs: 40419, Bias: -1.049894, T: 99561, Avg. loss: 0.022902
Total training time: 0.41 seconds.
-- Epoch 11
Norm: 23.76, NNZs: 24790, Bias: -1.017556, T: 99561, Avg. loss: 0.029408
Total training time: 0.46 seconds.
Convergence after 11 epochs took 0.42 seconds
Norm: 23.75, NNZs: 47675, Bias: -1.089841, T: 90510, Avg. loss: 0.029021
Total training time: 0.48 seconds.
Norm: 22.47, NNZs: 29880, Bias: -1.048558, T: 99572, Avg. loss: 0.023092
Total training time: 0.44 seconds.
Convergence after 11 epochs took 0.44 seconds
-- Epoch 11
Norm: 23.67, NNZs: 25945, Bias: -1.133496, T: 99561, Avg. loss: 0.034827
Total training time: 0.48 seconds.
Norm: 24.10, NNZs: 27451, Bias: -1.008260, T: 54306, Avg. loss: 0.021923
Total training time: 0.21 seconds.
-- Epoch 7
Norm: 23.48, NNZs: 29373, Bias: -1.005004, T: 90510, Avg. loss: 0.024333
Norm: 23.56, NNZs: 26144, Bias: -1.001201, T: 99572, Avg. loss: 0.023927
Total training time: 0.47 seconds.
Norm: 19.55, NNZs: 22268, Bias: -1.038995, T: 108612, Avg. loss: 0.030959
Total training time: 0.36 seconds.
-- Epoch 12
Convergence after 12 epochs took 0.36 seconds
-- Epoch 1
Total training time: 0.47 seconds.
-- Epoch 11
-- Epoch 11
-- Epoch 1
Norm: 23.43, NNZs: 28069, Bias: -0.998463, T: 99572, Avg. loss: 0.019369
Total training time: 0.48 seconds.
-- Epoch 1
Norm: 25.31, NNZs: 29900, Bias: -1.020695, T: 72408, Avg. loss: 0.033206
Total training time: 0.20 seconds.
Convergence after 11 epochs took 0.48 seconds
Norm: 24.84, NNZs: 19934, Bias: -1.208673, T: 27153, Avg. loss: 0.036282
Total training time: 0.06 seconds.
-- Epoch 9
Norm: 24.09, NNZs: 25685, Bias: -1.009739, T: 54306, Avg. loss: 0.024676
Total training time: 0.13 seconds.
-- Epoch 4
-- Epoch 7
Norm: 29.16, NNZs: 14966, Bias: -1.395734, T: 9051, Avg. loss: 0.085257
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 12
Norm: 22.85, NNZs: 29851, Bias: -1.044171, T: 99561, Avg. loss: 0.021802
Total training time: 0.50 seconds.
Norm: 21.03, NNZs: 26123, Bias: -1.089280, T: 99561, Avg. loss: 0.018726
Total training time: 0.45 seconds.
Convergence after 11 epochs took 0.50 seconds
Norm: 21.39, NNZs: 25896, Bias: -0.999098, T: 99561, Avg. loss: 0.015627
Total training time: 0.46 seconds.
Norm: 29.56, NNZs: 15772, Bias: -1.332656, T: 9051, Avg. loss: 0.074183
Norm: 23.07, NNZs: 20572, Bias: -1.025301, T: 99561, Avg. loss: 0.025617
Total training time: 0.50 seconds.
Norm: 23.18, NNZs: 34860, Bias: -1.003638, T: 108612, Avg. loss: 0.020323
Norm: 22.56, NNZs: 18647, Bias: -1.048527, T: 99561, Avg. loss: 0.027300
-- Epoch 10
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 24.36, NNZs: 31368, Bias: -1.024302, T: 99561, Avg. loss: 0.030481
Total training time: 0.52 seconds.
Norm: 22.76, NNZs: 21467, Bias: -1.026103, T: 108612, Avg. loss: 0.026971
Total training time: 0.50 seconds.
-- Epoch 2
-- Epoch 12
-- Epoch 12
Total training time: 0.50 seconds.
-- Epoch 12
Norm: 30.85, NNZs: 13170, Bias: -1.267413, T: 9051, Avg. loss: 0.096900
Total training time: 0.06 seconds.
Norm: 30.82, NNZs: 13019, Bias: -1.302065, T: 9051, Avg. loss: 0.096890
Total training time: 0.10 seconds.
Norm: 25.55, NNZs: 18083, Bias: -1.304113, T: 18102, Avg. loss: 0.045972
-- Epoch 1
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 23.76, NNZs: 24980, Bias: -1.008315, T: 108612, Avg. loss: 0.029061
Total training time: 0.47 seconds.
-- Epoch 3
Convergence after 12 epochs took 0.48 seconds
Convergence after 11 epochs took 0.46 seconds
-- Epoch 12
-- Epoch 12
-- Epoch 1
Norm: 24.02, NNZs: 27908, Bias: -1.008504, T: 63357, Avg. loss: 0.020938
Total training time: 0.23 seconds.
Norm: 24.49, NNZs: 21396, Bias: -1.184154, T: 36204, Avg. loss: 0.032600
Total training time: 0.07 seconds.
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.7s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.7s finished
-- Epoch 8
Norm: 23.65, NNZs: 43739, Bias: -1.098394, T: 99572, Avg. loss: 0.029579
Total training time: 0.50 seconds.
-- Epoch 1
Total training time: 0.46 seconds.
Norm: 25.82, NNZs: 18430, Bias: -1.143877, T: 18102, Avg. loss: 0.032296
Total training time: 0.08 seconds.
Norm: 23.75, NNZs: 47920, Bias: -1.080258, T: 99561, Avg. loss: 0.028755
Total training time: 0.50 seconds.
Convergence after 12 epochs took 0.46 seconds
Norm: 27.34, NNZs: 13405, Bias: -1.223559, T: 9051, Avg. loss: 0.054614
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 12
Norm: 23.89, NNZs: 26129, Bias: -1.010587, T: 81459, Avg. loss: 0.029277
Norm: 24.42, NNZs: 30048, Bias: -1.164471, T: 27153, Avg. loss: 0.031004
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 24.58, NNZs: 19630, Bias: -1.221289, T: 27153, Avg. loss: 0.036968
Norm: 27.33, NNZs: 13531, Bias: -1.233642, T: 9051, Avg. loss: 0.056769
Total training time: 0.03 seconds.
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 24.45, NNZs: 31799, Bias: -1.016252, T: 108612, Avg. loss: 0.030228
Total training time: 0.53 seconds.
-- Epoch 4
Convergence after 12 epochs took 0.53 seconds
-- Epoch 1
Norm: 25.47, NNZs: 24090, Bias: -1.244209, T: 18102, Avg. loss: 0.038995
Total training time: 0.04 seconds.
Norm: 23.65, NNZs: 43865, Bias: -1.082624, T: 99561, Avg. loss: 0.029567
Norm: 23.60, NNZs: 26051, Bias: -1.122767, T: 108612, Avg. loss: 0.034381
Total training time: 0.52 seconds.
Total training time: 0.50 seconds.
Total training time: 0.51 seconds.
-- Epoch 13
-- Epoch 12
-- Epoch 10
Norm: 29.43, NNZs: 15562, Bias: -1.390921, T: 9051, Avg. loss: 0.085831
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 30.27, NNZs: 14284, Bias: -1.205449, T: 9051, Avg. loss: 0.075296
Total training time: 0.02 seconds.
-- Epoch 2
Convergence after 11 epochs took 0.50 seconds
Norm: 23.48, NNZs: 25789, Bias: -1.003224, T: 99561, Avg. loss: 0.023244
-- Epoch 5
Norm: 22.55, NNZs: 18826, Bias: -1.042391, T: 108612, Avg. loss: 0.027106
Total training time: 0.51 seconds.
Norm: 22.71, NNZs: 30014, Bias: -1.068588, T: 99561, Avg. loss: 0.022319
Total training time: 0.54 seconds.
Convergence after 12 epochs took 0.51 seconds
-- Epoch 1
Norm: 23.55, NNZs: 30602, Bias: -1.003901, T: 108624, Avg. loss: 0.023798
Norm: 23.80, NNZs: 29659, Bias: -1.006036, T: 108624, Avg. loss: 0.028720
-- Epoch 2
Norm: 23.03, NNZs: 20804, Bias: -1.016313, T: 108612, Avg. loss: 0.025322
Total training time: 0.52 seconds.
-- Epoch 2
Convergence after 12 epochs took 0.52 seconds
Norm: 23.97, NNZs: 31367, Bias: -1.133357, T: 36204, Avg. loss: 0.028103
Total training time: 0.07 seconds.
-- Epoch 5
Total training time: 0.48 seconds.
Norm: 26.25, NNZs: 17462, Bias: -1.113622, T: 18102, Avg. loss: 0.037601
Total training time: 0.11 seconds.
Norm: 27.45, NNZs: 17676, Bias: -1.201465, T: 18102, Avg. loss: 0.053458
Total training time: 0.08 seconds.
Norm: 29.97, NNZs: 14293, Bias: -1.278831, T: 9052, Avg. loss: 0.073498
Total training time: 0.03 seconds.
Total training time: 0.50 seconds.
-- Epoch 3
-- Epoch 3
Convergence after 11 epochs took 0.50 seconds
-- Epoch 2
Norm: 24.48, NNZs: 32011, Bias: -1.009816, T: 108624, Avg. loss: 0.030808
Total training time: 0.49 seconds.
Norm: 27.18, NNZs: 20802, Bias: -1.205238, T: 18104, Avg. loss: 0.052706
Total training time: 0.07 seconds.
-- Epoch 1
Norm: 25.21, NNZs: 32846, Bias: -1.005282, T: 81459, Avg. loss: 0.032645
Norm: 23.63, NNZs: 43965, Bias: -1.076335, T: 108612, Avg. loss: 0.029243
Total training time: 0.53 seconds.
-- Epoch 3
Total training time: 0.23 seconds.
-- Epoch 1
Convergence after 11 epochs took 0.54 seconds
Norm: 23.48, NNZs: 32704, Bias: -1.138092, T: 90520, Avg. loss: 0.036319
-- Epoch 12
Norm: 29.95, NNZs: 15289, Bias: -1.371896, T: 9052, Avg. loss: 0.084732
Norm: 23.80, NNZs: 26184, Bias: -1.001887, T: 63357, Avg. loss: 0.023492
Total training time: 0.16 seconds.
Norm: 23.67, NNZs: 48215, Bias: -1.074536, T: 108612, Avg. loss: 0.028254
Total training time: 0.52 seconds.
Total training time: 0.50 seconds.
Convergence after 12 epochs took 0.50 seconds
Convergence after 12 epochs took 0.52 seconds
Norm: 30.17, NNZs: 13923, Bias: -1.259657, T: 9051, Avg. loss: 0.068957
-- Epoch 3
Norm: 23.89, NNZs: 26413, Bias: -1.001161, T: 90510, Avg. loss: 0.029112
Total training time: 0.51 seconds.
Norm: 22.57, NNZs: 11113, Bias: -1.267585, T: 9051, Avg. loss: 0.077387
Norm: 29.24, NNZs: 12566, Bias: -1.372544, T: 9052, Avg. loss: 0.074932
Total training time: 0.03 seconds.
Total training time: 0.02 seconds.
Total training time: 0.41 seconds.
Norm: 27.78, NNZs: 14258, Bias: -1.441704, T: 9051, Avg. loss: 0.077190
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 27.29, NNZs: 15455, Bias: -1.355742, T: 9051, Avg. loss: 0.081454
-- Epoch 2
Total training time: 0.01 seconds.
-- Epoch 11
-- Epoch 2
-- Epoch 2
Norm: 27.41, NNZs: 16535, Bias: -1.211022, T: 18102, Avg. loss: 0.052957
Total training time: 0.13 seconds.
-- Epoch 3
Norm: 25.03, NNZs: 20441, Bias: -1.085990, T: 27153, Avg. loss: 0.030931
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 13
-- Epoch 1
Norm: 27.18, NNZs: 13297, Bias: -1.279752, T: 9052, Avg. loss: 0.057528
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 26.46, NNZs: 17305, Bias: -1.121085, T: 18102, Avg. loss: 0.040140
Total training time: 0.04 seconds.
Norm: 23.58, NNZs: 16236, Bias: -1.152100, T: 18102, Avg. loss: 0.029109
Total training time: 0.05 seconds.
Norm: 26.41, NNZs: 21256, Bias: -1.150422, T: 27153, Avg. loss: 0.044216
Total training time: 0.09 seconds.
-- Epoch 8
-- Epoch 3
-- Epoch 3
-- Epoch 4
Norm: 24.42, NNZs: 26226, Bias: -1.040651, T: 90510, Avg. loss: 0.029172
Total training time: 0.53 seconds.
-- Epoch 11
Norm: 29.52, NNZs: 16270, Bias: -1.380911, T: 9051, Avg. loss: 0.078276
Total training time: 0.02 seconds.
-- Epoch 11
Norm: 26.32, NNZs: 20779, Bias: -1.137216, T: 27153, Avg. loss: 0.044049
Total training time: 0.13 seconds.
-- Epoch 2
Norm: 24.47, NNZs: 22252, Bias: -1.057382, T: 36204, Avg. loss: 0.027573
Total training time: 0.12 seconds.
-- Epoch 4
-- Epoch 5
Norm: 24.39, NNZs: 20620, Bias: -1.203450, T: 36204, Avg. loss: 0.033307
Total training time: 0.05 seconds.
Norm: 20.65, NNZs: 14181, Bias: -1.240932, T: 18102, Avg. loss: 0.049591
Total training time: 0.03 seconds.
Norm: 23.31, NNZs: 32124, Bias: -1.120459, T: 108612, Avg. loss: 0.033588
Total training time: 0.55 seconds.
-- Epoch 5
Norm: 22.79, NNZs: 21589, Bias: -1.024239, T: 117663, Avg. loss: 0.026772
Total training time: 0.54 seconds.
-- Epoch 3
-- Epoch 14
Norm: 22.47, NNZs: 11523, Bias: -1.251666, T: 9051, Avg. loss: 0.079094
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 23.73, NNZs: 26646, Bias: -1.000860, T: 72408, Avg. loss: 0.023144
Total training time: 0.17 seconds.
-- Epoch 9
Norm: 23.79, NNZs: 26703, Bias: -1.005422, T: 99561, Avg. loss: 0.028665
Total training time: 0.53 seconds.
Norm: 25.54, NNZs: 25672, Bias: -1.243740, T: 18102, Avg. loss: 0.040553
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 12
Norm: 24.18, NNZs: 24798, Bias: -1.024983, T: 45255, Avg. loss: 0.025991
Total training time: 0.13 seconds.
Norm: 23.38, NNZs: 29671, Bias: -1.000742, T: 99561, Avg. loss: 0.024161
Total training time: 0.52 seconds.
-- Epoch 13
-- Epoch 6
Convergence after 11 epochs took 0.52 seconds
Norm: 24.27, NNZs: 26917, Bias: -1.148163, T: 27153, Avg. loss: 0.032001
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 23.61, NNZs: 16286, Bias: -1.189033, T: 18102, Avg. loss: 0.029767
Total training time: 0.05 seconds.
Norm: 24.07, NNZs: 22025, Bias: -1.173398, T: 45255, Avg. loss: 0.029730
Total training time: 0.08 seconds.
Norm: 24.00, NNZs: 21294, Bias: -1.186385, T: 45255, Avg. loss: 0.030605
Total training time: 0.06 seconds.
-- Epoch 2
Total training time: 0.11 seconds.
-- Epoch 6
Norm: 24.71, NNZs: 17880, Bias: -1.328363, T: 18102, Avg. loss: 0.040334
Total training time: 0.06 seconds.
Total training time: 0.06 seconds.
-- Epoch 10
-- Epoch 6
-- Epoch 2
Norm: 24.66, NNZs: 19747, Bias: -1.280148, T: 18102, Avg. loss: 0.044260
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 22.23, NNZs: 18058, Bias: -1.099736, T: 27153, Avg. loss: 0.024794
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 24.78, NNZs: 20497, Bias: -1.110417, T: 27153, Avg. loss: 0.026508
Total training time: 0.11 seconds.
-- Epoch 3
-- Epoch 4
Norm: 23.26, NNZs: 34893, Bias: -1.126906, T: 117663, Avg. loss: 0.033148
Total training time: 0.56 seconds.
Norm: 23.63, NNZs: 28291, Bias: -1.116753, T: 36204, Avg. loss: 0.029764
Total training time: 0.08 seconds.
Convergence after 13 epochs took 0.56 seconds
-- Epoch 5
Norm: 27.47, NNZs: 12988, Bias: -1.233138, T: 9051, Avg. loss: 0.054742
Norm: 22.69, NNZs: 23526, Bias: -1.042601, T: 99572, Avg. loss: 0.026912
Convergence after 12 epochs took 0.54 seconds
Total training time: 0.54 seconds.
Norm: 26.27, NNZs: 17386, Bias: -1.168102, T: 18104, Avg. loss: 0.032128
Total training time: 0.10 seconds.
Norm: 24.57, NNZs: 27643, Bias: -1.161699, T: 27153, Avg. loss: 0.033225
Total training time: 0.04 seconds.
-- Epoch 13
Norm: 23.70, NNZs: 32487, Bias: -1.107584, T: 45255, Avg. loss: 0.025806
Total training time: 0.10 seconds.
-- Epoch 6
Convergence after 11 epochs took 0.54 seconds
Norm: 24.16, NNZs: 22569, Bias: -1.125858, T: 54306, Avg. loss: 0.028679
Total training time: 0.12 seconds.
-- Epoch 7
Norm: 22.57, NNZs: 12292, Bias: -1.291717, T: 9051, Avg. loss: 0.077703
Norm: 24.00, NNZs: 28361, Bias: -1.002594, T: 72408, Avg. loss: 0.020004
Total training time: 0.27 seconds.
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 9
Norm: 23.93, NNZs: 19802, Bias: -1.256100, T: 27153, Avg. loss: 0.032957
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 23.93, NNZs: 21644, Bias: -1.205055, T: 27153, Avg. loss: 0.035884
-- Epoch 1
Total training time: 0.04 seconds.
Norm: 25.50, NNZs: 17751, Bias: -1.313742, T: 18104, Avg. loss: 0.044450
Total training time: 0.07 seconds.
Norm: 23.99, NNZs: 22997, Bias: -1.126190, T: 63357, Avg. loss: 0.027548
Total training time: 0.12 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 8
Norm: 20.70, NNZs: 15188, Bias: -1.243030, T: 18102, Avg. loss: 0.048239
Total training time: 0.04 seconds.
Norm: 23.69, NNZs: 33094, Bias: -1.094162, T: 54306, Avg. loss: 0.024686
Total training time: 0.11 seconds.
-- Epoch 3
-- Epoch 7
-- Epoch 3
Norm: 20.09, NNZs: 16038, Bias: -1.154270, T: 27153, Avg. loss: 0.041270
Norm: 23.93, NNZs: 29015, Bias: -1.003865, T: 81459, Avg. loss: 0.019645
Total training time: 0.05 seconds.
Total training time: 0.28 seconds.
Norm: 25.78, NNZs: 22211, Bias: -1.128856, T: 36204, Avg. loss: 0.039245
Total training time: 0.16 seconds.
Norm: 24.39, NNZs: 22005, Bias: -1.055081, T: 36204, Avg. loss: 0.023760
Total training time: 0.13 seconds.
Norm: 23.73, NNZs: 26973, Bias: -1.004895, T: 81459, Avg. loss: 0.022521
Total training time: 0.19 seconds.
-- Epoch 10
-- Epoch 4
-- Epoch 5
-- Epoch 5
-- Epoch 10
Norm: 23.50, NNZs: 21135, Bias: -1.187212, T: 36204, Avg. loss: 0.029859
Total training time: 0.08 seconds.
Norm: 25.22, NNZs: 33740, Bias: -1.003634, T: 90510, Avg. loss: 0.032374
Total training time: 0.27 seconds.
-- Epoch 1
-- Epoch 5
-- Epoch 11
Norm: 23.83, NNZs: 21960, Bias: -1.155619, T: 54306, Avg. loss: 0.029538
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 23.96, NNZs: 23294, Bias: -1.122502, T: 72408, Avg. loss: 0.026781
Total training time: 0.13 seconds.
Norm: 23.39, NNZs: 22735, Bias: -1.142975, T: 36204, Avg. loss: 0.032367
Total training time: 0.05 seconds.
-- Epoch 9
-- Epoch 5
Norm: 22.07, NNZs: 18142, Bias: -1.096174, T: 27153, Avg. loss: 0.025611
Total training time: 0.07 seconds.
-- Epoch 4
-- Epoch 1
Convergence after 12 epochs took 0.57 seconds
-- Epoch 3
Norm: 23.55, NNZs: 33507, Bias: -1.076548, T: 63357, Avg. loss: 0.023621
Total training time: 0.11 seconds.
-- Epoch 8
Norm: 25.52, NNZs: 15523, Bias: -1.203473, T: 18104, Avg. loss: 0.039017
Total training time: 0.07 seconds.
Norm: 23.51, NNZs: 32885, Bias: -1.128770, T: 99572, Avg. loss: 0.035676
Total training time: 0.45 seconds.
Norm: 23.56, NNZs: 27448, Bias: -1.005696, T: 90510, Avg. loss: 0.022050
Total training time: 0.20 seconds.
Norm: 21.28, NNZs: 26028, Bias: -1.001082, T: 108612, Avg. loss: 0.015367
-- Epoch 3
Total training time: 0.53 seconds.
-- Epoch 12
-- Epoch 11
Norm: 24.46, NNZs: 32140, Bias: -1.005454, T: 117676, Avg. loss: 0.030476
Total training time: 0.54 seconds.
Convergence after 12 epochs took 0.53 seconds
Convergence after 13 epochs took 0.54 seconds
Norm: 23.35, NNZs: 35318, Bias: -1.124509, T: 45255, Avg. loss: 0.027545
Total training time: 0.10 seconds.
Norm: 23.28, NNZs: 22017, Bias: -1.197324, T: 45255, Avg. loss: 0.027401
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 25.21, NNZs: 36572, Bias: -1.013661, T: 99561, Avg. loss: 0.031612
Total training time: 0.27 seconds.
-- Epoch 6
-- Epoch 12
Norm: 23.84, NNZs: 26961, Bias: -1.006119, T: 108612, Avg. loss: 0.028428
Total training time: 0.55 seconds.
Convergence after 12 epochs took 0.56 seconds
Norm: 26.23, NNZs: 22988, Bias: -1.159634, T: 27156, Avg. loss: 0.042757
Norm: 23.88, NNZs: 23597, Bias: -1.103108, T: 81459, Avg. loss: 0.025999
Total training time: 0.14 seconds.
Norm: 24.46, NNZs: 29481, Bias: -1.028879, T: 99561, Avg. loss: 0.028855
Total training time: 0.57 seconds.
-- Epoch 10
-- Epoch 12
Norm: 23.48, NNZs: 34211, Bias: -1.066328, T: 72408, Avg. loss: 0.022957
Total training time: 0.12 seconds.
-- Epoch 9
Norm: 20.16, NNZs: 17078, Bias: -1.147756, T: 27153, Avg. loss: 0.040151
Total training time: 0.05 seconds.
Total training time: 0.13 seconds.
-- Epoch 4
-- Epoch 4
Norm: 23.16, NNZs: 22849, Bias: -1.176262, T: 54306, Avg. loss: 0.025973
Total training time: 0.09 seconds.
Norm: 23.33, NNZs: 40441, Bias: -1.091197, T: 54306, Avg. loss: 0.026142
Total training time: 0.10 seconds.
Total training time: 0.05 seconds.
-- Epoch 7
Norm: 24.33, NNZs: 17932, Bias: -1.159792, T: 27156, Avg. loss: 0.032247
Total training time: 0.08 seconds.
-- Epoch 4
-- Epoch 7
Norm: 21.79, NNZs: 19963, Bias: -1.085300, T: 36204, Avg. loss: 0.023272
Total training time: 0.08 seconds.
Norm: 24.04, NNZs: 25798, Bias: -1.006606, T: 54306, Avg. loss: 0.024643
Total training time: 0.16 seconds.
-- Epoch 5
-- Epoch 4
-- Epoch 7
Norm: 23.93, NNZs: 23785, Bias: -1.099811, T: 90510, Avg. loss: 0.025682
Total training time: 0.14 seconds.
-- Epoch 11
Norm: 27.79, NNZs: 16401, Bias: -1.371051, T: 9052, Avg. loss: 0.080573
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 23.46, NNZs: 35656, Bias: -1.060667, T: 81459, Avg. loss: 0.022285
Total training time: 0.13 seconds.
-- Epoch 2
Norm: 22.77, NNZs: 21892, Bias: -1.016812, T: 126714, Avg. loss: 0.026514
Norm: 23.77, NNZs: 29869, Bias: -1.007488, T: 117676, Avg. loss: 0.028432
Total training time: 0.58 seconds.
Total training time: 0.55 seconds.
-- Epoch 10
-- Epoch 14
Convergence after 14 epochs took 0.58 seconds
Norm: 25.44, NNZs: 18430, Bias: -1.291360, T: 18102, Avg. loss: 0.046088
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 20.45, NNZs: 14420, Bias: -1.237482, T: 18102, Avg. loss: 0.050184
Total training time: 0.05 seconds.
Norm: 23.51, NNZs: 16423, Bias: -1.139779, T: 18104, Avg. loss: 0.030097
Total training time: 0.07 seconds.
Norm: 25.84, NNZs: 24840, Bias: -1.119930, T: 36208, Avg. loss: 0.038619
Total training time: 0.13 seconds.
-- Epoch 3
-- Epoch 3
-- Epoch 5
Norm: 23.12, NNZs: 24346, Bias: -1.140539, T: 63357, Avg. loss: 0.025227
Total training time: 0.10 seconds.
Norm: 21.42, NNZs: 20934, Bias: -1.059037, T: 45255, Avg. loss: 0.021592
Total training time: 0.09 seconds.
-- Epoch 8
Norm: 24.11, NNZs: 29005, Bias: -1.138793, T: 36204, Avg. loss: 0.030017
Total training time: 0.07 seconds.
-- Epoch 6
-- Epoch 5
Norm: 23.92, NNZs: 26309, Bias: -1.005326, T: 63357, Avg. loss: 0.023675
Total training time: 0.17 seconds.
Norm: 25.56, NNZs: 23779, Bias: -1.091289, T: 45255, Avg. loss: 0.036457
Total training time: 0.18 seconds.
-- Epoch 6
Norm: 27.45, NNZs: 14403, Bias: -1.379276, T: 9051, Avg. loss: 0.078854
Total training time: 0.03 seconds.
Norm: 22.69, NNZs: 11477, Bias: -1.275621, T: 9052, Avg. loss: 0.077873
Total training time: 0.03 seconds.
-- Epoch 2
-- Epoch 2
Norm: 25.03, NNZs: 20129, Bias: -1.131555, T: 27153, Avg. loss: 0.032879
Total training time: 0.09 seconds.
Norm: 23.41, NNZs: 35847, Bias: -1.052486, T: 90510, Avg. loss: 0.021838
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 23.60, NNZs: 27944, Bias: -1.008395, T: 99561, Avg. loss: 0.021886
Total training time: 0.22 seconds.
-- Epoch 11
-- Epoch 12
Norm: 19.74, NNZs: 16565, Bias: -1.170287, T: 27153, Avg. loss: 0.041908
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 24.26, NNZs: 19865, Bias: -1.226543, T: 27153, Avg. loss: 0.038652
Total training time: 0.08 seconds.
-- Epoch 4
Norm: 23.49, NNZs: 33038, Bias: -1.123946, T: 108624, Avg. loss: 0.035427
Total training time: 0.47 seconds.
Norm: 23.09, NNZs: 24897, Bias: -1.132426, T: 72408, Avg. loss: 0.024392
Total training time: 0.11 seconds.
Convergence after 12 epochs took 0.47 seconds
-- Epoch 9
Norm: 19.91, NNZs: 18072, Bias: -1.156556, T: 36204, Avg. loss: 0.036632
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 25.92, NNZs: 22742, Bias: -1.129484, T: 36204, Avg. loss: 0.039720
Total training time: 0.14 seconds.
Norm: 26.60, NNZs: 16946, Bias: -1.140199, T: 18104, Avg. loss: 0.037064
Total training time: 0.10 seconds.
Norm: 25.46, NNZs: 29511, Bias: -1.057843, T: 54306, Avg. loss: 0.035116
Total training time: 0.19 seconds.
-- Epoch 5
-- Epoch 3
-- Epoch 7
Norm: 23.98, NNZs: 22893, Bias: -1.031163, T: 45255, Avg. loss: 0.021602
Total training time: 0.16 seconds.
Norm: 24.79, NNZs: 23369, Bias: -1.067121, T: 36204, Avg. loss: 0.029621
Total training time: 0.10 seconds.
Norm: 23.35, NNZs: 41222, Bias: -1.056942, T: 99561, Avg. loss: 0.021443
Total training time: 0.14 seconds.
-- Epoch 6
-- Epoch 5
Norm: 23.54, NNZs: 28127, Bias: -1.001531, T: 108612, Avg. loss: 0.021604
Total training time: 0.23 seconds.
-- Epoch 12
-- Epoch 8
Convergence after 12 epochs took 0.23 seconds
Norm: 26.26, NNZs: 17258, Bias: -1.135668, T: 18102, Avg. loss: 0.034747
Total training time: 0.14 seconds.
Norm: 23.92, NNZs: 29258, Bias: -1.005941, T: 90510, Avg. loss: 0.019323
Total training time: 0.31 seconds.
-- Epoch 3
Convergence after 10 epochs took 0.31 seconds
Norm: 23.91, NNZs: 24034, Bias: -1.090470, T: 99561, Avg. loss: 0.025091
Total training time: 0.16 seconds.
Norm: 25.59, NNZs: 28702, Bias: -1.084379, T: 45260, Avg. loss: 0.036230
Total training time: 0.15 seconds.
-- Epoch 12
Norm: 23.05, NNZs: 25299, Bias: -1.143152, T: 81459, Avg. loss: 0.023624
Total training time: 0.11 seconds.
-- Epoch 6
-- Epoch 10
Norm: 24.21, NNZs: 21067, Bias: -1.187693, T: 36204, Avg. loss: 0.034805
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 24.72, NNZs: 17330, Bias: -1.304679, T: 18102, Avg. loss: 0.042837
Total training time: 0.04 seconds.
Norm: 21.31, NNZs: 21779, Bias: -1.019835, T: 54306, Avg. loss: 0.020771
Total training time: 0.10 seconds.
Norm: 25.38, NNZs: 30015, Bias: -1.051501, T: 63357, Avg. loss: 0.033821
Total training time: 0.20 seconds.
-- Epoch 3
-- Epoch 7
-- Epoch 8
Norm: 24.47, NNZs: 32249, Bias: -1.024718, T: 108612, Avg. loss: 0.028547
Total training time: 0.60 seconds.
Norm: 24.74, NNZs: 19703, Bias: -1.301794, T: 18104, Avg. loss: 0.042624
Total training time: 0.04 seconds.
Convergence after 12 epochs took 0.60 seconds
-- Epoch 3
Norm: 25.11, NNZs: 20017, Bias: -1.099897, T: 27156, Avg. loss: 0.027145
Total training time: 0.15 seconds.
Norm: 23.34, NNZs: 41343, Bias: -1.046795, T: 108612, Avg. loss: 0.021253
Norm: 23.84, NNZs: 26711, Bias: -1.006517, T: 72408, Avg. loss: 0.023042
Total training time: 0.19 seconds.
Total training time: 0.15 seconds.
-- Epoch 4
-- Epoch 9
Convergence after 12 epochs took 0.15 seconds
Norm: 25.00, NNZs: 19258, Bias: -1.090049, T: 27153, Avg. loss: 0.028634
Total training time: 0.14 seconds.
Norm: 23.24, NNZs: 40990, Bias: -1.075743, T: 63357, Avg. loss: 0.024986
Total training time: 0.13 seconds.
Norm: 23.33, NNZs: 16332, Bias: -1.157018, T: 18102, Avg. loss: 0.029155
Total training time: 0.08 seconds.
-- Epoch 8
-- Epoch 3
Norm: 23.00, NNZs: 26065, Bias: -1.138337, T: 90510, Avg. loss: 0.023350
Total training time: 0.12 seconds.
Norm: 22.66, NNZs: 17965, Bias: -1.068067, T: 27156, Avg. loss: 0.025026
Norm: 19.84, NNZs: 17190, Bias: -1.159325, T: 36204, Avg. loss: 0.037537
Total training time: 0.10 seconds.
Total training time: 0.09 seconds.
-- Epoch 11
-- Epoch 5
-- Epoch 4
Norm: 21.23, NNZs: 22755, Bias: -1.019074, T: 63357, Avg. loss: 0.019827
Total training time: 0.11 seconds.
-- Epoch 8
Norm: 23.78, NNZs: 32304, Bias: -1.003205, T: 126728, Avg. loss: 0.028347
Total training time: 0.57 seconds.
Convergence after 14 epochs took 0.57 seconds
Norm: 23.82, NNZs: 26634, Bias: -1.015586, T: 54306, Avg. loss: 0.020677
Total training time: 0.17 seconds.
Norm: 25.65, NNZs: 28180, Bias: -1.096039, T: 45255, Avg. loss: 0.036493
Norm: 19.76, NNZs: 19320, Bias: -1.121862, T: 45255, Avg. loss: 0.034526
Total training time: 0.08 seconds.
-- Epoch 7
-- Epoch 6
Norm: 25.34, NNZs: 19547, Bias: -1.207833, T: 27156, Avg. loss: 0.036974
Total training time: 0.12 seconds.
Norm: 21.80, NNZs: 19519, Bias: -1.067722, T: 36204, Avg. loss: 0.022597
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 25.18, NNZs: 37027, Bias: -1.004743, T: 108612, Avg. loss: 0.031215
Total training time: 0.32 seconds.
-- Epoch 4
-- Epoch 13
Total training time: 0.16 seconds.
-- Epoch 4
-- Epoch 6
Norm: 21.18, NNZs: 23221, Bias: -1.016061, T: 72408, Avg. loss: 0.019301
Total training time: 0.12 seconds.
Norm: 22.32, NNZs: 18353, Bias: -1.096913, T: 27153, Avg. loss: 0.024051
Total training time: 0.09 seconds.
-- Epoch 9
-- Epoch 4
Norm: 25.23, NNZs: 33965, Bias: -1.036273, T: 72408, Avg. loss: 0.032749
Total training time: 0.22 seconds.
Norm: 19.73, NNZs: 20144, Bias: -1.114108, T: 54306, Avg. loss: 0.033367
Total training time: 0.09 seconds.
Norm: 23.13, NNZs: 23609, Bias: -1.141995, T: 45255, Avg. loss: 0.030018
Total training time: 0.10 seconds.
Norm: 23.84, NNZs: 24252, Bias: -1.084144, T: 108612, Avg. loss: 0.024812
-- Epoch 9
Total training time: 0.18 seconds.
-- Epoch 7
-- Epoch 6
Norm: 20.57, NNZs: 14741, Bias: -1.237382, T: 18104, Avg. loss: 0.048051
Total training time: 0.06 seconds.
Convergence after 12 epochs took 0.18 seconds
Norm: 25.20, NNZs: 19749, Bias: -1.110184, T: 27156, Avg. loss: 0.030723
Total training time: 0.13 seconds.
-- Epoch 3
-- Epoch 4
Norm: 23.96, NNZs: 20350, Bias: -1.094572, T: 36208, Avg. loss: 0.029426
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 23.78, NNZs: 22567, Bias: -1.147025, T: 63357, Avg. loss: 0.028570
Norm: 24.50, NNZs: 20671, Bias: -1.151709, T: 36208, Avg. loss: 0.032354
Total training time: 0.13 seconds.
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 19.54, NNZs: 17932, Bias: -1.162389, T: 36204, Avg. loss: 0.038311
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 23.79, NNZs: 27301, Bias: -1.005065, T: 81459, Avg. loss: 0.022510
Total training time: 0.21 seconds.
Norm: 23.15, NNZs: 41579, Bias: -1.068405, T: 72408, Avg. loss: 0.024377
Total training time: 0.15 seconds.
-- Epoch 8
-- Epoch 10
-- Epoch 9
Norm: 21.72, NNZs: 19606, Bias: -1.084136, T: 36204, Avg. loss: 0.021902
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 25.19, NNZs: 37092, Bias: -1.003847, T: 117663, Avg. loss: 0.030962
Total training time: 0.33 seconds.
Norm: 23.97, NNZs: 21765, Bias: -1.164194, T: 45255, Avg. loss: 0.031863
Total training time: 0.12 seconds.
Convergence after 13 epochs took 0.33 seconds
Norm: 24.34, NNZs: 24327, Bias: -1.029366, T: 45255, Avg. loss: 0.027429
Total training time: 0.13 seconds.
-- Epoch 6
-- Epoch 6
Norm: 25.50, NNZs: 29973, Bias: -1.075414, T: 54312, Avg. loss: 0.034078
Total training time: 0.18 seconds.
Norm: 21.49, NNZs: 20372, Bias: -1.048452, T: 45255, Avg. loss: 0.020967
-- Epoch 7
Norm: 23.15, NNZs: 24272, Bias: -1.126704, T: 54306, Avg. loss: 0.028772
Total training time: 0.15 seconds.
Total training time: 0.11 seconds.
-- Epoch 7
-- Epoch 6
Norm: 24.72, NNZs: 21181, Bias: -1.065641, T: 36208, Avg. loss: 0.027473
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 19.67, NNZs: 20651, Bias: -1.102658, T: 63357, Avg. loss: 0.031941
Total training time: 0.11 seconds.
Norm: 19.47, NNZs: 18673, Bias: -1.125707, T: 45255, Avg. loss: 0.036046
Total training time: 0.10 seconds.
-- Epoch 8
Norm: 23.03, NNZs: 26214, Bias: -1.127766, T: 99561, Avg. loss: 0.022769
Total training time: 0.15 seconds.
Convergence after 11 epochs took 0.15 seconds
Norm: 23.74, NNZs: 29450, Bias: -1.006956, T: 90510, Avg. loss: 0.022086
Total training time: 0.22 seconds.
Norm: 21.06, NNZs: 23861, Bias: -1.015196, T: 81459, Avg. loss: 0.018795
Total training time: 0.14 seconds.
Norm: 23.72, NNZs: 22887, Bias: -1.129636, T: 72408, Avg. loss: 0.027623
Total training time: 0.15 seconds.
-- Epoch 11
-- Epoch 10
-- Epoch 9
Norm: 24.62, NNZs: 21696, Bias: -1.055851, T: 36208, Avg. loss: 0.023547
Total training time: 0.19 seconds.
Norm: 21.91, NNZs: 19105, Bias: -1.057942, T: 36208, Avg. loss: 0.022137
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 23.76, NNZs: 21085, Bias: -1.098919, T: 45260, Avg. loss: 0.027163
Total training time: 0.14 seconds.
-- Epoch 5
-- Epoch 6
Norm: 25.18, NNZs: 34804, Bias: -1.027077, T: 81459, Avg. loss: 0.032167
Total training time: 0.23 seconds.
Norm: 23.06, NNZs: 25039, Bias: -1.098594, T: 63357, Avg. loss: 0.027629
Total training time: 0.12 seconds.
-- Epoch 10
Norm: 23.94, NNZs: 21733, Bias: -1.223231, T: 27156, Avg. loss: 0.034907
Total training time: 0.07 seconds.
-- Epoch 8
Norm: 23.80, NNZs: 27653, Bias: -1.007078, T: 63357, Avg. loss: 0.019778
Total training time: 0.20 seconds.
-- Epoch 4
-- Epoch 8
Norm: 19.69, NNZs: 21237, Bias: -1.090314, T: 72408, Avg. loss: 0.031397
Total training time: 0.11 seconds.
-- Epoch 9
Norm: 20.06, NNZs: 16629, Bias: -1.170721, T: 27156, Avg. loss: 0.040466
Total training time: 0.08 seconds.
-- Epoch 4
Norm: 23.08, NNZs: 41916, Bias: -1.064183, T: 81459, Avg. loss: 0.023740
Total training time: 0.17 seconds.
-- Epoch 10
Norm: 21.06, NNZs: 24215, Bias: -1.007627, T: 90510, Avg. loss: 0.018468
Total training time: 0.15 seconds.
Convergence after 10 epochs took 0.15 seconds
Norm: 23.53, NNZs: 26909, Bias: -1.059348, T: 54312, Avg. loss: 0.025867
Total training time: 0.14 seconds.
-- Epoch 7
Norm: 25.17, NNZs: 37788, Bias: -1.016823, T: 90510, Avg. loss: 0.031762
Total training time: 0.24 seconds.
-- Epoch 11
Norm: 23.04, NNZs: 25381, Bias: -1.082407, T: 72408, Avg. loss: 0.026881
Total training time: 0.13 seconds.
Norm: 23.69, NNZs: 28715, Bias: -1.004419, T: 72408, Avg. loss: 0.018998
Total training time: 0.21 seconds.
-- Epoch 9
-- Epoch 9
Norm: 19.71, NNZs: 18288, Bias: -1.138028, T: 45255, Avg. loss: 0.035193
Total training time: 0.13 seconds.
Norm: 23.77, NNZs: 19184, Bias: -1.228005, T: 27153, Avg. loss: 0.035340
Total training time: 0.09 seconds.
-- Epoch 6
-- Epoch 4
Norm: 23.72, NNZs: 22564, Bias: -1.150110, T: 54306, Avg. loss: 0.030442
Total training time: 0.14 seconds.
Norm: 23.67, NNZs: 23054, Bias: -1.124974, T: 81459, Avg. loss: 0.026955
Total training time: 0.16 seconds.
-- Epoch 7
-- Epoch 10
Norm: 19.69, NNZs: 22051, Bias: -1.077116, T: 81459, Avg. loss: 0.030869
Total training time: 0.12 seconds.
-- Epoch 10
Norm: 23.04, NNZs: 43789, Bias: -1.050431, T: 90510, Avg. loss: 0.023420
Total training time: 0.17 seconds.
-- Epoch 11
Norm: 21.48, NNZs: 20705, Bias: -1.050983, T: 45255, Avg. loss: 0.020379
Norm: 24.45, NNZs: 20752, Bias: -1.070239, T: 36204, Avg. loss: 0.025509
Total training time: 0.19 seconds.
Total training time: 0.12 seconds.
-- Epoch 6
-- Epoch 5
-- Epoch 6
Norm: 25.49, NNZs: 28718, Bias: -1.065598, T: 54306, Avg. loss: 0.035289
Total training time: 0.20 seconds.
Norm: 22.95, NNZs: 25658, Bias: -1.088308, T: 81459, Avg. loss: 0.026158
Total training time: 0.13 seconds.
-- Epoch 7
-- Epoch 10
Norm: 21.23, NNZs: 21531, Bias: -1.012822, T: 54306, Avg. loss: 0.020113
Norm: 23.71, NNZs: 23391, Bias: -1.120375, T: 90510, Avg. loss: 0.026521
Total training time: 0.16 seconds.
Norm: 19.64, NNZs: 19108, Bias: -1.106893, T: 54306, Avg. loss: 0.033790
Total training time: 0.14 seconds.
-- Epoch 11
-- Epoch 7
Norm: 23.71, NNZs: 22954, Bias: -1.129679, T: 63357, Avg. loss: 0.029467
Total training time: 0.15 seconds.
Total training time: 0.17 seconds.
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.8s remaining:    0.1s
-- Epoch 7
Norm: 19.65, NNZs: 22451, Bias: -1.085936, T: 90510, Avg. loss: 0.030335
Total training time: 0.13 seconds.
Norm: 21.58, NNZs: 20239, Bias: -1.048771, T: 45260, Avg. loss: 0.020866
Total training time: 0.14 seconds.
-- Epoch 11
-- Epoch 6
Norm: 23.00, NNZs: 43981, Bias: -1.057286, T: 99561, Avg. loss: 0.022988
Total training time: 0.18 seconds.
-- Epoch 12
Norm: 25.15, NNZs: 37973, Bias: -1.025603, T: 99561, Avg. loss: 0.031182
Total training time: 0.25 seconds.
Norm: 23.47, NNZs: 23073, Bias: -1.190530, T: 36208, Avg. loss: 0.031377
-- Epoch 12
Norm: 24.22, NNZs: 22879, Bias: -1.058504, T: 45260, Avg. loss: 0.021903
Total training time: 0.21 seconds.
Norm: 22.97, NNZs: 25959, Bias: -1.081379, T: 90510, Avg. loss: 0.025859
Total training time: 0.14 seconds.
-- Epoch 6
-- Epoch 11
Norm: 24.53, NNZs: 21701, Bias: -1.165169, T: 45260, Avg. loss: 0.030654
Total training time: 0.17 seconds.
Norm: 24.42, NNZs: 23829, Bias: -1.046885, T: 45260, Avg. loss: 0.025627
Total training time: 0.17 seconds.
-- Epoch 6
-- Epoch 6
Norm: 19.63, NNZs: 19747, Bias: -1.096303, T: 63357, Avg. loss: 0.032703
Total training time: 0.15 seconds.
Norm: 23.37, NNZs: 27617, Bias: -1.066291, T: 63364, Avg. loss: 0.024917
Total training time: 0.16 seconds.
Norm: 23.71, NNZs: 23654, Bias: -1.104026, T: 99561, Avg. loss: 0.026102
Total training time: 0.17 seconds.
-- Epoch 8
-- Epoch 8
Convergence after 11 epochs took 0.17 seconds
Norm: 19.75, NNZs: 18305, Bias: -1.168184, T: 36208, Avg. loss: 0.036850
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 19.62, NNZs: 22714, Bias: -1.064977, T: 99561, Avg. loss: 0.030028
Total training time: 0.13 seconds.
Total training time: 0.10 seconds.
-- Epoch 12
Norm: 23.02, NNZs: 45194, Bias: -1.044678, T: 108612, Avg. loss: 0.022814
Total training time: 0.19 seconds.
Norm: 21.13, NNZs: 22700, Bias: -1.016073, T: 63357, Avg. loss: 0.019404
Total training time: 0.18 seconds.
Convergence after 12 epochs took 0.19 seconds
-- Epoch 8
Norm: 25.14, NNZs: 39240, Bias: -1.009681, T: 108612, Avg. loss: 0.030831
Total training time: 0.26 seconds.
-- Epoch 13
Norm: 24.03, NNZs: 23492, Bias: -1.022666, T: 54312, Avg. loss: 0.020877
Total training time: 0.21 seconds.
Norm: 24.32, NNZs: 22233, Bias: -1.130076, T: 54312, Avg. loss: 0.029163
Total training time: 0.17 seconds.
Norm: 22.97, NNZs: 26180, Bias: -1.072383, T: 99561, Avg. loss: 0.025390
-- Epoch 7
Total training time: 0.15 seconds.
-- Epoch 7
-- Epoch 12
Norm: 24.28, NNZs: 24598, Bias: -1.029384, T: 54312, Avg. loss: 0.024345
Total training time: 0.17 seconds.
Norm: 23.83, NNZs: 30453, Bias: -1.120646, T: 45255, Avg. loss: 0.027770
Norm: 19.60, NNZs: 20020, Bias: -1.080294, T: 72408, Avg. loss: 0.032025
Total training time: 0.15 seconds.
-- Epoch 7
-- Epoch 9
Norm: 19.62, NNZs: 18975, Bias: -1.118945, T: 45260, Avg. loss: 0.034851
Total training time: 0.10 seconds.
Norm: 23.29, NNZs: 20845, Bias: -1.194150, T: 36204, Avg. loss: 0.031601
Total training time: 0.11 seconds.
Norm: 19.44, NNZs: 19229, Bias: -1.094488, T: 54306, Avg. loss: 0.034405
Norm: 25.46, NNZs: 33948, Bias: -1.048585, T: 63357, Avg. loss: 0.034071
Total training time: 0.22 seconds.
Total training time: 0.13 seconds.
Norm: 25.31, NNZs: 31176, Bias: -1.054423, T: 63364, Avg. loss: 0.033050
-- Epoch 5
Total training time: 0.21 seconds.
-- Epoch 8
Norm: 23.37, NNZs: 28013, Bias: -1.043370, T: 72416, Avg. loss: 0.024156
Total training time: 0.17 seconds.
-- Epoch 5
Total training time: 0.15 seconds.
-- Epoch 7
-- Epoch 8
-- Epoch 6
Norm: 19.64, NNZs: 23102, Bias: -1.066805, T: 108612, Avg. loss: 0.029770
Total training time: 0.14 seconds.
Norm: 24.22, NNZs: 21799, Bias: -1.021649, T: 45255, Avg. loss: 0.023443
Total training time: 0.21 seconds.
Convergence after 12 epochs took 0.14 seconds
-- Epoch 6
-- Epoch 8
Norm: 22.97, NNZs: 26272, Bias: -1.061616, T: 108612, Avg. loss: 0.025179
Total training time: 0.15 seconds.
Convergence after 12 epochs took 0.15 seconds
Norm: 23.69, NNZs: 29689, Bias: -1.007530, T: 99561, Avg. loss: 0.021674
Total training time: 0.25 seconds.
Norm: 24.35, NNZs: 22810, Bias: -1.125198, T: 63364, Avg. loss: 0.028056
Total training time: 0.18 seconds.
Convergence after 11 epochs took 0.25 seconds
-- Epoch 8
Norm: 21.24, NNZs: 21431, Bias: -1.025963, T: 54306, Avg. loss: 0.019670
Norm: 19.35, NNZs: 19881, Bias: -1.089725, T: 63357, Avg. loss: 0.033533
Total training time: 0.15 seconds.
Total training time: 0.14 seconds.
Norm: 23.71, NNZs: 23419, Bias: -1.120120, T: 72408, Avg. loss: 0.028758
Total training time: 0.17 seconds.
Norm: 21.13, NNZs: 23054, Bias: -1.007143, T: 72408, Avg. loss: 0.018832
Total training time: 0.19 seconds.
Norm: 23.77, NNZs: 35527, Bias: -1.093421, T: 54306, Avg. loss: 0.026522
Total training time: 0.16 seconds.
-- Epoch 7
-- Epoch 9
-- Epoch 9
-- Epoch 7
Norm: 23.98, NNZs: 22878, Bias: -1.028809, T: 54306, Avg. loss: 0.022516
Total training time: 0.21 seconds.
Norm: 25.32, NNZs: 31633, Bias: -1.042834, T: 72416, Avg. loss: 0.032139
Total training time: 0.22 seconds.
-- Epoch 7
-- Epoch 9
Norm: 25.39, NNZs: 36917, Bias: -1.042721, T: 72408, Avg. loss: 0.033022
Norm: 23.53, NNZs: 32205, Bias: -1.005456, T: 81459, Avg. loss: 0.018646
Total training time: 0.23 seconds.
Total training time: 0.24 seconds.
-- Epoch 9
-- Epoch 10
Norm: 24.24, NNZs: 25873, Bias: -1.010489, T: 63364, Avg. loss: 0.023600
Total training time: 0.18 seconds.
Norm: 21.56, NNZs: 21718, Bias: -1.014083, T: 54312, Avg. loss: 0.019781
Total training time: 0.16 seconds.
-- Epoch 8
-- Epoch 7
Norm: 24.15, NNZs: 25276, Bias: -1.020203, T: 54306, Avg. loss: 0.026431
Total training time: 0.18 seconds.
-- Epoch 7
Norm: 25.16, NNZs: 39438, Bias: -1.010443, T: 117663, Avg. loss: 0.030517
Total training time: 0.28 seconds.
-- Epoch 8
Norm: 24.31, NNZs: 23415, Bias: -1.102980, T: 72416, Avg. loss: 0.026998
Total training time: 0.19 seconds.
-- Epoch 9
Norm: 23.17, NNZs: 21755, Bias: -1.165373, T: 45255, Avg. loss: 0.029387
Total training time: 0.12 seconds.
Norm: 23.70, NNZs: 23662, Bias: -1.107973, T: 81459, Avg. loss: 0.027962
Total training time: 0.17 seconds.
-- Epoch 6
Norm: 23.67, NNZs: 36109, Bias: -1.073303, T: 63357, Avg. loss: 0.025110
Total training time: 0.16 seconds.
-- Epoch 10
-- Epoch 8
Norm: 21.03, NNZs: 23248, Bias: -1.005659, T: 81459, Avg. loss: 0.018305
Total training time: 0.20 seconds.
-- Epoch 10
-- Epoch 6
-- Epoch 9
Norm: 25.35, NNZs: 37279, Bias: -1.039986, T: 81459, Avg. loss: 0.032158
Total training time: 0.23 seconds.
Norm: 25.16, NNZs: 36261, Bias: -1.047406, T: 81468, Avg. loss: 0.031300
Total training time: 0.23 seconds.
-- Epoch 10
-- Epoch 10
Norm: 21.34, NNZs: 22324, Bias: -1.014073, T: 63364, Avg. loss: 0.019139
Total training time: 0.17 seconds.
Norm: 24.14, NNZs: 26340, Bias: -1.017264, T: 72416, Avg. loss: 0.022333
Total training time: 0.19 seconds.
-- Epoch 8
-- Epoch 9
Norm: 24.07, NNZs: 27037, Bias: -1.012500, T: 63357, Avg. loss: 0.025540
Total training time: 0.19 seconds.
-- Epoch 8
Norm: 19.35, NNZs: 20274, Bias: -1.080217, T: 72408, Avg. loss: 0.032769
Total training time: 0.15 seconds.
-- Epoch 9
Norm: 19.61, NNZs: 20368, Bias: -1.086143, T: 81459, Avg. loss: 0.031620
Total training time: 0.17 seconds.
-- Epoch 10
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.8s finished
Norm: 23.22, NNZs: 23992, Bias: -1.169539, T: 45260, Avg. loss: 0.028858
Total training time: 0.12 seconds.
Norm: 19.56, NNZs: 19867, Bias: -1.101364, T: 54312, Avg. loss: 0.033789
Total training time: 0.13 seconds.
Norm: 21.10, NNZs: 22592, Bias: -1.012099, T: 63357, Avg. loss: 0.018707
Norm: 23.38, NNZs: 28601, Bias: -1.031437, T: 81468, Avg. loss: 0.023668
Total training time: 0.16 seconds.
Total training time: 0.19 seconds.
-- Epoch 6
-- Epoch 7
-- Epoch 8
-- Epoch 10
Norm: 25.25, NNZs: 36611, Bias: -1.034834, T: 90520, Avg. loss: 0.030999
Total training time: 0.24 seconds.
-- Epoch 11
Convergence after 13 epochs took 0.29 seconds
Norm: 24.09, NNZs: 26688, Bias: -1.001478, T: 81468, Avg. loss: 0.022125
Total training time: 0.20 seconds.
Norm: 24.06, NNZs: 26754, Bias: -1.013126, T: 63364, Avg. loss: 0.020136
Total training time: 0.24 seconds.
Norm: 23.86, NNZs: 26147, Bias: -1.002894, T: 63357, Avg. loss: 0.021547
Total training time: 0.23 seconds.
-- Epoch 10
-- Epoch 8
-- Epoch 8
Norm: 23.11, NNZs: 22570, Bias: -1.156765, T: 54306, Avg. loss: 0.027773
Total training time: 0.14 seconds.
-- Epoch 7
Norm: 19.35, NNZs: 20573, Bias: -1.071466, T: 81459, Avg. loss: 0.032297
Total training time: 0.16 seconds.
Norm: 19.58, NNZs: 20206, Bias: -1.079773, T: 63364, Avg. loss: 0.032834
Total training time: 0.13 seconds.
-- Epoch 10
-- Epoch 8
Norm: 25.16, NNZs: 36779, Bias: -1.028260, T: 99572, Avg. loss: 0.030407
Total training time: 0.25 seconds.
Norm: 23.68, NNZs: 36505, Bias: -1.066638, T: 72408, Avg. loss: 0.024664
Total training time: 0.18 seconds.
-- Epoch 12
Norm: 23.26, NNZs: 24806, Bias: -1.139508, T: 54312, Avg. loss: 0.027999
Total training time: 0.13 seconds.
-- Epoch 9
-- Epoch 7
Norm: 21.06, NNZs: 23171, Bias: -1.004832, T: 72408, Avg. loss: 0.018277
Norm: 21.33, NNZs: 22944, Bias: -1.006371, T: 72416, Avg. loss: 0.018552
Total training time: 0.19 seconds.
Total training time: 0.17 seconds.
Norm: 23.70, NNZs: 23845, Bias: -1.104678, T: 90510, Avg. loss: 0.027380
Total training time: 0.19 seconds.
Norm: 23.80, NNZs: 26573, Bias: -1.011320, T: 72408, Avg. loss: 0.020900
Total training time: 0.24 seconds.
Norm: 23.97, NNZs: 27007, Bias: -1.008250, T: 90520, Avg. loss: 0.021520
Total training time: 0.21 seconds.
-- Epoch 9
-- Epoch 9
-- Epoch 11
-- Epoch 9
-- Epoch 11
Norm: 24.24, NNZs: 23629, Bias: -1.096453, T: 81468, Avg. loss: 0.026494
Total training time: 0.21 seconds.
-- Epoch 10
Norm: 19.31, NNZs: 20971, Bias: -1.071642, T: 90510, Avg. loss: 0.031800
Total training time: 0.17 seconds.
-- Epoch 11
Norm: 25.18, NNZs: 36896, Bias: -1.016559, T: 108624, Avg. loss: 0.030191
Total training time: 0.25 seconds.
Norm: 23.46, NNZs: 37047, Bias: -1.067297, T: 81459, Avg. loss: 0.023878
Total training time: 0.19 seconds.
Norm: 23.10, NNZs: 25294, Bias: -1.135330, T: 63364, Avg. loss: 0.026652
Total training time: 0.14 seconds.
-- Epoch 10
Convergence after 12 epochs took 0.25 seconds
-- Epoch 8
Norm: 24.03, NNZs: 27394, Bias: -1.001976, T: 72408, Avg. loss: 0.024713
Total training time: 0.21 seconds.
-- Epoch 9
Norm: 23.75, NNZs: 24149, Bias: -1.091572, T: 99561, Avg. loss: 0.026901
Total training time: 0.20 seconds.
Convergence after 11 epochs took 0.20 seconds
Norm: 21.26, NNZs: 23648, Bias: -1.006145, T: 81468, Avg. loss: 0.018221
Total training time: 0.20 seconds.
Norm: 23.00, NNZs: 23202, Bias: -1.138256, T: 63357, Avg. loss: 0.026662
Total training time: 0.15 seconds.
Norm: 23.97, NNZs: 27285, Bias: -1.009846, T: 99572, Avg. loss: 0.021512
Total training time: 0.22 seconds.
Norm: 24.15, NNZs: 23855, Bias: -1.096994, T: 90520, Avg. loss: 0.025795
Total training time: 0.22 seconds.
-- Epoch 10
-- Epoch 8
-- Epoch 12
Norm: 23.51, NNZs: 32568, Bias: -0.997658, T: 90510, Avg. loss: 0.018374
Norm: 19.58, NNZs: 20698, Bias: -1.081523, T: 90510, Avg. loss: 0.030990
Total training time: 0.20 seconds.
Total training time: 0.27 seconds.
Convergence after 10 epochs took 0.27 seconds
-- Epoch 11
Norm: 21.06, NNZs: 23792, Bias: -0.998907, T: 81459, Avg. loss: 0.017802
Total training time: 0.18 seconds.
-- Epoch 10
Norm: 23.24, NNZs: 28954, Bias: -1.026795, T: 90520, Avg. loss: 0.023294
Total training time: 0.21 seconds.
Norm: 23.52, NNZs: 37506, Bias: -1.048555, T: 90510, Avg. loss: 0.023607
Total training time: 0.20 seconds.
-- Epoch 11
-- Epoch 11
Norm: 23.07, NNZs: 25717, Bias: -1.122404, T: 72416, Avg. loss: 0.025973
Total training time: 0.15 seconds.
-- Epoch 9
Norm: 23.95, NNZs: 27632, Bias: -1.014521, T: 81459, Avg. loss: 0.023936
Total training time: 0.21 seconds.
Norm: 19.57, NNZs: 20725, Bias: -1.076823, T: 72416, Avg. loss: 0.032104
Total training time: 0.15 seconds.
-- Epoch 10
-- Epoch 9
Norm: 21.16, NNZs: 23889, Bias: -1.002641, T: 90520, Avg. loss: 0.017877
Total training time: 0.20 seconds.
-- Epoch 11
Norm: 22.96, NNZs: 24039, Bias: -1.119961, T: 72408, Avg. loss: 0.026403
Norm: 23.93, NNZs: 27625, Bias: -1.011190, T: 108624, Avg. loss: 0.021090
Total training time: 0.22 seconds.
Total training time: 0.16 seconds.
-- Epoch 9
-- Epoch 13
Norm: 20.95, NNZs: 24105, Bias: -1.012682, T: 90510, Avg. loss: 0.017366
Total training time: 0.19 seconds.
Norm: 25.34, NNZs: 37651, Bias: -1.028179, T: 90510, Avg. loss: 0.031576
Total training time: 0.27 seconds.
Convergence after 10 epochs took 0.19 seconds
-- Epoch 11
Norm: 19.58, NNZs: 20903, Bias: -1.055491, T: 99561, Avg. loss: 0.030643
Total training time: 0.20 seconds.
Norm: 23.84, NNZs: 27306, Bias: -1.000183, T: 72416, Avg. loss: 0.019328
Total training time: 0.27 seconds.
-- Epoch 12
-- Epoch 9
Norm: 23.22, NNZs: 30251, Bias: -1.027528, T: 99572, Avg. loss: 0.022993
Total training time: 0.22 seconds.
Norm: 20.98, NNZs: 23493, Bias: -1.005560, T: 90510, Avg. loss: 0.017899
Total training time: 0.23 seconds.
Norm: 19.30, NNZs: 21245, Bias: -1.050729, T: 99561, Avg. loss: 0.031488
Total training time: 0.18 seconds.
Convergence after 11 epochs took 0.22 seconds
Convergence after 10 epochs took 0.24 seconds
Convergence after 11 epochs took 0.18 seconds
Norm: 23.07, NNZs: 25935, Bias: -1.116496, T: 81468, Avg. loss: 0.025479
Total training time: 0.15 seconds.
-- Epoch 10
Norm: 19.55, NNZs: 21129, Bias: -1.081505, T: 81468, Avg. loss: 0.031247
Total training time: 0.16 seconds.
Norm: 23.91, NNZs: 28035, Bias: -1.009094, T: 90510, Avg. loss: 0.023673
Total training time: 0.22 seconds.
-- Epoch 10
Convergence after 10 epochs took 0.22 seconds
Norm: 21.13, NNZs: 24130, Bias: -1.004600, T: 99572, Avg. loss: 0.017752
Total training time: 0.21 seconds.
Norm: 22.92, NNZs: 24604, Bias: -1.108381, T: 81459, Avg. loss: 0.025482
Total training time: 0.16 seconds.
Convergence after 11 epochs took 0.21 seconds
-- Epoch 10
Norm: 23.93, NNZs: 29398, Bias: -0.998793, T: 117676, Avg. loss: 0.020785
Total training time: 0.23 seconds.
Convergence after 13 epochs took 0.23 seconds
-- Epoch 11
Norm: 25.27, NNZs: 37901, Bias: -1.040045, T: 99561, Avg. loss: 0.031017
Total training time: 0.27 seconds.
-- Epoch 12
Norm: 19.58, NNZs: 21052, Bias: -1.064061, T: 108612, Avg. loss: 0.030400
Total training time: 0.21 seconds.
Norm: 23.74, NNZs: 27743, Bias: -1.016371, T: 81468, Avg. loss: 0.018726
Total training time: 0.27 seconds.
Convergence after 12 epochs took 0.21 seconds
-- Epoch 10
Norm: 23.76, NNZs: 26920, Bias: -1.006836, T: 81459, Avg. loss: 0.020194
Total training time: 0.26 seconds.
-- Epoch 10
Norm: 23.03, NNZs: 26136, Bias: -1.102058, T: 90520, Avg. loss: 0.024962
Total training time: 0.16 seconds.
-- Epoch 11
Norm: 22.91, NNZs: 24873, Bias: -1.113528, T: 90510, Avg. loss: 0.025057
Total training time: 0.17 seconds.
-- Epoch 11
Norm: 24.17, NNZs: 24005, Bias: -1.086080, T: 99572, Avg. loss: 0.025542
Total training time: 0.23 seconds.
-- Epoch 12
Norm: 23.68, NNZs: 27208, Bias: -1.001059, T: 90510, Avg. loss: 0.019964
Total training time: 0.27 seconds.
Norm: 25.32, NNZs: 38038, Bias: -1.023050, T: 108612, Avg. loss: 0.030871
Total training time: 0.28 seconds.
Convergence after 10 epochs took 0.27 seconds
-- Epoch 13
Norm: 23.48, NNZs: 37668, Bias: -1.054319, T: 99561, Avg. loss: 0.023196
Total training time: 0.21 seconds.
-- Epoch 12
Norm: 23.04, NNZs: 26323, Bias: -1.096528, T: 99572, Avg. loss: 0.024583
Total training time: 0.16 seconds.
-- Epoch 12
Norm: 23.79, NNZs: 28254, Bias: -1.004287, T: 90520, Avg. loss: 0.018426
Total training time: 0.28 seconds.
Norm: 22.93, NNZs: 25095, Bias: -1.107368, T: 99561, Avg. loss: 0.024582
Total training time: 0.17 seconds.
-- Epoch 11
-- Epoch 12
Norm: 19.53, NNZs: 21453, Bias: -1.071956, T: 90520, Avg. loss: 0.030893
Total training time: 0.17 seconds.
Norm: 24.13, NNZs: 24115, Bias: -1.080826, T: 108624, Avg. loss: 0.025240
Total training time: 0.24 seconds.
-- Epoch 11
-- Epoch 13
Norm: 23.51, NNZs: 37814, Bias: -1.044420, T: 108612, Avg. loss: 0.022889
Total training time: 0.22 seconds.
Norm: 25.34, NNZs: 40835, Bias: -1.018483, T: 117663, Avg. loss: 0.030424
Total training time: 0.28 seconds.
Convergence after 12 epochs took 0.22 seconds
Convergence after 13 epochs took 0.28 seconds
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.8s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.8s finished
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.8s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.8s finished
[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.8s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.8s finished
Norm: 23.05, NNZs: 26469, Bias: -1.093585, T: 108624, Avg. loss: 0.024228
Total training time: 0.17 seconds.
Convergence after 12 epochs took 0.17 seconds
Norm: 22.90, NNZs: 25273, Bias: -1.104742, T: 108612, Avg. loss: 0.024346
Total training time: 0.18 seconds.
Convergence after 12 epochs took 0.18 seconds
Norm: 23.71, NNZs: 28463, Bias: -1.004171, T: 99572, Avg. loss: 0.018187
Total training time: 0.28 seconds.
Convergence after 11 epochs took 0.28 seconds
Norm: 24.18, NNZs: 24238, Bias: -1.076089, T: 117676, Avg. loss: 0.025057
Total training time: 0.24 seconds.
Norm: 19.52, NNZs: 21820, Bias: -1.067028, T: 99572, Avg. loss: 0.030574
Total training time: 0.17 seconds.
Convergence after 13 epochs took 0.24 seconds
Convergence after 11 epochs took 0.17 seconds
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.1s finished
	accuracy: 5-fold cross validation: [0.77198409 0.7512152  0.76182059 0.77728679 0.75198939]
	test accuracy: 5-fold cross validation accuracy: 0.76 (+/- 0.02)
dimensionality: 101322
density: 0.318741



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.701141795007966
	accuracy score (normalize=False):  5281

compute the precision
	precision score (average=macro):  0.6968184359215412
	precision score (average=micro):  0.701141795007966
	precision score (average=weighted):  0.704144502134341
	precision score (average=None):  [0.5513308  0.69007264 0.64341085 0.67021277 0.73387097 0.81586402
 0.76283619 0.77297297 0.82417582 0.54126984 0.85714286 0.83880597
 0.61891892 0.76699029 0.73891626 0.63976378 0.59090909 0.83381089
 0.57539683 0.46969697]
	precision score (average=None, zero_division=1):  [0.5513308  0.69007264 0.64341085 0.67021277 0.73387097 0.81586402
 0.76283619 0.77297297 0.82417582 0.54126984 0.85714286 0.83880597
 0.61891892 0.76699029 0.73891626 0.63976378 0.59090909 0.83381089
 0.57539683 0.46969697]

compute the precision
	recall score (average=macro):  0.6878962398677808
	recall score (average=micro):  0.701141795007966
	recall score (average=weighted):  0.701141795007966
	recall score (average=None):  [0.45454545 0.73264781 0.6319797  0.64285714 0.70909091 0.72911392
 0.8        0.72222222 0.75376884 0.85894207 0.88721805 0.70959596
 0.5826972  0.7979798  0.76142132 0.81658291 0.67857143 0.77393617
 0.46774194 0.24701195]
	recall score (average=None, zero_division=1):  [0.45454545 0.73264781 0.6319797  0.64285714 0.70909091 0.72911392
 0.8        0.72222222 0.75376884 0.85894207 0.88721805 0.70959596
 0.5826972  0.7979798  0.76142132 0.81658291 0.67857143 0.77393617
 0.46774194 0.24701195]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.6869130863745768
	f1 score (average=micro):  0.701141795007966
	f1 score (average=weighted):  0.6977051333499774
	f1 score (average=None):  [0.49828179 0.71072319 0.63764405 0.65625    0.72126816 0.77005348
 0.78097622 0.74673629 0.78740157 0.66407011 0.87192118 0.76880985
 0.60026212 0.78217822 0.75       0.71743929 0.63171355 0.80275862
 0.51601423 0.32375979]

compute the F-beta score
	f beta score (average=macro):  0.6914261973402177
	f beta score (average=micro):  0.7011417950079659
	f beta score (average=weighted):  0.700356207846992
	f beta score (average=None):  [0.52881109 0.69818716 0.64109166 0.66455696 0.72877736 0.79690094
 0.76999013 0.76226013 0.80906149 0.58450463 0.86299366 0.8093318
 0.61131874 0.77299413 0.74331021 0.66872428 0.60658153 0.82110609
 0.55007587 0.39794608]

compute the average Hamming loss
	hamming loss:  0.29885820499203397

jaccard similarity coefficient score
	jaccard score (average=macro):  0.5356733877288301
	jaccard score (average=None):  [0.33180778 0.55125725 0.46804511 0.48837209 0.56404959 0.62608696
 0.64065708 0.59583333 0.64935065 0.49708455 0.77292576 0.62444444
 0.42883895 0.64227642 0.6        0.55938038 0.46168224 0.67050691
 0.34772182 0.19314642]


Accuracy score for the TWENTY_NEWS_GROUPS dataset (removing headers signatures and quoting): Final classification report: 
1) ADA_BOOST_CLASSIFIER
		Accuracy score = 0.36537440254912373		Training time = 4.748219966888428		Test time = 0.2506265640258789		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

2) BERNOULLI_NB
		Accuracy score = 0.4584439723844928		Training time = 0.06657028198242188		Test time = 0.05463433265686035		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

3) COMPLEMENT_NB
		Accuracy score = 0.7133563462559745		Training time = 0.06121230125427246		Test time = 0.010654211044311523		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

4) DECISION_TREE_CLASSIFIER
		Accuracy score = 0.4391927774827403		Training time = 10.729600667953491		Test time = 0.00562596321105957		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

5) EXTRA_TREE_CLASSIFIER
		Accuracy score = 0.2942113648433351		Training time = 0.5345175266265869		Test time = 0.008513450622558594		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

6) EXTRA_TREES_CLASSIFIER
		Accuracy score = 0.6530801911842804		Training time = 10.354766845703125		Test time = 0.20385122299194336		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

7) GRADIENT_BOOSTING_CLASSIFIER
		Accuracy score = 0.5967870419543282		Training time = 342.2281596660614		Test time = 0.18443989753723145		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

8) K_NEIGHBORS_CLASSIFIER
		Accuracy score = 0.07010090281465746		Training time = 0.0036284923553466797		Test time = 1.6711516380310059		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

9) LINEAR_SVC
		Accuracy score = 0.69676048858205		Training time = 0.7999742031097412		Test time = 0.010244131088256836		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

10) LOGISTIC_REGRESSION
		Accuracy score = 0.6946362187997875		Training time = 17.27361035346985		Test time = 0.011645078659057617		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

11) LOGISTIC_REGRESSION_CV
		Accuracy score = 0.6935740839086564		Training time = 412.4349091053009		Test time = 0.011150121688842773		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

12) MLP_CLASSIFIER
		Accuracy score = 0.699814126394052		Training time = 1377.6278533935547		Test time = 0.04182124137878418		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

13) MULTINOMIAL_NB
		Accuracy score = 0.6712692511949018		Training time = 0.04485201835632324		Test time = 0.012325525283813477		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

14) NEAREST_CENTROID
		Accuracy score = 0.6427243759957515		Training time = 0.021462202072143555		Test time = 0.01430368423461914		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

15) NU_SVC
		Accuracy score = 0.6919808815719597		Training time = 82.94072079658508		Test time = 27.46919894218445		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

16) PASSIVE_AGGRESSIVE_CLASSIFIER
		Accuracy score = 0.6848114710568242		Training time = 0.40996646881103516		Test time = 0.011363506317138672		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

17) PERCEPTRON
		Accuracy score = 0.6336962294211365		Training time = 0.41254210472106934		Test time = 0.01150059700012207		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

18) RANDOM_FOREST_CLASSIFIER
		Accuracy score = 0.6267923526287839		Training time = 6.548064708709717		Test time = 0.20558428764343262		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

19) RIDGE_CLASSIFIER
		Accuracy score = 0.7035315985130112		Training time = 2.3770463466644287		Test time = 0.02271580696105957		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

20) RIDGE_CLASSIFIERCV
		Accuracy score = 0.7036643653744026		Training time = 172.96024584770203		Test time = 0.018374204635620117		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']

21) SGD_CLASSIFIER
		Accuracy score = 0.701141795007966		Training time = 0.5144875049591064		Test time = 0.011795997619628906		Cross validation scores = [array([0.39726027, 0.39460893, 0.40167919, 0.39151569, 0.41246684]), array([0.48033584, 0.46133451, 0.48563853, 0.47547503, 0.48938992]), array([0.77419355, 0.76623951, 0.78258948, 0.77596111, 0.76259947]), array([0.49270879, 0.46663721, 0.46840477, 0.48696421, 0.49071618]), array([0.32213875, 0.32346443, 0.35616438, 0.37295625, 0.3377542 ]), array([0.7003977 , 0.68581529, 0.69907203, 0.70967742, 0.69893899]), array([0.65002209, 0.61157755, 0.63985859, 0.65002209, 0.64721485]), array([0.08351745, 0.08749448, 0.09058772, 0.08042422, 0.0795756 ]), array([0.76933274, 0.74900574, 0.75916924, 0.77551922, 0.75066313]), array([0.75165709, 0.73840035, 0.74281927, 0.75165709, 0.7250221 ]), array([0.76182059, 0.74237738, 0.75165709, 0.76579761, 0.74093722]), array([0.77507733, 0.75828546, 0.77021653, 0.78391516, 0.75066313]), array([0.72735307, 0.71453822, 0.71321255, 0.71851525, 0.7020336 ]), array([0.69686257, 0.68625718, 0.68758285, 0.69863014, 0.69319187]), array([0.76226248, 0.73751657, 0.74856385, 0.75872735, 0.7351901 ]), array([0.76093681, 0.7410517 , 0.74326116, 0.76226248, 0.73784262]), array([0.70702607, 0.69465312, 0.70172338, 0.71851525, 0.69982317]), array([0.68581529, 0.66372072, 0.66813964, 0.68183827, 0.66445623]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77551922, 0.7565179 , 0.76756518, 0.78479894, 0.75773652]), array([0.77198409, 0.7512152 , 0.76182059, 0.77728679, 0.75198939])]		Average accuracy score (Cross Validation) = ['0.40 (+/- 0.01)', '0.48 (+/- 0.02)', '0.77 (+/- 0.01)', '0.48 (+/- 0.02)', '0.34 (+/- 0.04)', '0.70 (+/- 0.02)', '0.64 (+/- 0.03)', '0.08 (+/- 0.01)', '0.76 (+/- 0.02)', '0.74 (+/- 0.02)', '0.75 (+/- 0.02)', '0.77 (+/- 0.02)', '0.72 (+/- 0.02)', '0.69 (+/- 0.01)', '0.75 (+/- 0.02)', '0.75 (+/- 0.02)', '0.70 (+/- 0.02)', '0.67 (+/- 0.02)', '0.77 (+/- 0.02)', '0.77 (+/- 0.02)', '0.76 (+/- 0.02)']



Best algorithm:
===> 3) COMPLEMENT_NB
		Accuracy score = 0.7133563462559745		Training time = 0.06121230125427246		Test time = 0.010654211044311523

Loading IMDB_REVIEWS dataset:

===> Reading files from /home/rpessoa/github/comp551-2020-p2_classification_of_textual_data/code/datasets/imdb_reviews/aclImdb/train/neg

===> Reading files from /home/rpessoa/github/comp551-2020-p2_classification_of_textual_data/code/datasets/imdb_reviews/aclImdb/train/pos

===> Reading files from /home/rpessoa/github/comp551-2020-p2_classification_of_textual_data/code/datasets/imdb_reviews/aclImdb/test/neg

===> Reading files from /home/rpessoa/github/comp551-2020-p2_classification_of_textual_data/code/datasets/imdb_reviews/aclImdb/test/pos
data loaded
25000 documents - 33.133MB (training set)
25000 documents - 32.351MB (test set)

Extracting features from the training data using a vectorizer
done in 2.395850s at 13.829MB/s
n_samples: 25000, n_features: 74535

Extracting features from the test data using the same vectorizer
done in 2.329922s at 13.885MB/s
n_samples: 25000, n_features: 74535

================================================================================
Classifier.ADA_BOOST_CLASSIFIER
________________________________________________________________________________
Training: 
AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=0)
train time: 11.363s
test time:  0.709s
accuracy:   0.359


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
	accuracy: 5-fold cross validation: [0.3464 0.3472 0.3528 0.3464 0.3606]
	test accuracy: 5-fold cross validation accuracy: 0.35 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.3586
	accuracy score (normalize=False):  8965

compute the precision
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   30.5s finished
	precision score (average=macro):  0.26236863031825464
	precision score (average=micro):  0.3586
	precision score (average=weighted):  0.291892877716099
	precision score (average=None):  [0.44809524 0.1943662  0.24293785 0.22022989 0.21127596 0.21587031
 0.2        0.3661736 ]
	precision score (average=None, zero_division=1):  [0.44809524 0.1943662  0.24293785 0.22022989 0.21127596 0.21587031
 0.2        0.3661736 ]

compute the precision
	recall score (average=macro):  0.25436659720293275
	recall score (average=micro):  0.3586
	recall score (average=weighted):  0.3586
	recall score (average=None):  [0.74950219 0.02997394 0.03384494 0.18178368 0.15431296 0.08877193
 0.00938567 0.78735747]
	recall score (average=None, zero_division=1):  [0.74950219 0.02997394 0.03384494 0.18178368 0.15431296 0.08877193
 0.00938567 0.78735747]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.21166966854701233
	f1 score (average=micro):  0.3586
	f1 score (average=weighted):  0.2769177384673768
	f1 score (average=None):  [0.56087021 0.05193828 0.05941278 0.1991684  0.17835671 0.12580806
 0.01792991 0.499873  ]

compute the F-beta score
	f beta score (average=macro):  0.21425758592260394
	f beta score (average=micro):  0.3586
	f beta score (average=weighted):  0.2627204938831104
	f beta score (average=None):  [0.48728704 0.0926921  0.10866818 0.21129246 0.1967503  0.1678164
 0.03951149 0.41004271]

compute the average Hamming loss
	hamming loss:  0.6414

jaccard similarity coefficient score
	jaccard score (average=macro):  0.13311337312415
	jaccard score (average=None):  [0.38972872 0.02666151 0.03061588 0.11059801 0.09790979 0.06712656
 0.00904605 0.33322045]


================================================================================
Classifier.BERNOULLI_NB
________________________________________________________________________________
Training: 
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
train time: 0.039s
test time:  0.039s
accuracy:   0.371


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished
	accuracy: 5-fold cross validation: [0.3788 0.376  0.3768 0.374  0.3704]
	test accuracy: 5-fold cross validation accuracy: 0.38 (+/- 0.01)
dimensionality: 74535
density: 1.000000



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.37132
	accuracy score (normalize=False):  9283

compute the precision
	precision score (average=macro):  0.29239624288932897
	precision score (average=micro):  0.37132
	precision score (average=weighted):  0.31421654089743284
	precision score (average=None):  [0.39137423 0.26415094 0.23539232 0.27796383 0.24869383 0.25090689
 0.26       0.41068789]
	precision score (average=None, zero_division=1):  [0.39137423 0.26415094 0.23539232 0.27796383 0.24869383 0.25090689
 0.26       0.41068789]

compute the precision
	recall score (average=macro):  0.2613599934409504
	recall score (average=micro):  0.37132
	recall score (average=weighted):  0.37132
	recall score (average=None):  [0.87275986 0.00608167 0.05548996 0.15749526 0.10316428 0.14561404
 0.02773038 0.72254451]
	recall score (average=None, zero_division=1):  [0.87275986 0.00608167 0.05548996 0.15749526 0.10316428 0.14561404
 0.02773038 0.72254451]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.21838882094170003
	f1 score (average=micro):  0.37132000000000004
	f1 score (average=weighted):  0.2838575744099875
	f1 score (average=None):  [0.54041058 0.0118896  0.08980892 0.20106589 0.14583333 0.18428064
 0.05011565 0.52370596]

compute the F-beta score
	f beta score (average=macro):  0.22643494576046563
	f beta score (average=micro):  0.37132
	f beta score (average=weighted):  0.2727365590140134
	f beta score (average=None):  [0.43990124 0.02784407 0.14279927 0.24108284 0.19396903 0.21920558
 0.097189   0.44948854]

compute the average Hamming loss
	hamming loss:  0.62868

jaccard similarity coefficient score
	jaccard score (average=macro):  0.1369503562505267
	jaccard score (average=None):  [0.37024835 0.00598035 0.04701567 0.11176946 0.07865169 0.10149181
 0.02570186 0.35474367]


================================================================================
Classifier.COMPLEMENT_NB
________________________________________________________________________________
Training: 
ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)
train time: 0.032s
test time:  0.017s
accuracy:   0.373


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
	accuracy: 5-fold cross validation: [0.3906 0.3834 0.389  0.3826 0.39  ]
	test accuracy: 5-fold cross validation accuracy: 0.39 (+/- 0.01)
dimensionality: 74535
density: 1.000000



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.37312
	accuracy score (normalize=False):  9328

compute the precision
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished
	precision score (average=macro):  0.27363326505219393
	precision score (average=micro):  0.37312
	precision score (average=weighted):  0.2994660530993966
	precision score (average=None):  [0.36629266 0.16666667 0.2388664  0.32254197 0.29855538 0.25352113
 0.11949686 0.42312507]
	precision score (average=None, zero_division=1):  [0.36629266 0.16666667 0.2388664  0.32254197 0.29855538 0.25352113
 0.11949686 0.42312507]

compute the precision
	recall score (average=macro):  0.25339159255883764
	recall score (average=micro):  0.37312
	recall score (average=weighted):  0.37312
	recall score (average=None):  [0.93608124 0.00695048 0.02321921 0.10208729 0.08062419 0.12631579
 0.0081058  0.74374875]
	recall score (average=None, zero_division=1):  [0.93608124 0.00695048 0.02321921 0.10208729 0.08062419 0.12631579
 0.0081058  0.74374875]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.19843157648587142
	f1 score (average=micro):  0.37312
	f1 score (average=weighted):  0.2678673639718913
	f1 score (average=None):  [0.5265457  0.01334445 0.04232425 0.15508792 0.12696246 0.16861827
 0.01518178 0.53938778]

compute the F-beta score
	f beta score (average=macro):  0.20692953092757968
	f beta score (average=micro):  0.37312
	f beta score (average=weighted):  0.2562804137558405
	f beta score (average=None):  [0.4170659  0.02978407 0.08359309 0.2252554  0.19379037 0.21101993
 0.03187919 0.4630483 ]

compute the average Hamming loss
	hamming loss:  0.62688

jaccard similarity coefficient score
	jaccard score (average=macro):  0.12581843255504344
	jaccard score (average=None):  [0.35735462 0.00671704 0.02161964 0.0840625  0.06778426 0.09207161
 0.00764895 0.36928884]


================================================================================
Classifier.DECISION_TREE_CLASSIFIER
________________________________________________________________________________
Training: 
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=0, splitter='best')
train time: 35.037s
test time:  0.014s
accuracy:   0.258


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   29.9s finished
	accuracy: 5-fold cross validation: [0.262  0.255  0.251  0.2444 0.2582]
	test accuracy: 5-fold cross validation accuracy: 0.25 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.25764
	accuracy score (normalize=False):  6441

compute the precision
	precision score (average=macro):  0.21362304964357112
	precision score (average=micro):  0.25764
	precision score (average=weighted):  0.25103584141748414
	precision score (average=None):  [0.42689946 0.13079019 0.15378044 0.15636219 0.1443299  0.18577982
 0.14651039 0.36453202]
	precision score (average=None, zero_division=1):  [0.42689946 0.13079019 0.15378044 0.15636219 0.1443299  0.18577982
 0.14651039 0.36453202]

compute the precision
	recall score (average=macro):  0.21502426300409405
	recall score (average=micro):  0.25764
	recall score (average=weighted):  0.25764
	recall score (average=None):  [0.44305058 0.1251086  0.14167651 0.1487666  0.14564369 0.19894737
 0.11732082 0.39967994]
	recall score (average=None, zero_division=1):  [0.44305058 0.1251086  0.14167651 0.1487666  0.14564369 0.19894737
 0.11732082 0.39967994]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.21392280955326504
	f1 score (average=micro):  0.25764
	f1 score (average=weighted):  0.25392785078483454
	f1 score (average=None):  [0.43482509 0.12788632 0.14748054 0.15246986 0.14498382 0.19213826
 0.13030088 0.38129771]

compute the F-beta score
	f beta score (average=macro):  0.2136391148585169
	f beta score (average=micro):  0.25764
	f beta score (average=weighted):  0.25209010709994056
	f beta score (average=None):  [0.43003479 0.12961296 0.15119698 0.15478165 0.14459076 0.18827201
 0.13956557 0.3710582 ]

compute the average Hamming loss
	hamming loss:  0.74236

jaccard similarity coefficient score
	jaccard score (average=macro):  0.12474328004630376
	jaccard score (average=None):  [0.27781246 0.0683112  0.07961079 0.08252632 0.07815771 0.10627929
 0.06969083 0.23555765]


================================================================================
Classifier.EXTRA_TREE_CLASSIFIER
________________________________________________________________________________
Training: 
ExtraTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                    max_depth=None, max_features='auto', max_leaf_nodes=None,
                    min_impurity_decrease=0.0, min_impurity_split=None,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, random_state=0,
                    splitter='random')
train time: 0.989s
test time:  0.019s
accuracy:   0.221


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.4s finished
	accuracy: 5-fold cross validation: [0.215  0.2322 0.2226 0.2232 0.2272]
	test accuracy: 5-fold cross validation accuracy: 0.22 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.2212
	accuracy score (normalize=False):  5530

compute the precision
	precision score (average=macro):  0.17549725031599872
	precision score (average=micro):  0.2212
	precision score (average=weighted):  0.20300511556018203
	precision score (average=None):  [0.32314205 0.11398091 0.13983287 0.13593883 0.12685422 0.14784547
 0.12018977 0.29619388]
	precision score (average=None, zero_division=1):  [0.32314205 0.11398091 0.13983287 0.13593883 0.12685422 0.14784547
 0.12018977 0.29619388]

compute the precision
	recall score (average=macro):  0.17843638803710815
	recall score (average=micro):  0.2212
	recall score (average=weighted):  0.2212
	recall score (average=None):  [0.41039427 0.08818419 0.09878001 0.12144213 0.10749892 0.13964912
 0.09726962 0.36427285]
	recall score (average=None, zero_division=1):  [0.41039427 0.08818419 0.09878001 0.12144213 0.10749892 0.13964912
 0.09726962 0.36427285]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.17491587510862083
	f1 score (average=micro):  0.2212
	f1 score (average=weighted):  0.20960469716151434
	f1 score (average=None):  [0.36157895 0.09943669 0.11577491 0.12828222 0.11637729 0.14363046
 0.10752181 0.32672468]

compute the F-beta score
	f beta score (average=macro):  0.17476187607648197
	f beta score (average=micro):  0.2212
	f beta score (average=weighted):  0.2050731043008619
	f beta score (average=None):  [0.33749263 0.10768088 0.12910194 0.13276906 0.12244495 0.14613012
 0.11478051 0.30769491]

compute the average Hamming loss
	hamming loss:  0.7788

jaccard similarity coefficient score
	jaccard score (average=macro):  0.09927748248082374
	jaccard score (average=None):  [0.22068744 0.05231959 0.06144431 0.06853716 0.06178376 0.0773717
 0.05681535 0.19526056]


================================================================================
Classifier.EXTRA_TREES_CLASSIFIER
________________________________________________________________________________
Training: 
ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_impurity_split=None,
                     min_samples_leaf=1, min_samples_split=2,
                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=0, verbose=True,
                     warm_start=False)
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.4s
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   16.2s finished
train time: 16.379s
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.2s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.5s finished
test time:  0.519s
accuracy:   0.374


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   32.3s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   34.3s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   34.5s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   34.5s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   34.8s
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.2s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.4s finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.2s finished
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.3s finished
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.2s finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished
	accuracy: 5-fold cross validation: [0.3702 0.371  0.3744 0.3726 0.3702]
	test accuracy: 5-fold cross validation accuracy: 0.37 (+/- 0.00)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.37404
	accuracy score (normalize=False):  9351

compute the precision
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.6min finished
	precision score (average=macro):  0.38636094656301334
	precision score (average=micro):  0.37404
	precision score (average=weighted):  0.38207912501960956
	precision score (average=None):  [0.38020748 0.51428571 0.42982456 0.38066465 0.28740157 0.25
 0.47222222 0.37628137]
	precision score (average=None, zero_division=1):  [0.38020748 0.51428571 0.42982456 0.38066465 0.28740157 0.25
 0.47222222 0.37628137]

compute the precision
	recall score (average=macro):  0.24406362431761874
	recall score (average=micro):  0.37404
	recall score (average=weighted):  0.37404
	recall score (average=None):  [0.91955396 0.00781929 0.01928375 0.04781784 0.03164283 0.06736842
 0.00725256 0.85177035]
	recall score (average=None, zero_division=1):  [0.91955396 0.00781929 0.01928375 0.04781784 0.03164283 0.06736842
 0.00725256 0.85177035]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.1718325904899807
	f1 score (average=micro):  0.37404
	f1 score (average=weighted):  0.24526772505985317
	f1 score (average=None):  [0.53797763 0.01540436 0.03691149 0.08496291 0.05700898 0.10613599
 0.01428571 0.52197364]

compute the F-beta score
	f beta score (average=macro):  0.17976922639599177
	f beta score (average=micro):  0.37404
	f beta score (average=weighted):  0.23151805800270625
	f beta score (average=None):  [0.43073537 0.03685504 0.08174842 0.15913109 0.10984051 0.1621074
 0.03416399 0.42357201]

compute the average Hamming loss
	hamming loss:  0.62596

jaccard similarity coefficient score
	jaccard score (average=macro):  0.11057900113905393
	jaccard score (average=None):  [0.36796813 0.00776197 0.01880276 0.0443662  0.02934084 0.05604203
 0.00719424 0.35315584]


================================================================================
Classifier.GRADIENT_BOOSTING_CLASSIFIER
________________________________________________________________________________
Training: 
GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_iter_no_change=None, presort='deprecated',
                           random_state=0, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=True,
                           warm_start=False)
      Iter       Train Loss   Remaining Time 
         1       49944.5758            6.84m
         2       49361.8139            6.75m
         3       48865.8490            6.68m
         4       48435.6162            6.59m
         5       48044.9568            6.52m
         6       47698.4252            6.45m
         7       47371.1355            6.38m
         8       47062.4232            6.31m
         9       46785.6489            6.24m
        10       46523.9006            6.16m
        20       44475.3602            5.46m
        30       43009.1486            4.77m
        40       41840.5535            4.08m
        50       40860.7617            3.39m
        60       40042.0764            2.71m
        70       39335.0087            2.03m
        80       38676.0345            1.35m
        90       38064.4440           40.55s
       100       37507.5335            0.00s
train time: 405.139s
test time:  0.293s
accuracy:   0.376


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
      Iter       Train Loss   Remaining Time 
      Iter       Train Loss   Remaining Time 
      Iter       Train Loss   Remaining Time 
      Iter       Train Loss   Remaining Time 
      Iter       Train Loss   Remaining Time 
         1       39939.0701           11.03m
         1       39927.6991           11.04m
         1       39928.7184           11.05m
         1       39941.3796           11.17m
         1       39949.5431           11.16m
         2       39464.4917           10.89m
         2       39437.3473           10.94m
         2       39449.5450           10.93m
         2       39482.5885           11.04m
         2       39456.4653           11.08m
         3       39055.2295           10.75m
         3       39021.7540           10.83m
         3       39035.0815           10.82m
         3       39080.2137           10.92m
         3       39047.2763           10.94m
         4       38684.8988           10.63m
         4       38660.1859           10.70m
         4       38680.5127           10.70m
         4       38722.5013           10.79m
         4       38688.0921           10.82m
         5       38357.4285           10.52m
         5       38336.8051           10.58m
         5       38352.2150           10.58m
         5       38398.9340           10.66m
         5       38355.8198           10.69m
         6       38047.6526           10.41m
         6       38037.7698           10.46m
         6       38048.1428           10.46m
         6       38107.2178           10.53m
         6       38058.7013           10.56m
         7       37781.9079           10.31m
         7       37764.6862           10.35m
         7       37787.9851           10.35m
         7       37846.7135           10.41m
         7       37779.2781           10.44m
         8       37523.4289           10.21m
         8       37502.2682           10.25m
         8       37534.2826           10.25m
         8       37597.7592           10.30m
         8       37528.4870           10.34m
         9       37278.8755           10.14m
         9       37268.7665           10.18m
         9       37305.7834           10.18m
         9       37362.3636           10.21m
         9       37282.0673           10.27m
        10       37062.6342           10.06m
        10       37048.3256           10.09m
        10       37080.0874           10.09m
        10       37137.1151           10.12m
        10       37073.8546           10.18m
        20       35349.3025            9.02m
        20       35327.0433            9.03m
        20       35348.0216            9.05m
        20       35380.7300            9.05m
        20       35356.9971            9.12m
        30       34080.8264            7.93m
        30       34083.4427            7.94m
        30       34148.3892            7.94m
        30       34113.2803            7.94m
        30       34122.5981            8.00m
        40       33108.3090            6.79m
        40       33099.2568            6.81m
        40       33182.8370            6.82m
        40       33137.4897            6.82m
        40       33162.9012            6.85m
        50       32312.9446            5.66m
        50       32302.3162            5.67m
        50       32387.2780            5.68m
        50       32341.3011            5.68m
        50       32373.6472            5.70m
        60       31621.9771            4.53m
        60       31638.1098            4.53m
        60       31649.1900            4.54m
        60       31708.6823            4.54m
        60       31670.3830            4.56m
        70       31027.8082            3.39m
        70       30982.8828            3.40m
        70       31029.7398            3.40m
        70       31073.5297            3.41m
        70       31063.6705            3.41m
        80       30466.0803            2.26m
        80       30414.2480            2.27m
        80       30449.9559            2.27m
        80       30509.8944            2.27m
        80       30475.2617            2.28m
        90       29987.6499            1.13m
        90       29886.4041            1.13m
        90       29914.2046            1.13m
        90       29993.2928            1.13m
        90       29948.5821            1.14m
       100       29493.0197            0.00s
       100       29406.5191            0.00s
       100       29446.6238            0.00s
       100       29490.4150            0.00s
       100       29444.7556            0.00s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 11.4min finished
	accuracy: 5-fold cross validation: [0.3796 0.3688 0.3648 0.364  0.3734]
	test accuracy: 5-fold cross validation accuracy: 0.37 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.37624
	accuracy score (normalize=False):  9406

compute the precision
	precision score (average=macro):  0.282985916508967
	precision score (average=micro):  0.37624
	precision score (average=weighted):  0.313060777475666
	precision score (average=None):  [0.47294764 0.21113244 0.24586288 0.27602617 0.2742681  0.23303697
 0.16426513 0.38634799]
	precision score (average=None, zero_division=1):  [0.47294764 0.21113244 0.24586288 0.27602617 0.2742681  0.23303697
 0.16426513 0.38634799]

compute the precision
	recall score (average=macro):  0.274817663091955
	recall score (average=micro):  0.37624
	recall score (average=weighted):  0.37624
	recall score (average=None):  [0.75368379 0.04778454 0.08185754 0.17609108 0.15431296 0.17473684
 0.02431741 0.78575715]
	recall score (average=None, zero_division=1):  [0.75368379 0.04778454 0.08185754 0.17609108 0.15431296 0.17473684
 0.02431741 0.78575715]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.2443180898738927
	f1 score (average=micro):  0.37624
	f1 score (average=weighted):  0.3076165636498346
	f1 score (average=None):  [0.58119002 0.07793128 0.12282256 0.2150139  0.19750347 0.19971927
 0.04236343 0.51800079]

compute the F-beta score
	f beta score (average=macro):  0.2527617289987046
	f beta score (average=micro):  0.37624
	f beta score (average=weighted):  0.29813330675636096
	f beta score (average=None):  [0.5110169  0.125399   0.17552743 0.24788973 0.23736498 0.21845938
 0.07636656 0.43006985]

compute the average Hamming loss
	hamming loss:  0.62376

jaccard similarity coefficient score
	jaccard score (average=macro):  0.15346779288549714
	jaccard score (average=None):  [0.40963203 0.04054552 0.06542938 0.12045691 0.10957218 0.11093785
 0.02164009 0.34952839]


================================================================================
Classifier.K_NEIGHBORS_CLASSIFIER
________________________________________________________________________________
Training: 
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
train time: 0.006s
test time:  13.394s
accuracy:   0.264


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
	accuracy: 5-fold cross validation: [0.3044 0.3134 0.3262 0.319  0.3132]
	test accuracy: 5-fold cross validation accuracy: 0.32 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.26352
	accuracy score (normalize=False):  6588

compute the precision
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    6.0s finished
	precision score (average=macro):  0.19609270681427454
	precision score (average=micro):  0.26352
	precision score (average=weighted):  0.22680414279511307
	precision score (average=None):  [0.33583881 0.12083333 0.1270183  0.15963203 0.15362731 0.18643133
 0.13185913 0.3535014 ]
	precision score (average=None, zero_division=1):  [0.33583881 0.12083333 0.1270183  0.15963203 0.15362731 0.18643133
 0.13185913 0.3535014 ]

compute the precision
	recall score (average=macro):  0.20072262142430777
	recall score (average=micro):  0.26352
	recall score (average=weighted):  0.26352
	recall score (average=None):  [0.64058144 0.10078193 0.09287682 0.11195446 0.09362809 0.11859649
 0.06868601 0.37867574]
	recall score (average=None, zero_division=1):  [0.64058144 0.10078193 0.09287682 0.11195446 0.09362809 0.11859649
 0.06868601 0.37867574]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.18834475786044502
	f1 score (average=micro):  0.26352
	f1 score (average=weighted):  0.23226402514824934
	f1 score (average=None):  [0.44065475 0.10990052 0.10729711 0.1316083  0.11634797 0.14497105
 0.09032258 0.36565579]

compute the F-beta score
	f beta score (average=macro):  0.19073615435621155
	f beta score (average=micro):  0.26352
	f beta score (average=weighted):  0.22650683094900975
	f beta score (average=None):  [0.37115234 0.11620918 0.11831946 0.14710282 0.13617451 0.16729361
 0.11137244 0.35826489]

compute the average Hamming loss
	hamming loss:  0.73648

jaccard similarity coefficient score
	jaccard score (average=macro):  0.10985142939702205
	jaccard score (average=None):  [0.2825896  0.05814536 0.05668989 0.07043935 0.06176723 0.07815029
 0.0472973  0.22373242]


================================================================================
Classifier.LINEAR_SVC
________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
          verbose=True)
....*
optimization finished, #iter = 41
Objective value = -4835.989524
nSV = 13807
...*.
optimization finished, #iter = 40
Objective value = -3863.811590
nSV = 13560
...*.
optimization finished, #iter = 40
Objective value = -4034.135653
nSV = 14041
...*
optimization finished, #iter = 39
Objective value = -4215.146690
nSV = 14325
...*
optimization finished, #iter = 39
Objective value = -3959.934463
nSV = 13434
....*
optimization finished, #iter = 41
Objective value = -4767.545340
nSV = 14563
...*.
optimization finished, #iter = 40
Objective value = -3808.434762
nSV = 13173
...*.
optimization finished, #iter = 40
Objective value = -5277.013048
nSV = 14543
[LibLinear]train time: 1.786s
test time:  0.018s
accuracy:   0.373


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
............*
optimization finished, #iter = 38
Objective value = -3792.325058
nSV = 11382
.*.
optimization finished, #iter = 39
Objective value = -3770.832771
nSV = 11424
.*
optimization finished, #iter = 39
Objective value = -3776.533137
nSV = 11356
**
optimization finished, #iter = 38
Objective value = -3766.501158
nSV = 11290

optimization finished, #iter = 39
Objective value = -3777.626494
nSV = 11348
............*
optimization finished, #iter = 39
Objective value = -2987.958233
nSV = 11062
.*..
optimization finished, #iter = 39
Objective value = -2986.767764
nSV = 11055
*
optimization finished, #iter = 39
Objective value = -2992.393646
nSV = 11183
*
optimization finished, #iter = 37
Objective value = -2949.386925
nSV = 11024
*
optimization finished, #iter = 38
Objective value = -2979.473863
nSV = 11109
............*.
optimization finished, #iter = 40
Objective value = -3119.622591
nSV = 11500
...*
optimization finished, #iter = 39
Objective value = -3126.466382
nSV = 11510
*.
optimization finished, #iter = 40
Objective value = -3107.110165
nSV = 11485
**
optimization finished, #iter = 39
Objective value = -3103.471611
nSV = 11450
.
optimization finished, #iter = 40
Objective value = -3127.889088
nSV = 11513
............*
optimization finished, #iter = 39
Objective value = -3286.817671
nSV = 11851
..*.
optimization finished, #iter = 37
Objective value = -3253.141516
nSV = 11785
*.
optimization finished, #iter = 40
Objective value = -3271.902945
nSV = 11744
*.*.
optimization finished, #iter = 40
Objective value = -3276.997945
nSV = 11705
.
optimization finished, #iter = 41
Objective value = -3272.794444
nSV = 11732
..........*.
optimization finished, #iter = 38
Objective value = -3040.666340
nSV = 10998
.*.
optimization finished, #iter = 39
Objective value = -3064.685080
nSV = 11115
.*
optimization finished, #iter = 38
Objective value = -3072.822735
nSV = 11049
**.
optimization finished, #iter = 38
Objective value = -3044.869894
nSV = 10998

optimization finished, #iter = 37
Objective value = -3070.770111
nSV = 11060
..........*.
optimization finished, #iter = 38
Objective value = -3673.739175
nSV = 11904
.*..
optimization finished, #iter = 39
Objective value = -3692.503809
nSV = 11869
*.*.
optimization finished, #iter = 40
Objective value = -3689.582903
nSV = 11909

optimization finished, #iter = 39
Objective value = -3677.812390
nSV = 11923
*.
optimization finished, #iter = 40
Objective value = -3669.367194
nSV = 11854
........*
optimization finished, #iter = 39
Objective value = -2937.027712
nSV = 10674
...*.
optimization finished, #iter = 38
Objective value = -2933.011754
nSV = 10715
..*
optimization finished, #iter = 37
Objective value = -2935.036437
nSV = 10703
.*
optimization finished, #iter = 39
Objective value = -2946.668278
nSV = 10873
*
optimization finished, #iter = 39
Objective value = -2918.909117
nSV = 10718
........*
optimization finished, #iter = 38
Objective value = -4111.727694
nSV = 11922
....*
optimization finished, #iter = 39
Objective value = -4120.307383
nSV = 11936
..*
optimization finished, #iter = 39
Objective value = -4126.721992
nSV = 11953
*
optimization finished, #iter = 38
Objective value = -4079.636650
nSV = 11899
*
optimization finished, #iter = 39
Objective value = -4098.976580
nSV = 11902
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.6s finished
	accuracy: 5-fold cross validation: [0.3972 0.4006 0.3916 0.395  0.3878]
	test accuracy: 5-fold cross validation accuracy: 0.39 (+/- 0.01)
dimensionality: 74535
density: 0.846716



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.37328
	accuracy score (normalize=False):  9332

compute the precision
	precision score (average=macro):  0.2919042639572227
	precision score (average=micro):  0.37328
	precision score (average=weighted):  0.33707372107030964
	precision score (average=None):  [0.5340259  0.170347   0.20914346 0.26468345 0.24261275 0.23097826
 0.19230769 0.4911356 ]
	precision score (average=None, zero_division=1):  [0.5340259  0.170347   0.20914346 0.26468345 0.24261275 0.23097826
 0.19230769 0.4911356 ]

compute the precision
	recall score (average=macro):  0.2989916659389612
	recall score (average=micro):  0.37328
	recall score (average=weighted):  0.37328
	recall score (average=None):  [0.70629231 0.11728931 0.15663125 0.26337761 0.20286086 0.20877193
 0.12158703 0.61512302]
	recall score (average=None, zero_division=1):  [0.70629231 0.11728931 0.15663125 0.26337761 0.20286086 0.20877193
 0.12158703 0.61512302]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.29071337763868943
	f1 score (average=micro):  0.37327999999999995
	f1 score (average=weighted):  0.349575953906067
	f1 score (average=None):  [0.60819616 0.13892462 0.17911791 0.26402891 0.22096317 0.21931441
 0.14898066 0.54618117]

compute the F-beta score
	f beta score (average=macro):  0.29021405572724585
	f beta score (average=micro):  0.37328
	f beta score (average=weighted):  0.34076380108228776
	f beta score (average=None):  [0.56141184 0.15621384 0.19600118 0.26442125 0.23346304 0.22616695
 0.17226789 0.51176647]

compute the average Hamming loss
	hamming loss:  0.62672

jaccard similarity coefficient score
	jaccard score (average=macro):  0.18320412313360424
	jaccard score (average=None):  [0.43698411 0.0746475  0.09836876 0.15209292 0.12420382 0.12316291
 0.08048574 0.37568723]


================================================================================
Classifier.LOGISTIC_REGRESSION
________________________________________________________________________________
Training: 
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class='auto', n_jobs=-1, penalty='l2', random_state=0,
                   solver='lbfgs', tol=0.0001, verbose=True, warm_start=False)
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    9.6s finished
train time: 9.605s
test time:  0.020s
accuracy:   0.421


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.1s finished
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.3s finished
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.6s finished
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.6s finished
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.6s finished
	accuracy: 5-fold cross validation: [0.4314 0.4318 0.4226 0.4322 0.4258]
	test accuracy: 5-fold cross validation accuracy: 0.43 (+/- 0.01)
dimensionality: 74535
density: 1.000000



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.42084
	accuracy score (normalize=False):  10521

compute the precision
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   21.8s finished
	precision score (average=macro):  0.3204049131606921
	precision score (average=micro):  0.42084
	precision score (average=weighted):  0.35624480223221294
	precision score (average=None):  [0.51511428 0.20557491 0.25490196 0.31676529 0.31853786 0.26916868
 0.20851064 0.47466569]
	precision score (average=None, zero_division=1):  [0.51511428 0.20557491 0.25490196 0.31676529 0.31853786 0.26916868
 0.20851064 0.47466569]

compute the precision
	recall score (average=macro):  0.32057672476265786
	recall score (average=micro):  0.42084
	recall score (average=weighted):  0.42084
	recall score (average=None):  [0.8347272  0.05125977 0.11255411 0.30474383 0.21153013 0.23403509
 0.04180887 0.77395479]
	recall score (average=None, zero_division=1):  [0.8347272  0.05125977 0.11255411 0.30474383 0.21153013 0.23403509
 0.04180887 0.77395479]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.2935794987267588
	f1 score (average=micro):  0.42084
	f1 score (average=weighted):  0.36034459757943405
	f1 score (average=None):  [0.63708207 0.08205841 0.15615616 0.3106383  0.25423287 0.25037538
 0.06965174 0.58844106]

compute the F-beta score
	f beta score (average=macro):  0.29811609883201995
	f beta score (average=micro):  0.42084
	f beta score (average=weighted):  0.3479078151029721
	f beta score (average=None):  [0.55783254 0.12831666 0.20344288 0.31428571 0.2892709  0.26132268
 0.11600379 0.51445363]

compute the average Hamming loss
	hamming loss:  0.57916

jaccard similarity coefficient score
	jaccard score (average=macro):  0.19006002807957717
	jaccard score (average=None):  [0.46743979 0.04278463 0.08469055 0.18387909 0.14562817 0.14310234
 0.03608247 0.41687318]


================================================================================
Classifier.LOGISTIC_REGRESSION_CV
________________________________________________________________________________
Training: 
LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,
                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,
                     max_iter=100, multi_class='auto', n_jobs=-1, penalty='l2',
                     random_state=0, refit=True, scoring=None, solver='lbfgs',
                     tol=0.0001, verbose=True)
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   6.381D-02   4.053D+04
  F =   40525.163214755936     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.05204D+04    |proj g|=  5.36626D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   5.028D-02   4.049D+04
  F =   40488.786795757544     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.04526D+04    |proj g|=  5.32781D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     14      1     0     0   3.485D-02   4.022D+04
  F =   40216.954191496952     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.99534D+04    |proj g|=  5.07273D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     23      1     0     0   3.957D-02   3.855D+04
  F =   38545.694533354821     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.71563D+04    |proj g|=  3.84242D+01

At iterate   50    f=  3.27595D+04    |proj g|=  4.89860D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     68     75      1     0     0   2.475D-02   3.276D+04
  F =   32759.440681164193     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.91064D+04    |proj g|=  1.43825D+01

At iterate   50    f=  2.12448D+04    |proj g|=  1.04701D+01

At iterate  100    f=  2.12152D+04    |proj g|=  1.68785D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   1.688D+01   2.122D+04
  F =   21215.185616145238     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.52854D+04    |proj g|=  1.68785D+01

At iterate   50    f=  8.72365D+03    |proj g|=  7.49560D+01

At iterate  100    f=  8.54829D+03    |proj g|=  4.02206D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   4.022D+00   8.548D+03
  F =   8548.2949501604508     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.43398D+03    |proj g|=  4.02206D+00

At iterate   50    f=  2.41854D+03    |proj g|=  1.89405D+00

At iterate  100    f=  2.38228D+03    |proj g|=  2.73061D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    114      1     0     0   2.731D+00   2.382D+03
  F =   2382.2803879496205     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  9.77508D+02    |proj g|=  2.73061D+00

At iterate   50    f=  5.56706D+02    |proj g|=  4.09203D+00

At iterate  100    f=  5.50823D+02    |proj g|=  1.52505D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   1.525D-01   5.508D+02
  F =   550.82282956565098     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.99984D+02    |proj g|=  1.52505D-01

At iterate   50    f=  1.14726D+02    |proj g|=  4.87173D-02

At iterate  100    f=  1.14366D+02    |proj g|=  1.23712D-01

           * * *

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   6.446D-02   4.052D+04
  F =   40524.873344502572     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.05202D+04    |proj g|=  5.32077D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   4.531D-02   4.049D+04
  F =   40488.462971938090     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.04522D+04    |proj g|=  5.28297D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     14      1     0     0   4.122D-02   4.022D+04
  F =   40216.311786751598     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.99524D+04    |proj g|=  5.04810D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   4.793D-02   3.854D+04
  F =   38540.933347699807     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.71469D+04    |proj g|=  3.86599D+01

At iterate   50    f=  3.27342D+04    |proj g|=  1.42455D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     83     91      1     0     0   1.027D+00   3.273D+04
  F =   32734.060862339960     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.90677D+04    |proj g|=  1.50291D+01

At iterate   50    f=  2.12403D+04    |proj g|=  3.88914D+01

At iterate  100    f=  2.11746D+04    |proj g|=  2.40698D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   2.407D+01   2.117D+04
  F =   21174.616221561624     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.53236D+04    |proj g|=  2.40698D+01

At iterate   50    f=  8.67594D+03    |proj g|=  4.86027D+01

At iterate  100    f=  8.50677D+03    |proj g|=  4.13472D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   4.135D+00   8.507D+03
  F =   8506.7713577178947     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.41647D+03    |proj g|=  4.13472D+00

At iterate   50    f=  2.40105D+03    |proj g|=  6.07605D+00

At iterate  100    f=  2.37568D+03    |proj g|=  4.07409D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   4.074D+00   2.376D+03
  F =   2375.6825639502704     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  9.91660D+02    |proj g|=  4.07409D+00

At iterate   50    f=  5.49600D+02    |proj g|=  8.45548D-01

At iterate  100    f=  5.47917D+02    |proj g|=  4.67537D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   4.675D-01   5.479D+02
  F =   547.91681158447534     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.98594D+02    |proj g|=  4.67537D-01

At iterate   50    f=  1.14880D+02    |proj g|=  2.62736D-01

At iterate  100    f=  1.13827D+02    |proj g|=  3.83129D-02

           * * *

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   6.428D-02   4.052D+04
  F =   40524.570090035697     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.05198D+04    |proj g|=  5.32039D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   4.883D-02   4.049D+04
  F =   40488.051335825468     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.04517D+04    |proj g|=  5.28242D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     14      1     0     0   3.553D-02   4.022D+04
  F =   40215.113431805948     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.99505D+04    |proj g|=  5.04966D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     25     29      1     0     0   1.718D-02   3.854D+04
  F =   38536.372502215410     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.71407D+04    |proj g|=  3.85088D+01

At iterate   50    f=  3.27369D+04    |proj g|=  3.91803D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     85     94      1     0     0   2.077D-02   3.274D+04
  F =   32736.684084094417     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.90819D+04    |proj g|=  1.47510D+01

At iterate   50    f=  2.12723D+04    |proj g|=  1.05286D+01

At iterate  100    f=  2.12076D+04    |proj g|=  9.56054D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   9.561D+00   2.121D+04
  F =   21207.606106306124     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.52685D+04    |proj g|=  9.56054D+00

At iterate   50    f=  8.68734D+03    |proj g|=  5.65199D+01

At iterate  100    f=  8.54726D+03    |proj g|=  1.62988D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    113      1     0     0   1.630D+01   8.547D+03
  F =   8547.2628356034093     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.56693D+03    |proj g|=  1.62988D+01

At iterate   50    f=  2.41192D+03    |proj g|=  2.48277D+00

At iterate  100    f=  2.38606D+03    |proj g|=  1.45769D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   1.458D+00   2.386D+03
  F =   2386.0640002575497     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  9.53352D+02    |proj g|=  1.45769D+00

At iterate   50    f=  5.50272D+02    |proj g|=  1.49478D+00

At iterate  100    f=  5.47941D+02    |proj g|=  6.34361D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   6.344D-01   5.479D+02
  F =   547.94074699718567     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.91972D+02    |proj g|=  6.34361D-01

At iterate   50    f=  1.13722D+02    |proj g|=  2.50699D-01

At iterate  100    f=  1.12788D+02    |proj g|=  1.26071D-01

           * * *

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   6.414D-02   4.052D+04
  F =   40524.398622218971     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.05197D+04    |proj g|=  5.46948D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   5.086D-02   4.049D+04
  F =   40487.841139988428     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.04514D+04    |proj g|=  5.44239D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     14      1     0     0   3.415D-02   4.021D+04
  F =   40214.670383686978     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.99499D+04    |proj g|=  5.23731D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     24     27      1     0     0   5.758D-02   3.854D+04
  F =   38536.053843946102     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.71414D+04    |proj g|=  3.98422D+01

At iterate   50    f=  3.27377D+04    |proj g|=  7.99769D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     92     99      1     0     0   7.035D-02   3.274D+04
  F =   32737.609111901158     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.90798D+04    |proj g|=  1.50778D+01

At iterate   50    f=  2.12984D+04    |proj g|=  1.18616D+01

At iterate  100    f=  2.12026D+04    |proj g|=  6.18979D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   6.190D+00   2.120D+04
  F =   21202.579367588285     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.53509D+04    |proj g|=  6.18979D+00

At iterate   50    f=  8.64547D+03    |proj g|=  4.88469D+01

At iterate  100    f=  8.52111D+03    |proj g|=  6.64629D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    104      1     0     0   6.646D+00   8.521D+03
  F =   8521.1128897203707     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.38402D+03    |proj g|=  6.64629D+00

At iterate   50    f=  2.39556D+03    |proj g|=  1.14521D+00

At iterate  100    f=  2.37723D+03    |proj g|=  6.52253D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   6.523D+00   2.377D+03
  F =   2377.2255170766289     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  9.78142D+02    |proj g|=  6.52253D+00

At iterate   50    f=  5.57836D+02    |proj g|=  2.36645D+00

At iterate  100    f=  5.49915D+02    |proj g|=  4.52650D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   4.526D-01   5.499D+02
  F =   549.91542192644852     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.92968D+02    |proj g|=  4.52650D-01

At iterate   50    f=  1.15940D+02    |proj g|=  3.11895D-01

At iterate  100    f=  1.14298D+02    |proj g|=  5.74866D-02

           * * *

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   6.414D-02   4.052D+04
  F =   40524.507557670862     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.05198D+04    |proj g|=  5.34908D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   5.053D-02   4.049D+04
  F =   40488.024997764289     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.04517D+04    |proj g|=  5.30987D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     14      1     0     0   2.537D-02   4.022D+04
  F =   40215.421641801462     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.99512D+04    |proj g|=  5.01825D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     19     23      1     0     0   8.708D-02   3.854D+04
  F =   38540.135343612463     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.71476D+04    |proj g|=  3.75282D+01

At iterate   50    f=  3.27356D+04    |proj g|=  1.80871D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     93    105      1     0     0   2.116D-01   3.274D+04
  F =   32735.220193766188     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.90698D+04    |proj g|=  1.39866D+01

At iterate   50    f=  2.12199D+04    |proj g|=  3.56201D+01

At iterate  100    f=  2.11603D+04    |proj g|=  1.65724D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   1.657D+01   2.116D+04
  F =   21160.297096453742     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.52048D+04    |proj g|=  1.65724D+01

At iterate   50    f=  8.68705D+03    |proj g|=  9.16636D+01

At iterate  100    f=  8.51753D+03    |proj g|=  2.92682D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   2.927D+01   8.518D+03
  F =   8517.5335810628785     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.46795D+03    |proj g|=  2.92682D+01

At iterate   50    f=  2.40100D+03    |proj g|=  7.41542D+00

At iterate  100    f=  2.37489D+03    |proj g|=  3.03382D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   3.034D+00   2.375D+03
  F =   2374.8887798274095     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  9.78711D+02    |proj g|=  3.03382D+00

At iterate   50    f=  5.55127D+02    |proj g|=  2.82779D+00

At iterate  100    f=  5.47752D+02    |proj g|=  2.88639D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   2.886D-01   5.478D+02
  F =   547.75240274225644     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.96563D+02    |proj g|=  2.88639D-01

At iterate   50    f=  1.14508D+02    |proj g|=  4.97745D-01

At iterate  100    f=  1.13718D+02    |proj g|=  5.16099D-02

           * * *

[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.3min finished
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
train time: 153.313s
test time:  0.033s
accuracy:   0.405


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

At iterate   50    f=  2.77031D+04    |proj g|=  2.70050D+01

At iterate  100    f=  2.76707D+04    |proj g|=  2.25527D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   2.255D+00   2.767D+04
  F =   27670.681108630131     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.020D-01   3.242D+04
  F =   32420.421958934581     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.023D-01   3.242D+04
  F =   32420.096135092481     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.017D-01   3.242D+04
  F =   32420.436549634731     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.030D-01   3.242D+04
  F =   32420.037689231005     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.031D-01   3.242D+04
  F =   32420.708680682921     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24170D+04    |proj g|=  4.27856D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24173D+04    |proj g|=  4.28281D+01

At iterate    0    f=  3.24173D+04    |proj g|=  4.32744D+01

At iterate    0    f=  3.24169D+04    |proj g|=  4.25327D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24176D+04    |proj g|=  4.28806D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.515D-02   3.240D+04
  F =   32395.931194712590     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.685D-02   3.240D+04
  F =   32396.183792996984     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.721D-02   3.240D+04
  F =   32395.745435535737     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

At iterate   50    f=  2.76636D+04    |proj g|=  5.07005D+01

At iterate  100    f=  2.76476D+04    |proj g|=  6.99339D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   6.993D+00   2.765D+04
  F =   27647.606762781841     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.036D-01   3.242D+04
  F =   32420.686445343199     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.038D-01   3.242D+04
  F =   32420.476010381837     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.027D-01   3.242D+04
  F =   32420.769928962382     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.039D-01   3.242D+04
  F =   32420.436788423362     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.033D-01   3.242D+04
  F =   32420.784437374448     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24175D+04    |proj g|=  4.22492D+01

At iterate    0    f=  3.24174D+04    |proj g|=  4.18117D+01

At iterate    0    f=  3.24173D+04    |proj g|=  4.30508D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24176D+04    |proj g|=  4.35112D+01

At iterate    0    f=  3.24176D+04    |proj g|=  4.34370D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.575D-02   3.240D+04
  F =   32396.372782029790     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.209D-02   3.240D+04
  F =   32396.476341786270     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.263D-02   3.240D+04
  F =   32396.454464534658     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

At iterate   50    f=  2.76657D+04    |proj g|=  1.46614D+01

At iterate  100    f=  2.76550D+04    |proj g|=  9.52234D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   9.522D+00   2.765D+04
  F =   27654.961948513705     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.027D-01   3.242D+04
  F =   32420.732351455132     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.024D-01   3.242D+04
  F =   32420.690788264314     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.009D-01   3.242D+04
  F =   32421.085082223140     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.020D-01   3.242D+04
  F =   32420.723966767378     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.012D-01   3.242D+04
  F =   32421.076970866161     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24176D+04    |proj g|=  4.27042D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24176D+04    |proj g|=  4.23319D+01

At iterate    0    f=  3.24176D+04    |proj g|=  4.35058D+01

At iterate    0    f=  3.24179D+04    |proj g|=  4.34459D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24179D+04    |proj g|=  4.37791D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.823D-02   3.240D+04
  F =   32396.488815060660     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.433D-02   3.240D+04
  F =   32396.540545530657     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.526D-02   3.240D+04
  F =   32396.475784755461     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  5.19860D+04    |proj g|=  1.97500D+03

At iterate   50    f=  3.42068D+04    |proj g|=  6.15924D+01

At iterate  100    f=  3.41640D+04    |proj g|=  2.57417D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   2.574D+00   3.416D+04
  F =   34164.019144800834     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.023D-01   3.242D+04
  F =   32420.764112132194     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.020D-01   3.242D+04
  F =   32420.316201431619     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.020D-01   3.242D+04
  F =   32420.217064747558     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.024D-01   3.242D+04
  F =   32419.968648156100     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.022D-01   3.242D+04
  F =   32420.006158365552     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.24176D+04    |proj g|=  4.34953D+01

At iterate    0    f=  3.24172D+04    |proj g|=  4.46771D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24171D+04    |proj g|=  4.38199D+01

At iterate    0    f=  3.24168D+04    |proj g|=  4.36677D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24169D+04    |proj g|=  4.31388D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.519D-02   3.240D+04
  F =   32395.912523132531     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.814D-02   3.240D+04
  F =   32395.875199903119     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.781D-02   3.240D+04
  F =   32396.328689965754     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

At iterate    0    f=  3.32711D+04    |proj g|=  1.26400D+03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.025D-01   3.242D+04
  F =   32420.450955101918     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.027D-01   3.242D+04
  F =   32420.404566665213     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.031D-01   3.242D+04
  F =   32420.104180210052     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.034D-01   3.242D+04
  F =   32420.781777561460     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     10     13      1     0     0   1.026D-01   3.242D+04
  F =   32420.212843106518     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24173D+04    |proj g|=  4.18358D+01

At iterate    0    f=  3.24172D+04    |proj g|=  4.24876D+01

At iterate    0    f=  3.24169D+04    |proj g|=  4.30470D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.24176D+04    |proj g|=  4.30732D+01

At iterate    0    f=  3.24171D+04    |proj g|=  4.34753D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.468D-02   3.240D+04
  F =   32396.358894537556     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.538D-02   3.240D+04
  F =   32395.740330520319     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.316D-02   3.240D+04
  F =   32396.462998679774     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.618D-02   3.240D+04
  F =   32396.002361706272     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.752D-02   3.240D+04
  F =   32395.855873325279     

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.594D-02   3.240D+04
  F =   32395.601269259794     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.23716D+04    |proj g|=  4.45053D+01

At iterate    0    f=  3.23716D+04    |proj g|=  4.36499D+01

At iterate    0    f=  3.23720D+04    |proj g|=  4.33233D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.644D-02   3.240D+04
  F =   32395.889912971470     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.23713D+04    |proj g|=  4.34911D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23719D+04    |proj g|=  4.29684D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     10      1     0     0   2.379D-02   3.221D+04
  F =   32212.128094415140     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     10      1     0     0   2.053D-02   3.221D+04
  F =   32212.597030135872     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     10      1     0     0   2.670D-02   3.221D+04
  F =   32212.395996805441     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     10      1     0     0   5.659D-02   3.221D+04
  F =   32212.149806409470     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20338D+04    |proj g|=  4.23381D+01

At iterate    0    f=  3.20330D+04    |proj g|=  4.19989D+01

At iterate    0    f=  3.20328D+04    |proj g|=  4.31799D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     10      1     0     0   4.086D-02   3.221D+04
  F =   32214.280213923310     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.20332D+04    |proj g|=  4.21305D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20370D+04    |proj g|=  4.16548D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     23      1     0     0   5.767D-02   3.104D+04
  F =   31036.297401552609     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     18     22      1     0     0   2.261D-01   3.103D+04
  F =   31033.483978703145     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     18     22      1     0     0   8.735D-02   3.103D+04
  F =   31031.008296003234     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00270D+04    |proj g|=  3.38532D+01
RUNNING THE L-BFGS-B CODE

           * * *

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.601D-02   3.240D+04
  F =   32396.071927335812     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23718D+04    |proj g|=  4.25362D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.473D-02   3.240D+04
  F =   32396.358341941941     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.23720D+04    |proj g|=  4.30279D+01

At iterate    0    f=  3.23718D+04    |proj g|=  4.26526D+01

At iterate    0    f=  3.23715D+04    |proj g|=  4.22842D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23721D+04    |proj g|=  4.26304D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      7      9      1     0     0   3.341D-01   3.221D+04
  F =   32212.846325652834     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     10      1     0     0   1.548D-02   3.221D+04
  F =   32212.744543959379     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   5.366D-02   3.221D+04
  F =   32213.977577217731     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   8.626D-02   3.221D+04
  F =   32213.546284685945     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20344D+04    |proj g|=  4.03913D+01

At iterate    0    f=  3.20364D+04    |proj g|=  4.06382D+01

At iterate    0    f=  3.20353D+04    |proj g|=  4.11496D+01

At iterate    0    f=  3.20339D+04    |proj g|=  4.12994D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      9     11      1     0     0   4.684D-03   3.221D+04
  F =   32213.048816671351     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20342D+04    |proj g|=  4.07268D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     19     22      1     0     0   3.221D-01   3.104D+04
  F =   31038.989531682091     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     18     23      1     0     0   2.753D-02   3.105D+04
  F =   31045.424516111296     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.00316D+04    |proj g|=  3.14706D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     21     25      1     0     0   3.165D-02   3.104D+04
  F =   31039.855767789086     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.237D-02   3.240D+04
  F =   32396.174479275840     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23726D+04    |proj g|=  4.15773D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.188D-02   3.240D+04
  F =   32396.483778345675     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23723D+04    |proj g|=  4.20125D+01

At iterate    0    f=  3.23721D+04    |proj g|=  4.32685D+01

At iterate    0    f=  3.23720D+04    |proj g|=  4.28038D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23723D+04    |proj g|=  4.32704D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   8.218D-02   3.221D+04
  F =   32213.949972572766     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     10      1     0     0   2.466D-01   3.221D+04
  F =   32212.682860604938     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   8.353D-02   3.222D+04
  F =   32215.692803629088     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   7.545D-02   3.221D+04
  F =   32213.461959457938     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20358D+04    |proj g|=  4.04192D+01

At iterate    0    f=  3.20334D+04    |proj g|=  4.14227D+01

At iterate    0    f=  3.20392D+04    |proj g|=  3.97927D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      9     11      1     0     0   4.715D-03   3.221D+04
  F =   32213.468087052872     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20352D+04    |proj g|=  4.09224D+01

At iterate    0    f=  3.20349D+04    |proj g|=  4.19857D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   1.013D-01   3.103D+04
  F =   31033.453929862837     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   1.910D-02   3.105D+04
  F =   31051.685804296980     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     19     22      1     0     0   3.362D-02   3.104D+04
  F =   31036.968160562130     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.23723D+04    |proj g|=  4.16005D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23715D+04    |proj g|=  4.27985D+01

At iterate    0    f=  3.23717D+04    |proj g|=  4.22449D+01

At iterate    0    f=  3.23722D+04    |proj g|=  4.28663D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23716D+04    |proj g|=  4.33073D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      9     11      1     0     0   4.699D-03   3.221D+04
  F =   32212.284047058292     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   2.675D-02   3.221D+04
  F =   32214.893283788173     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   1.773D-02   3.221D+04
  F =   32212.276772966274     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      7      9      1     0     0   3.458D-01   3.221D+04
  F =   32212.447938216792     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   1.633D-01   3.221D+04
  F =   32213.297065957759     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.20330D+04    |proj g|=  4.03973D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20378D+04    |proj g|=  3.98070D+01

At iterate    0    f=  3.20333D+04    |proj g|=  4.09055D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20335D+04    |proj g|=  4.20106D+01

At iterate    0    f=  3.20346D+04    |proj g|=  4.15992D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     19     23      1     0     0   6.470D-02   3.103D+04
  F =   31033.656822470090     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     19     23      1     0     0   5.321D-02   3.103D+04
  F =   31034.207856160967     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     18     23      1     0     0   1.923D-02   3.105D+04
  F =   31047.452883398993     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     23      1     0     0   5.160D-02   3.104D+04
  F =   31035.387223582813     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00231D+04    |proj g|=  3.19483D+01

At iterate    0    f=  3.00226D+04    |proj g|=  3.20006D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.683D-02   3.240D+04
  F =   32396.891865607715     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23723D+04    |proj g|=  4.32539D+01

At iterate    0    f=  3.23724D+04    |proj g|=  4.24620D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      5      8      1     0     0   2.633D-02   3.240D+04
  F =   32396.830030553290     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.23724D+04    |proj g|=  4.20904D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23728D+04    |proj g|=  4.32053D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.23727D+04    |proj g|=  4.36035D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   9.702D-02   3.221D+04
  F =   32213.907508819768     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     10      1     0     0   7.607D-02   3.221D+04
  F =   32214.396798941048     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      8     11      1     0     0   8.865D-02   3.221D+04
  F =   32214.256908039195     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      9     12      1     0     0   5.811D-02   3.221D+04
  F =   32214.686417595047     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.20358D+04    |proj g|=  4.13349D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      7      9      1     0     0   1.990D-01   3.221D+04
  F =   32214.264214994804     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.20367D+04    |proj g|=  4.02543D+01

At iterate    0    f=  3.20365D+04    |proj g|=  4.06855D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.20369D+04    |proj g|=  4.15899D+01

At iterate    0    f=  3.20361D+04    |proj g|=  4.22493D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     19     23      1     0     0   6.818D-02   3.104D+04
  F =   31044.802722469569     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     21     25      1     0     0   1.640D-02   3.104D+04
  F =   31042.006036496896     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00404D+04    |proj g|=  3.16115D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   3.823D-02   3.104D+04
  F =   31044.210557247538     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00361D+04    |proj g|=  3.19359D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

At iterate   50    f=  2.76965D+04    |proj g|=  1.29531D+02

At iterate  100    f=  2.76735D+04    |proj g|=  8.83137D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   8.831D-01   2.767D+04
  F =   27673.453656472226     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
 This problem is unconstrained.
 This problem is unconstrained.
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.15888D+04    |proj g|=  1.58000D+03

At iterate   50    f=  2.77121D+04    |proj g|=  9.83595D+01

At iterate  100    f=  2.76928D+04    |proj g|=  1.15773D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   1.158D+00   2.769D+04
  F =   27692.796652515950     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   2.691D-02   3.104D+04
  F =   31036.519249171302     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.00419D+04    |proj g|=  3.13687D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00316D+04    |proj g|=  3.22682D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     19     23      1     0     0   2.657D-01   3.104D+04
  F =   31037.180055830348     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.00273D+04    |proj g|=  3.26069D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00282D+04    |proj g|=  3.12694D+01

At iterate   50    f=  2.65691D+04    |proj g|=  7.74925D-01

At iterate   50    f=  2.65561D+04    |proj g|=  7.39718D-01

At iterate   50    f=  2.65831D+04    |proj g|=  5.14917D-01

At iterate   50    f=  2.65554D+04    |proj g|=  1.49530D+00

At iterate   50    f=  2.65613D+04    |proj g|=  1.66197D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     63     69      1     0     0   2.557D-02   2.657D+04
  F =   26569.032484538016     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36276D+04    |proj g|=  1.27764D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     66     71      1     0     0   3.015D-02   2.656D+04
  F =   26556.050348837511     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36003D+04    |proj g|=  1.31307D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     69     77      1     0     0   2.166D-01   2.658D+04
  F =   26583.104990869382     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36412D+04    |proj g|=  1.25250D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     70     80      1     0     0   5.383D-01   2.656D+04
  F =   26561.294773116635     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36160D+04    |proj g|=  1.30252D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     73     83      1     0     0   2.400D-01   2.656D+04
  F =   26555.340195882218     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36031D+04    |proj g|=  1.27442D+01

At iterate   50    f=  1.70359D+04    |proj g|=  1.86786D+01

At iterate   50    f=  1.70180D+04    |proj g|=  2.18625D+01

At iterate   50    f=  1.70481D+04    |proj g|=  6.26386D+00

At iterate   50    f=  1.70347D+04    |proj g|=  1.96859D+01

At iterate   50    f=  1.70001D+04    |proj g|=  5.13856D+01

At iterate  100    f=  1.70171D+04    |proj g|=  1.61957D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    113      1     0     0   1.620D+00   1.702D+04
  F =   17017.121240065444     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20693D+04    |proj g|=  2.80118D+00

At iterate  100    f=  1.69748D+04    |proj g|=  8.64925D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   8.649D+00   1.697D+04
  F =   16974.809898924821     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  1.70231D+04    |proj g|=  3.79517D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00398D+04    |proj g|=  3.23895D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   7.093D-02   3.104D+04
  F =   31043.132498056431     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   1.680D-02   3.104D+04
  F =   31042.383242055970     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00367D+04    |proj g|=  3.35115D+01

At iterate    0    f=  3.00363D+04    |proj g|=  3.31896D+01

At iterate   50    f=  2.65832D+04    |proj g|=  2.68798D+00

At iterate   50    f=  2.65692D+04    |proj g|=  1.94074D+00

At iterate   50    f=  2.65723D+04    |proj g|=  5.87822D-01

At iterate   50    f=  2.65917D+04    |proj g|=  2.54455D-01

At iterate   50    f=  2.65862D+04    |proj g|=  5.55847D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     56     64      1     0     0   3.688D-01   2.657D+04
  F =   26572.271061292635     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36280D+04    |proj g|=  1.27486D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     69     76      1     0     0   1.204D-01   2.658D+04
  F =   26583.147285479736     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36457D+04    |proj g|=  1.29617D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     65     75      1     0     0   5.905D-02   2.659D+04
  F =   26591.650096114532     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36661D+04    |proj g|=  1.32767D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     73     78      1     0     0   4.736D-02   2.657D+04
  F =   26569.145819009256     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36234D+04    |proj g|=  1.36442D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     75     84      1     0     0   2.659D-01   2.659D+04
  F =   26586.092402465962     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36554D+04    |proj g|=  1.35031D+01

At iterate   50    f=  1.70308D+04    |proj g|=  3.49690D+01

At iterate   50    f=  1.70612D+04    |proj g|=  4.81070D+01

At iterate   50    f=  1.71005D+04    |proj g|=  1.22338D+01

At iterate   50    f=  1.70734D+04    |proj g|=  3.63503D+01

At iterate   50    f=  1.70731D+04    |proj g|=  2.96405D+01

At iterate  100    f=  1.70058D+04    |proj g|=  4.97941D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   4.979D+00   1.701D+04
  F =   17005.780360460070     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.21236D+04    |proj g|=  4.97941D+00

At iterate  100    f=  1.70295D+04    |proj g|=  8.10456D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    114      1     0     0   8.105D+00   1.703D+04
  F =   17029.499833387796     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  1.70765D+04    |proj g|=  8.05003D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
*****     20     24      1     0     0   1.993D-02   3.104D+04
  F =   31040.390905042012     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     19     23      1     0     0   4.624D-02   3.104D+04
  F =   31039.252256736818     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00263D+04    |proj g|=  3.36792D+01

At iterate    0    f=  3.00212D+04    |proj g|=  3.30979D+01

At iterate    0    f=  3.00502D+04    |proj g|=  3.18627D+01

At iterate    0    f=  3.00319D+04    |proj g|=  3.25366D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00306D+04    |proj g|=  3.20748D+01

At iterate   50    f=  2.65552D+04    |proj g|=  5.71020D-01

At iterate   50    f=  2.65466D+04    |proj g|=  4.33679D-01

At iterate   50    f=  2.65923D+04    |proj g|=  1.57519D+00

At iterate   50    f=  2.65660D+04    |proj g|=  2.18204D+00

At iterate   50    f=  2.65563D+04    |proj g|=  8.02830D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     66     73      1     0     0   2.741D-02   2.657D+04
  F =   26566.027803726673     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     65     74      1     0     0   1.178D-01   2.659D+04
  F =   26592.226455083961     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     72     81      1     0     0   1.545D-02   2.656D+04
  F =   26555.090310474236     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36252D+04    |proj g|=  1.38743D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36544D+04    |proj g|=  1.33302D+01

At iterate    0    f=  2.36109D+04    |proj g|=  1.41505D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     66     72      1     0     0   3.988D-02   2.656D+04
  F =   26556.253317126007     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36032D+04    |proj g|=  1.33413D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     77     88      1     0     0   5.995D-02   2.655D+04
  F =   26546.527248625222     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.35969D+04    |proj g|=  1.39551D+01

At iterate   50    f=  1.70425D+04    |proj g|=  1.04272D+01

At iterate   50    f=  1.70667D+04    |proj g|=  4.39046D+01

At iterate   50    f=  1.70282D+04    |proj g|=  4.87334D+00

At iterate   50    f=  1.70055D+04    |proj g|=  2.34824D+01

At iterate   50    f=  1.70288D+04    |proj g|=  1.24712D+01

At iterate  100    f=  1.70181D+04    |proj g|=  7.44280D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   7.443D+00   1.702D+04
  F =   17018.148677446265     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20499D+04    |proj g|=  7.44280D+00

At iterate  100    f=  1.70369D+04    |proj g|=  6.30900D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   6.309D+00   1.704D+04
  F =   17036.891544220016     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00228D+04    |proj g|=  3.34876D+01

At iterate    0    f=  3.00166D+04    |proj g|=  3.45947D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   2.538D-02   3.104D+04
  F =   31035.388390384309     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     20     24      1     0     0   4.067D-02   3.105D+04
  F =   31047.126415512328     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  3.00260D+04    |proj g|=  3.33865D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00444D+04    |proj g|=  3.31660D+01

At iterate   50    f=  2.65694D+04    |proj g|=  2.30424D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     48     54      1     0     0   9.090D-02   2.659D+04
  F =   26593.208717441823     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate   50    f=  2.65610D+04    |proj g|=  8.57411D-01

At iterate   50    f=  2.65357D+04    |proj g|=  2.75052D-01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36607D+04    |proj g|=  1.34597D+01

At iterate   50    f=  2.65609D+04    |proj g|=  4.67386D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     59     68      1     0     0   5.081D-01   2.656D+04
  F =   26560.904447607947     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36179D+04    |proj g|=  1.34698D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     71     79      1     0     0   3.799D-01   2.654D+04
  F =   26535.708092500943     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.35842D+04    |proj g|=  1.44281D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     80     90      1     0     0   1.630D-01   2.657D+04
  F =   26569.370007479116     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     79     88      1     0     0   2.797D-01   2.656D+04
  F =   26560.954463599279     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36359D+04    |proj g|=  1.40212D+01

At iterate    0    f=  2.36234D+04    |proj g|=  1.38007D+01

At iterate   50    f=  1.71011D+04    |proj g|=  8.11108D+01

At iterate   50    f=  1.70396D+04    |proj g|=  1.38088D+01

At iterate   50    f=  1.70203D+04    |proj g|=  1.09691D+01

At iterate   50    f=  1.70730D+04    |proj g|=  2.26042D+01

At iterate   50    f=  1.70493D+04    |proj g|=  5.42318D+01

At iterate  100    f=  1.70595D+04    |proj g|=  1.06589D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   1.066D+00   1.706D+04
  F =   17059.496230867397     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20776D+04    |proj g|=  3.07511D+00

At iterate  100    f=  1.70057D+04    |proj g|=  2.64576D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   2.646D+00   1.701D+04
  F =   17005.657940327648     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20946D+04    |proj g|=  3.02727D+00

At iterate    0    f=  3.00439D+04    |proj g|=  3.18000D+01

At iterate    0    f=  3.00233D+04    |proj g|=  3.33987D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     21     26      1     0     0   8.398D-02   3.103D+04
  F =   31034.889639069861     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.00243D+04    |proj g|=  3.36133D+01

At iterate   50    f=  2.65527D+04    |proj g|=  4.55352D-01

At iterate   50    f=  2.65608D+04    |proj g|=  4.23798D-01

At iterate   50    f=  2.65908D+04    |proj g|=  2.58418D-01

At iterate   50    f=  2.65490D+04    |proj g|=  1.25900D+00

At iterate   50    f=  2.65632D+04    |proj g|=  4.69085D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     50     57      1     0     0   4.691D-02   2.656D+04
  F =   26563.246727271613     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36258D+04    |proj g|=  1.39411D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     71     84      1     0     0   1.648D-02   2.656D+04
  F =   26560.834458386493     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     71     81      1     0     0   1.734D-01   2.659D+04
  F =   26590.764632420705     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  2.36249D+04    |proj g|=  1.33079D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36590D+04    |proj g|=  1.31384D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     78     88      1     0     0   1.111D-01   2.655D+04
  F =   26552.677009191873     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****     74     87      1     0     0   8.366D-02   2.655D+04
  F =   26548.954585963609     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

At iterate    0    f=  2.36061D+04    |proj g|=  1.30680D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  2.36017D+04    |proj g|=  1.40873D+01

At iterate   50    f=  1.70670D+04    |proj g|=  1.89420D+01

At iterate   50    f=  1.70259D+04    |proj g|=  7.82709D+00

At iterate   50    f=  1.70570D+04    |proj g|=  2.42014D+01

At iterate   50    f=  1.70923D+04    |proj g|=  1.94594D+01

At iterate   50    f=  1.70224D+04    |proj g|=  2.01816D+01

At iterate  100    f=  1.70374D+04    |proj g|=  3.25087D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    113      1     0     0   3.251D+00   1.704D+04
  F =   17037.449888234652     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20547D+04    |proj g|=  3.25087D+00

At iterate  100    f=  1.70037D+04    |proj g|=  4.47350D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   4.473D+00   1.700D+04
  F =   17003.720252439474     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  1.70293D+04    |proj g|=  6.89906D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   6.899D+00   1.703D+04
  F =   17029.292787952771     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20754D+04    |proj g|=  4.47350D+00

At iterate  100    f=  1.70670D+04    |proj g|=  2.16430D+00

[LibLinear]Tit   = total number of iterations
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   1.237D-01   1.144D+02
  F =   114.36637995206640     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
[LibLinear]Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   3.831D-02   1.138D+02
  F =   113.82735303221637     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   1.261D-01   1.128D+02
  F =   112.78828072788849     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
[LibLinear]Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   5.749D-02   1.143D+02
  F =   114.29822919670841     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
[LibLinear]Tit   = total number of iterations
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   5.161D-02   1.137D+02
  F =   113.71836320660441     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   8.050D+00   1.708D+04
  F =   17076.473417186222     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate  100    f=  1.70178D+04    |proj g|=  3.77242D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   3.772D+00   1.702D+04
  F =   17017.765708774095     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate    0    f=  1.21325D+04    |proj g|=  8.10456D+00
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.22380D+04    |proj g|=  8.05003D+00

At iterate    0    f=  1.20392D+04    |proj g|=  3.77242D+00

At iterate  100    f=  1.70610D+04    |proj g|=  4.08643D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   4.086D+00   1.706D+04
  F =   17061.032978194788     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.21412D+04    |proj g|=  4.08643D+00

At iterate   50    f=  6.74631D+03    |proj g|=  2.39917D+01

At iterate   50    f=  6.73054D+03    |proj g|=  9.69770D+00

At iterate   50    f=  6.76738D+03    |proj g|=  1.66317D+01

At iterate   50    f=  6.75008D+03    |proj g|=  2.79510D+01

At iterate   50    f=  6.80912D+03    |proj g|=  1.96579D+01

At iterate  100    f=  6.64858D+03    |proj g|=  4.76075D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   4.761D+00   6.649D+03
  F =   6648.5817361932559     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.43074D+03    |proj g|=  4.76075D+00

At iterate  100    f=  6.65881D+03    |proj g|=  5.20402D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   5.204D+00   6.659D+03
  F =   6658.8071873455665     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.44070D+03    |proj g|=  5.20402D+00

At iterate  100    f=  6.69216D+03    |proj g|=  4.52011D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   4.520D+00   6.692D+03
  F =   6692.1555001826518     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  6.65772D+03    |proj g|=  1.11379D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   1.114D+01   6.658D+03
  F =   6657.7226432385032     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.34493D+03    |proj g|=  4.52011D+00

At iterate    0    f=  3.31772D+03    |proj g|=  1.11379D+01

At iterate  100    f=  6.67911D+03    |proj g|=  1.96823D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   1.968D+01   6.679D+03
  F =   6679.1124491976552     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.42631D+03    |proj g|=  1.96823D+01

At iterate   50    f=  1.83491D+03    |proj g|=  1.75517D+00

At iterate   50    f=  1.83223D+03    |proj g|=  7.52764D-01

At iterate   50    f=  1.83194D+03    |proj g|=  1.03968D+01

At iterate   50    f=  1.84324D+03    |proj g|=  6.09197D+00

At iterate   50    f=  1.84556D+03    |proj g|=  2.01801D+00

At iterate  100    f=  1.81650D+03    |proj g|=  1.26205D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   1.262D+00   1.817D+03
At iterate  100    f=  1.69765D+04    |proj g|=  8.40018D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   8.400D+00   1.698D+04
  F =   16976.488321989324     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20553D+04    |proj g|=  8.40018D+00

At iterate  100    f=  1.70480D+04    |proj g|=  2.99486D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   2.995D+00   1.705D+04
  F =   17047.968265197534     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.21275D+04    |proj g|=  2.99486D+00

At iterate  100    f=  1.70254D+04    |proj g|=  1.27488D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   1.275D+01   1.703D+04
  F =   17025.364099555052     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20428D+04    |proj g|=  1.27488D+01

At iterate   50    f=  6.73277D+03    |proj g|=  3.22388D+01

At iterate   50    f=  6.73151D+03    |proj g|=  6.53432D+00

At iterate   50    f=  6.70826D+03    |proj g|=  1.93671D+01

At iterate   50    f=  6.72886D+03    |proj g|=  3.89776D+01

At iterate   50    f=  6.73718D+03    |proj g|=  2.49291D+01

At iterate  100    f=  6.67891D+03    |proj g|=  8.93412D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   8.934D+00   6.679D+03
  F =   6678.9105981054654     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.40223D+03    |proj g|=  8.93412D+00

At iterate  100    f=  6.65093D+03    |proj g|=  4.04119D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   4.041D+00   6.651D+03
  F =   6650.9285010451140     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.41718D+03    |proj g|=  4.04119D+00

At iterate  100    f=  6.62788D+03    |proj g|=  1.65905D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   1.659D+01   6.628D+03
  F =   6627.8847764701495     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.31142D+03    |proj g|=  1.65905D+01

At iterate  100    f=  6.67647D+03    |proj g|=  2.94003D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   2.940D+00   6.676D+03
  F =   6676.4721274575932     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.41186D+03    |proj g|=  2.94003D+00

At iterate  100    f=  6.65990D+03    |proj g|=  2.37917D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   2.379D+01   6.660D+03
  F =   6659.8975997127254     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.41056D+03    |proj g|=  2.37917D+01

At iterate   50    f=  1.84018D+03    |proj g|=  8.46298D+00

At iterate   50    f=  1.84047D+03    |proj g|=  2.30837D+00

At iterate   50    f=  1.81922D+03    |proj g|=  4.33529D-01

At iterate   50    f=  1.84252D+03    |proj g|=  5.58191D+00

At iterate   50    f=  1.83385D+03    |proj g|=  1.78250D+01

At iterate  100    f=  1.82551D+03    |proj g|=  2.84277D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   2.843D+00   1.826D+03
  F =   1825.5085005141641     

Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   3.795D+00   1.702D+04
  F =   17023.117890231078     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20661D+04    |proj g|=  8.64925D+00

At iterate    0    f=  1.20923D+04    |proj g|=  3.79517D+00

At iterate  100    f=  1.70043D+04    |proj g|=  1.82474D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   1.825D+01   1.700D+04
  F =   17004.340092963976     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  1.69712D+04    |proj g|=  1.17867D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   1.179D+00   1.697D+04
  F =   16971.156416776899     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.21183D+04    |proj g|=  1.82474D+01

At iterate    0    f=  1.20493D+04    |proj g|=  2.91145D+00

At iterate   50    f=  6.75188D+03    |proj g|=  2.19098D+01

At iterate   50    f=  6.72799D+03    |proj g|=  1.47957D+01

At iterate   50    f=  6.71825D+03    |proj g|=  4.49795D+00

At iterate   50    f=  6.71704D+03    |proj g|=  1.12573D+01

At iterate   50    f=  6.69387D+03    |proj g|=  3.21180D+01

At iterate  100    f=  6.65792D+03    |proj g|=  1.14991D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   1.150D+01   6.658D+03
  F =   6657.9230723008604     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.33981D+03    |proj g|=  1.14991D+01

At iterate  100    f=  6.63329D+03    |proj g|=  1.23169D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   1.232D+01   6.633D+03
  F =   6633.2945529559784     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.27454D+03    |proj g|=  1.23169D+01

At iterate  100    f=  6.64660D+03    |proj g|=  3.89981D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   3.900D+00   6.647D+03
  F =   6646.6029055183426     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.34861D+03    |proj g|=  3.89981D+00

At iterate  100    f=  6.64427D+03    |proj g|=  2.27322D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   2.273D+01   6.644D+03
  F =   6644.2672879067577     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.32594D+03    |proj g|=  2.27322D+01

At iterate  100    f=  6.62225D+03    |proj g|=  1.63402D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   1.634D+01   6.622D+03
  F =   6622.2460150850075     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.35046D+03    |proj g|=  1.63402D+01

At iterate   50    f=  1.83283D+03    |proj g|=  4.55656D+00

At iterate   50    f=  1.82513D+03    |proj g|=  1.14556D+01

At iterate   50    f=  1.83266D+03    |proj g|=  6.47456D+00

At iterate   50    f=  1.83378D+03    |proj g|=  4.38508D+00

At iterate   50    f=  1.82670D+03    |proj g|=  1.32657D+01

At iterate  100    f=  1.81802D+03    |proj g|=  7.48619D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   7.486D-01   1.818D+03
           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   2.164D+00   1.707D+04
  F =   17066.956591521859     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate    0    f=  1.21245D+04    |proj g|=  6.89906D+00
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.21311D+04    |proj g|=  3.02266D+00

At iterate  100    f=  1.70017D+04    |proj g|=  1.50079D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    114      1     0     0   1.501D+01   1.700D+04
  F =   17001.697132864563     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20983D+04    |proj g|=  1.50079D+01

At iterate   50    f=  6.74604D+03    |proj g|=  3.64967D+01

At iterate   50    f=  6.77435D+03    |proj g|=  2.61110D+01

At iterate   50    f=  6.73781D+03    |proj g|=  5.69361D+00

At iterate   50    f=  6.77014D+03    |proj g|=  3.25264D+01

At iterate   50    f=  6.75180D+03    |proj g|=  2.11204D+01

At iterate  100    f=  6.65852D+03    |proj g|=  3.00250D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   3.003D+00   6.659D+03
  F =   6658.5188039944424     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.36699D+03    |proj g|=  3.00250D+00

At iterate  100    f=  6.66639D+03    |proj g|=  5.14247D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    105      1     0     0   5.142D+00   6.666D+03
  F =   6666.3929695811275     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.48779D+03    |proj g|=  5.14247D+00

At iterate  100    f=  6.65647D+03    |proj g|=  4.70566D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   4.706D+00   6.656D+03
  F =   6656.4663916607824     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.39968D+03    |proj g|=  4.70566D+00

At iterate  100    f=  6.68649D+03    |proj g|=  1.30917D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   1.309D+01   6.686D+03
  F =   6686.4934658193251     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.37980D+03    |proj g|=  1.30917D+01

At iterate  100    f=  6.64773D+03    |proj g|=  4.09562D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   4.096D+00   6.648D+03
  F =   6647.7335189977221     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.40165D+03    |proj g|=  4.09562D+00

At iterate   50    f=  1.83500D+03    |proj g|=  7.22240D+00

At iterate   50    f=  1.84424D+03    |proj g|=  1.37534D+01

At iterate   50    f=  1.83558D+03    |proj g|=  7.75692D+00

At iterate   50    f=  1.84499D+03    |proj g|=  6.07922D+00

At iterate   50    f=  1.82613D+03    |proj g|=  6.04517D+00

At iterate  100    f=  1.82181D+03    |proj g|=  9.28186D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   9.282D-01   1.822D+03
  F =   1821.8128891033643     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.28896D+02    |proj g|=  9.28186D-01

At iterate  100    f=  1.82205D+03    |proj g|=  5.20406D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   5.204D+00   1.822D+03
  F =   1822.0516753078628     

At X0         0 variables are exactly at the bounds

At iterate  100    f=  1.69995D+04    |proj g|=  1.10441D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    112      1     0     0   1.104D+01   1.700D+04
  F =   16999.495720032122     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate    0    f=  1.20746D+04    |proj g|=  6.30900D+00
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20797D+04    |proj g|=  1.10441D+01

At iterate  100    f=  1.69779D+04    |proj g|=  1.37544D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   1.375D+01   1.698D+04
  F =   16977.943531425350     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20581D+04    |proj g|=  1.37544D+01

At iterate  100    f=  1.69929D+04    |proj g|=  1.57537D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   1.575D+00   1.699D+04
  F =   16992.925641062517     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.20963D+04    |proj g|=  3.28932D+00

At iterate   50    f=  6.71203D+03    |proj g|=  1.54404D+01

At iterate   50    f=  6.73351D+03    |proj g|=  5.29632D+01

At iterate   50    f=  6.72981D+03    |proj g|=  2.17131D+01

At iterate   50    f=  6.73380D+03    |proj g|=  1.13007D+01

At iterate   50    f=  6.76355D+03    |proj g|=  6.31410D+01

At iterate  100    f=  6.64971D+03    |proj g|=  1.11628D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   1.116D+01   6.650D+03
  F =   6649.7089543926850     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  6.65838D+03    |proj g|=  3.40353D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   3.404D+00   6.658D+03
  F =   6658.3792680085771     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.44550D+03    |proj g|=  1.11628D+01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.40029D+03    |proj g|=  3.40353D+00

At iterate  100    f=  6.64001D+03    |proj g|=  1.62605D+01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   1.626D+01   6.640D+03
  F =   6640.0062478070604     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.36064D+03    |proj g|=  1.62605D+01

At iterate  100    f=  6.63096D+03    |proj g|=  3.33998D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   3.340D+00   6.631D+03
  F =   6630.9637339226811     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  6.64196D+03    |proj g|=  5.86643D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   5.866D+00   6.642D+03
  F =   6641.9645004680760     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  3.31728D+03    |proj g|=  3.33998D+00

At iterate    0    f=  3.32739D+03    |proj g|=  5.86643D+00

At iterate   50    f=  1.83591D+03    |proj g|=  2.60923D+00

At iterate   50    f=  1.83598D+03    |proj g|=  2.11290D+00

At iterate   50    f=  1.84000D+03    |proj g|=  2.44180D+00

At iterate   50    f=  1.83008D+03    |proj g|=  1.19297D+01

At iterate   50    f=  1.82767D+03    |proj g|=  1.18351D+01

At iterate  100    f=  1.82130D+03    |proj g|=  1.05749D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
  F =   1818.0223216916943     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.29997D+02    |proj g|=  7.48619D-01

At iterate  100    f=  1.81298D+03    |proj g|=  9.52716D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   9.527D-01   1.813D+03
  F =   1812.9842469305881     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.30986D+02    |proj g|=  9.52716D-01

At iterate  100    f=  1.81800D+03    |proj g|=  2.35942D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   2.359D-01   1.818D+03
  F =   1817.9992260289637     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.41170D+02    |proj g|=  2.35942D-01

At iterate  100    f=  1.81543D+03    |proj g|=  1.41924D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   1.419D+00   1.815D+03
  F =   1815.4341989174445     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.43000D+02    |proj g|=  1.41924D+00

At iterate  100    f=  1.81109D+03    |proj g|=  3.19332D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   3.193D+00   1.811D+03
  F =   1811.0872767249086     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.24872D+02    |proj g|=  3.19332D+00

At iterate   50    f=  4.17805D+02    |proj g|=  8.48866D-01

At iterate   50    f=  4.16560D+02    |proj g|=  1.72698D+00

At iterate   50    f=  4.18078D+02    |proj g|=  4.75247D-01

At iterate   50    f=  4.16776D+02    |proj g|=  3.05135D-01

At iterate   50    f=  4.16016D+02    |proj g|=  7.63658D-01

At iterate  100    f=  4.14847D+02    |proj g|=  6.66807D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   6.668D-01   4.148D+02
  F =   414.84721018760587     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.47225D+02    |proj g|=  6.66807D-01

At iterate  100    f=  4.14073D+02    |proj g|=  1.72036D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   1.720D-01   4.141D+02
  F =   414.07306881618615     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.39510D+02    |proj g|=  1.72036D-01

At iterate  100    f=  4.13870D+02    |proj g|=  2.80847D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   2.808D-01   4.139D+02
  F =   413.86956810102413     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate  100    f=  4.15689D+02    |proj g|=  8.65806D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   8.658D-01   4.157D+02
  F =   415.68850155535199     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate    0    f=  1.44419D+02    |proj g|=  2.80847D-01

At iterate  100    f=  4.13617D+02    |proj g|=  4.19035D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   4.190D-01   4.136D+02
  F =   413.61732152847179     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

*****    100    109      1     0     0   1.057D+00   1.821D+03
  F =   1821.2975889072047     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  1.81691D+03    |proj g|=  5.66150D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   5.662D-01   1.817D+03
  F =   1816.9136358058106     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate  100    f=  1.81570D+03    |proj g|=  3.11245D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    105      1     0     0   3.112D+00   1.816D+03
  F =   1815.7037199492343     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.26518D+02    |proj g|=  1.05749D+00
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.38512D+02    |proj g|=  3.11245D+00

At iterate    0    f=  7.51637D+02    |proj g|=  5.66150D-01

At iterate  100    f=  1.81153D+03    |proj g|=  3.34627D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   3.346D+00   1.812D+03
  F =   1811.5303491621623     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.42428D+02    |proj g|=  3.34627D+00

At iterate  100    f=  1.81543D+03    |proj g|=  5.97108D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   5.971D-01   1.815D+03
  F =   1815.4275260927766     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.32212D+02    |proj g|=  5.97108D-01

At iterate   50    f=  4.16470D+02    |proj g|=  7.78992D-01

At iterate   50    f=  4.15898D+02    |proj g|=  1.36825D+00

At iterate   50    f=  4.18602D+02    |proj g|=  7.85935D-01

At iterate   50    f=  4.18025D+02    |proj g|=  1.54423D+00

At iterate   50    f=  4.18451D+02    |proj g|=  3.31826D+00

At iterate  100    f=  4.13766D+02    |proj g|=  1.76651D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   1.767D-01   4.138D+02
  F =   413.76628431380425     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  4.13405D+02    |proj g|=  1.36932D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   1.369D+00   4.134D+02
  F =   413.40536325667222     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.39759D+02    |proj g|=  1.36932D+00

At iterate    0    f=  1.42766D+02    |proj g|=  1.76651D-01

At iterate  100    f=  4.15969D+02    |proj g|=  4.42674D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   4.427D-01   4.160D+02
  F =   415.96901903211062     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.44825D+02    |proj g|=  4.42674D-01

At iterate  100    f=  4.14173D+02    |proj g|=  8.61529D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   8.615D-02   4.142D+02
  F =   414.17333297325689     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.42945D+02    |proj g|=  8.61529D-02

At iterate  100    f=  4.15431D+02    |proj g|=  2.01702D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   2.017D-01   4.154D+02
  F =   1816.5037687136200     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.36619D+02    |proj g|=  1.26205D+00

At iterate  100    f=  1.81743D+03    |proj g|=  6.37140D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   6.371D-01   1.817D+03
  F =   1817.4321557236456     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  1.81989D+03    |proj g|=  2.05803D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   2.058D+00   1.820D+03
  F =   1819.8929697490830     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.40263D+02    |proj g|=  6.37140D-01

At iterate    0    f=  7.45994D+02    |proj g|=  2.05803D+00

At iterate  100    f=  1.82870D+03    |proj g|=  3.02789D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   3.028D+00   1.829D+03
  F =   1828.6995574890448     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.36902D+02    |proj g|=  3.02789D+00

At iterate  100    f=  1.82365D+03    |proj g|=  1.18064D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   1.181D+00   1.824D+03
  F =   1823.6455842230912     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.36498D+02    |proj g|=  1.18064D+00

At iterate   50    f=  4.19010D+02    |proj g|=  8.42632D-01

At iterate   50    f=  4.19081D+02    |proj g|=  5.61631D-01

At iterate   50    f=  4.19673D+02    |proj g|=  1.06614D+00

At iterate   50    f=  4.19430D+02    |proj g|=  2.81436D-01

At iterate   50    f=  4.18408D+02    |proj g|=  9.86517D-01

At iterate  100    f=  4.15610D+02    |proj g|=  9.91660D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   9.917D-02   4.156D+02
  F =   415.60958557367860     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.41929D+02    |proj g|=  9.91660D-02

At iterate  100    f=  4.15930D+02    |proj g|=  3.10200D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   3.102D-01   4.159D+02
  F =   415.92984799037060     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.43952D+02    |proj g|=  3.10200D-01

At iterate  100    f=  4.16842D+02    |proj g|=  2.61035D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   2.610D-01   4.168D+02
  F =   416.84195083981706     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  4.17494D+02    |proj g|=  3.04785D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   3.048D-01   4.175D+02
  F =   417.49356163905941     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.50768D+02    |proj g|=  2.61035D-01

At iterate    0    f=  1.44410D+02    |proj g|=  3.04785D-01

At iterate  100    f=  4.16134D+02    |proj g|=  3.85977D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    105      1     0     0   3.860D-01   4.161D+02
  F =   416.13412931791726     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.35957D+02    |proj g|=  2.84277D+00

At iterate  100    f=  1.81880D+03    |proj g|=  1.92321D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   1.923D+00   1.819D+03
  F =   1818.8016475292129     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  1.80947D+03    |proj g|=  3.08915D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   3.089D+00   1.809D+03
  F =   1809.4675528360408     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.08087D+02    |proj g|=  1.92321D+00
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.29050D+02    |proj g|=  3.08915D+00

At iterate  100    f=  1.82673D+03    |proj g|=  5.59665D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   5.597D-01   1.827D+03
  F =   1826.7299050908898     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.34956D+02    |proj g|=  5.59665D-01

At iterate  100    f=  1.81909D+03    |proj g|=  2.68051D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   2.681D+00   1.819D+03
  F =   1819.0875085794805     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.42928D+02    |proj g|=  2.68051D+00

At iterate   50    f=  4.19126D+02    |proj g|=  9.45301D-01

At iterate   50    f=  4.15407D+02    |proj g|=  9.53381D-01

At iterate   50    f=  4.18480D+02    |proj g|=  3.68405D-01

At iterate   50    f=  4.19266D+02    |proj g|=  1.12490D+00

At iterate   50    f=  4.19849D+02    |proj g|=  1.59279D+00

At iterate  100    f=  4.17116D+02    |proj g|=  2.70718D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   2.707D-01   4.171D+02
  F =   417.11622876677370     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.42397D+02    |proj g|=  2.70718D-01

At iterate  100    f=  4.12946D+02    |proj g|=  1.33814D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   1.338D+00   4.129D+02
  F =   412.94628201031969     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate  100    f=  4.16038D+02    |proj g|=  7.64325D-01

At iterate    0    f=  1.41556D+02    |proj g|=  1.33814D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   7.643D-01   4.160D+02
  F =   416.03818513063209     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.40931D+02    |proj g|=  7.64325D-01

At iterate  100    f=  4.16317D+02    |proj g|=  1.02483D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   1.025D+00   4.163D+02
  F =   416.31656905291783     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.42663D+02    |proj g|=  1.02483D+00

At iterate  100    f=  4.16285D+02    |proj g|=  8.13006D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   8.130D-01   4.163D+02
  F =   416.28486512029269     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.26509D+02    |proj g|=  5.20406D+00

At iterate  100    f=  1.81959D+03    |proj g|=  1.15445D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   1.154D+00   1.820D+03
  F =   1819.5855297662520     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.47662D+02    |proj g|=  1.15445D+00

At iterate  100    f=  1.82746D+03    |proj g|=  4.13900D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   4.139D-01   1.827D+03
  F =   1827.4637730527534     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate  100    f=  1.81602D+03    |proj g|=  1.33442D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   1.334D+00   1.816D+03
  F =   1816.0193849544173     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate    0    f=  7.46612D+02    |proj g|=  4.13900D-01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.38931D+02    |proj g|=  1.33442D+00

At iterate   50    f=  4.18823D+02    |proj g|=  2.90479D+00

At iterate   50    f=  4.17383D+02    |proj g|=  1.32018D+00

At iterate   50    f=  4.16233D+02    |proj g|=  2.36226D-01

At iterate   50    f=  4.19000D+02    |proj g|=  1.12033D+00

At iterate   50    f=  4.14630D+02    |proj g|=  3.68613D-01

At iterate  100    f=  4.15066D+02    |proj g|=  4.69223D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   4.692D-01   4.151D+02
  F =   415.06601706103595     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  4.15026D+02    |proj g|=  4.15231D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    105      1     0     0   4.152D-01   4.150D+02
  F =   415.02637683082236     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.42183D+02    |proj g|=  4.69223D-01

At iterate    0    f=  1.42998D+02    |proj g|=  4.15231D-01

At iterate  100    f=  4.16539D+02    |proj g|=  6.47986D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   6.480D-02   4.165D+02
  F =   416.53852391386681     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  4.14456D+02    |proj g|=  5.15296D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    114      1     0     0   5.153D-02   4.145D+02
  F =   414.45628015585635     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate  100    f=  4.13040D+02    |proj g|=  8.11796D-02

At iterate    0    f=  1.44679D+02    |proj g|=  6.47986D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   8.118D-02   4.130D+02
  F =   413.03960839010824     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate    0    f=  1.41538D+02    |proj g|=  5.15296D-02
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.43913D+02    |proj g|=  8.11796D-02

At iterate   50    f=  8.53784D+01    |proj g|=  1.38890D-01

At iterate   50    f=  8.51679D+01    |proj g|=  3.86542D-01

At iterate   50    f=  8.54001D+01    |proj g|=  1.20670D-01

At iterate   50    f=  8.48549D+01    |proj g|=  3.65033D-02

At iterate   50    f=  8.49888D+01    |proj g|=  4.89233D-02

At iterate  100    f=  8.48107D+01    |proj g|=  3.98538D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 11.8min finished
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 11.8min finished
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 11.8min finished
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 11.8min finished
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 11.8min finished
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
	accuracy: 5-fold cross validation: [0.4184 0.4236 0.4178 0.4234 0.4154]
	test accuracy: 5-fold cross validation accuracy: 0.42 (+/- 0.01)
dimensionality: 74535
density: 1.000000



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.40532
	accuracy score (normalize=False):  10133

compute the precision
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 12.1min finished
	precision score (average=macro):  0.31843769562873836
	precision score (average=micro):  0.40532
	precision score (average=weighted):  0.3595998308997645
	precision score (average=None):  [0.54411765 0.20325901 0.24628198 0.29514925 0.29513695 0.25720789
 0.21052632 0.49582253]
	precision score (average=None, zero_division=1):  [0.54411765 0.20325901 0.24628198 0.29514925 0.29513695 0.25720789
 0.21052632 0.49582253]

compute the precision
	recall score (average=macro):  0.32240123915759383
	recall score (average=micro):  0.40532
	recall score (average=weighted):  0.40532
	recall score (average=None):  [0.75886101 0.10295395 0.16292798 0.30018975 0.22886866 0.23789474
 0.09897611 0.68853771]
	recall score (average=None, zero_division=1):  [0.75886101 0.10295395 0.16292798 0.30018975 0.22886866 0.23789474
 0.09897611 0.68853771]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.3100465654944375
	f1 score (average=micro):  0.40531999999999996
	f1 score (average=weighted):  0.37107775002769294
	f1 score (average=None):  [0.63379345 0.1366782  0.19611559 0.29764817 0.2578125  0.24717463
 0.13464887 0.57650113]

compute the F-beta score
	f beta score (average=macro):  0.31194268821602805
	f beta score (average=micro):  0.40532
	f beta score (average=weighted):  0.3611749596229165
	f beta score (average=None):  [0.57676009 0.17011197 0.22342148 0.29614377 0.2789813  0.2530984
 0.17180095 0.52522355]

compute the average Hamming loss
	hamming loss:  0.59468

jaccard similarity coefficient score
	jaccard score (average=macro):  0.19837415012445714
	jaccard score (average=None):  [0.46390749 0.0733519  0.10871849 0.17484527 0.14798206 0.14101498
 0.07218419 0.40498882]


================================================================================
Classifier.MLP_CLASSIFIER
________________________________________________________________________________
Training: 
MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_fun=15000, max_iter=200,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=0, shuffle=True, solver='adam',
              tol=0.0001, validation_fraction=0.1, verbose=True,
              warm_start=False)
Iteration 1, loss = 1.88821954
Iteration 2, loss = 1.38074371
Iteration 3, loss = 0.99497041
Iteration 4, loss = 0.66169230
Iteration 5, loss = 0.40625200
Iteration 6, loss = 0.24563625
Iteration 7, loss = 0.15386605
Iteration 8, loss = 0.10187055
Iteration 9, loss = 0.07194523
Iteration 10, loss = 0.05388829
Iteration 11, loss = 0.04236856
Iteration 12, loss = 0.03470406
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.40550D+02    |proj g|=  8.13006D-01

At iterate   50    f=  8.67494D+01    |proj g|=  4.09371D-02

At iterate   50    f=  8.72447D+01    |proj g|=  1.74901D-01

At iterate   50    f=  8.45782D+01    |proj g|=  2.36902D-01

At iterate   50    f=  8.52955D+01    |proj g|=  1.36983D-01

At iterate   50    f=  8.62595D+01    |proj g|=  9.47170D-02

At iterate  100    f=  8.62772D+01    |proj g|=  5.19805D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   5.198D-02   8.628D+01
  F =   86.277236191179753     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.61424D+01    |proj g|=  4.42525D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    105      1     0     0   4.425D-02   8.614D+01
  F =   86.142436600719137     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.43270D+01    |proj g|=  8.13311D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   8.133D-02   8.433D+01
  F =   84.327007341955806     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.49832D+01    |proj g|=  3.30747D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   3.307D-02   8.498D+01
  F =   84.983157950822758     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.60607D+01    |proj g|=  7.47540D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   7.475D-02   8.606D+01
  F =   86.060682133720348     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
  F =   415.43073266277679     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.41415D+02    |proj g|=  2.01702D-01

At iterate   50    f=  8.46896D+01    |proj g|=  1.29727D-01

At iterate   50    f=  8.65246D+01    |proj g|=  8.81948D-02

At iterate   50    f=  8.48602D+01    |proj g|=  1.08252D-01

At iterate   50    f=  8.60453D+01    |proj g|=  1.03014D-01

At iterate   50    f=  8.65485D+01    |proj g|=  5.83001D-02

At iterate  100    f=  8.43828D+01    |proj g|=  8.19168D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   8.192D-02   8.438D+01
  F =   84.382829868470580     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.60908D+01    |proj g|=  8.94713D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    105      1     0     0   8.947D-03   8.609D+01
  F =   86.090783664539970     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.44921D+01    |proj g|=  8.54150D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   8.542D-02   8.449D+01
  F =   84.492083343576041     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.56651D+01    |proj g|=  5.39240D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   5.392D-02   8.567D+01
  F =   85.665134067260766     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.59874D+01    |proj g|=  1.04297D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   1.043D-01   8.599D+01
  F =   85.987397579327038     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.41848D+02    |proj g|=  3.85977D-01

At iterate   50    f=  8.62187D+01    |proj g|=  5.18329D-02

At iterate   50    f=  8.62517D+01    |proj g|=  4.31277D-02

At iterate   50    f=  8.56589D+01    |proj g|=  3.21148D-01

At iterate   50    f=  8.64205D+01    |proj g|=  9.23819D-02

At iterate   50    f=  8.51295D+01    |proj g|=  4.84839D-02

At iterate  100    f=  8.59642D+01    |proj g|=  2.48118D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   2.481D-02   8.596D+01
  F =   85.964223083873094     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.60312D+01    |proj g|=  5.22764D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   5.228D-02   8.603D+01
  F =   86.031231360704396     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.62105D+01    |proj g|=  4.70300D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    110      1     0     0   4.703D-02   8.621D+01
  F =   86.210549855890690     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.52467D+01    |proj g|=  2.94414D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   2.944D-02   8.525D+01
  F =   85.246732066332072     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.49254D+01    |proj g|=  4.54216D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   4.542D-02   8.493D+01
  F =   84.925376127729464     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
At iterate    0    f=  1.44893D+02    |proj g|=  8.65806D-01
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       596288     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.42012D+02    |proj g|=  4.19035D-01

At iterate   50    f=  8.50127D+01    |proj g|=  1.18312D-01

At iterate   50    f=  8.58619D+01    |proj g|=  8.02181D-02

At iterate   50    f=  8.49129D+01    |proj g|=  1.61720D-01

At iterate   50    f=  8.57305D+01    |proj g|=  2.24649D-02

At iterate   50    f=  8.63555D+01    |proj g|=  2.43467D-01

At iterate  100    f=  8.46596D+01    |proj g|=  1.47508D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   1.475D-02   8.466D+01
  F =   84.659578011594249     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.55688D+01    |proj g|=  4.69730D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    111      1     0     0   4.697D-02   8.557D+01
  F =   85.568807397146600     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.45232D+01    |proj g|=  2.42685D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   2.427D-02   8.452D+01
  F =   84.523200801027656     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.55416D+01    |proj g|=  9.34806D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    108      1     0     0   9.348D-02   8.554D+01
  F =   85.541606581011621     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.60454D+01    |proj g|=  1.26509D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   1.265D-01   8.605D+01
  F =   86.045449197895820     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
[LibLinear]Projg = norm of the final projected gradient
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    109      1     0     0   3.985D-02   8.481D+01
  F =   84.810705933262227     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.48242D+01    |proj g|=  2.88870D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    105      1     0     0   2.889D-02   8.482D+01
  F =   84.824209190303492     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.51214D+01    |proj g|=  2.41090D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    107      1     0     0   2.411D-02   8.512D+01
  F =   85.121435672331870     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.44361D+01    |proj g|=  5.04599D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    106      1     0     0   5.046D-02   8.444D+01
  F =   84.436138601325396     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 

At iterate  100    f=  8.46509D+01    |proj g|=  2.21606D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    100    113      1     0     0   2.216D-02   8.465D+01
  F =   84.650881695781408     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
Iteration 13, loss = 0.02940594
Iteration 14, loss = 0.02567412
Iteration 15, loss = 0.02281451
Iteration 16, loss = 0.02067803
Iteration 17, loss = 0.01896610
Iteration 18, loss = 0.01764399
Iteration 19, loss = 0.01658385
Iteration 20, loss = 0.01565647
Iteration 21, loss = 0.01492700
Iteration 22, loss = 0.01435956
Iteration 23, loss = 0.01384673
Iteration 24, loss = 0.01337927
Iteration 25, loss = 0.01290557
Iteration 26, loss = 0.01258513
Iteration 27, loss = 0.01215420
Iteration 28, loss = 0.01190445
Iteration 29, loss = 0.01157396
Iteration 30, loss = 0.01127146
Iteration 31, loss = 0.01102765
Iteration 32, loss = 0.01080103
Iteration 33, loss = 0.01057912
Iteration 34, loss = 0.01041932
Iteration 35, loss = 0.01016635
Iteration 36, loss = 0.00998523
Iteration 37, loss = 0.00982923
Iteration 38, loss = 0.00966837
Iteration 39, loss = 0.00943278
Iteration 40, loss = 0.00935212
Iteration 41, loss = 0.00910540
Iteration 42, loss = 0.00892370
Iteration 43, loss = 0.00876417
Iteration 44, loss = 0.00870520
Iteration 45, loss = 0.00846627
Iteration 46, loss = 0.00847361
Iteration 47, loss = 0.00867171
Iteration 48, loss = 0.00808645
Iteration 49, loss = 0.00795000
Iteration 50, loss = 0.00771547
Iteration 51, loss = 0.00756760
Iteration 52, loss = 0.00743549
Iteration 53, loss = 0.00718323
Iteration 54, loss = 0.00716503
Iteration 55, loss = 0.00705607
Iteration 56, loss = 0.00741243
Iteration 57, loss = 0.00686271
Iteration 58, loss = 0.00685018
Iteration 59, loss = 0.00661466
Iteration 60, loss = 0.00644355
Iteration 61, loss = 0.00636596
Iteration 62, loss = 0.00632173
Iteration 63, loss = 0.00690166
Iteration 64, loss = 0.00628380
Iteration 65, loss = 0.00627582
Iteration 66, loss = 0.00635025
Iteration 67, loss = 0.00608425
Iteration 68, loss = 0.00579360
Iteration 69, loss = 0.00579813
Iteration 70, loss = 0.00572884
Iteration 71, loss = 0.00552667
Iteration 72, loss = 0.00610480
Iteration 73, loss = 0.00626026
Iteration 74, loss = 0.00566808
Iteration 75, loss = 0.00590936
Iteration 76, loss = 0.00536676
Iteration 77, loss = 0.00528492
Iteration 78, loss = 0.00620725
Iteration 79, loss = 0.00522557
Iteration 80, loss = 0.00528428
Iteration 81, loss = 0.00527695
Iteration 82, loss = 0.00505894
Iteration 83, loss = 0.00502477
Iteration 84, loss = 0.00484341
Iteration 85, loss = 0.00500680
Iteration 86, loss = 0.00548555
Iteration 87, loss = 0.00515078
Iteration 88, loss = 0.00511730
Iteration 89, loss = 0.00500726
Iteration 90, loss = 0.00544699
Iteration 91, loss = 0.00531749
Iteration 92, loss = 0.00550182
Iteration 93, loss = 0.00556722
Iteration 94, loss = 0.00574197
Iteration 95, loss = 0.00522414
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
train time: 2298.639s
test time:  0.161s
accuracy:   0.345


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
Iteration 1, loss = 1.93437509
Iteration 1, loss = 1.93862111
Iteration 1, loss = 1.94221200
Iteration 1, loss = 1.93191898
Iteration 1, loss = 1.93621782
Iteration 2, loss = 1.46695990
Iteration 2, loss = 1.47755238
Iteration 2, loss = 1.48083014
Iteration 2, loss = 1.46887996
Iteration 2, loss = 1.47412499
Iteration 3, loss = 1.06308593
Iteration 3, loss = 1.05784719
Iteration 3, loss = 1.06918583
Iteration 3, loss = 1.06685605
Iteration 3, loss = 1.06478650
Iteration 4, loss = 0.71011937
Iteration 4, loss = 0.71216728
Iteration 4, loss = 0.71686670
Iteration 4, loss = 0.71368582
Iteration 4, loss = 0.72031957
Iteration 5, loss = 0.44959707
Iteration 5, loss = 0.44207834
Iteration 5, loss = 0.44677037
Iteration 5, loss = 0.44584767
Iteration 5, loss = 0.45307818
Iteration 6, loss = 0.27778562
Iteration 6, loss = 0.27046887
Iteration 6, loss = 0.27355590
Iteration 6, loss = 0.27238488
Iteration 6, loss = 0.27842081
Iteration 7, loss = 0.17596885
Iteration 7, loss = 0.17065996
Iteration 7, loss = 0.17254493
Iteration 7, loss = 0.17073736
Iteration 7, loss = 0.17533014
Iteration 8, loss = 0.11697325
Iteration 8, loss = 0.11358244
Iteration 8, loss = 0.11480232
Iteration 8, loss = 0.11295113
Iteration 8, loss = 0.11605971
Iteration 9, loss = 0.08211967
Iteration 9, loss = 0.08000343
Iteration 9, loss = 0.08087217
Iteration 9, loss = 0.07908000
Iteration 9, loss = 0.08134743
Iteration 10, loss = 0.06102236
Iteration 10, loss = 0.05960440
Iteration 10, loss = 0.06021869
Iteration 10, loss = 0.05868888
Iteration 10, loss = 0.06025487
Iteration 11, loss = 0.04743766
Iteration 11, loss = 0.04653351
Iteration 11, loss = 0.04697667
Iteration 11, loss = 0.04573460
Iteration 11, loss = 0.04688796
Iteration 12, loss = 0.03837216
Iteration 12, loss = 0.03780626
Iteration 12, loss = 0.03813455
Iteration 12, loss = 0.03708766
Iteration 12, loss = 0.03795695
Iteration 13, loss = 0.03209399
Iteration 13, loss = 0.03171980
Iteration 13, loss = 0.03197008
Iteration 13, loss = 0.03109045
Iteration 13, loss = 0.03175146
Iteration 14, loss = 0.02759732
Iteration 14, loss = 0.02732950
Iteration 14, loss = 0.02750663
Iteration 14, loss = 0.02676074
Iteration 14, loss = 0.02728513
Iteration 15, loss = 0.02432016
Iteration 15, loss = 0.02411741
Iteration 15, loss = 0.02425256
Iteration 15, loss = 0.02354467
Iteration 15, loss = 0.02398764
Iteration 16, loss = 0.02173223
Iteration 16, loss = 0.02159824
Iteration 16, loss = 0.02170160
Iteration 16, loss = 0.02111670
Iteration 16, loss = 0.02144804
Iteration 17, loss = 0.01969867
Iteration 17, loss = 0.01959218
Iteration 17, loss = 0.01967170
Iteration 17, loss = 0.01918805
Iteration 17, loss = 0.01947428
Iteration 18, loss = 0.01813624
Iteration 18, loss = 0.01805511
Iteration 18, loss = 0.01811767
Iteration 18, loss = 0.01769727
Iteration 18, loss = 0.01792076
Iteration 19, loss = 0.01693282
Iteration 19, loss = 0.01686178
Iteration 19, loss = 0.01693431
Iteration 19, loss = 0.01647886
Iteration 19, loss = 0.01665577
Iteration 20, loss = 0.01590067
Iteration 20, loss = 0.01584751
Iteration 20, loss = 0.01590230
Iteration 20, loss = 0.01545421
Iteration 20, loss = 0.01562547
Iteration 21, loss = 0.01496082
Iteration 21, loss = 0.01492049
Iteration 21, loss = 0.01495808
Iteration 21, loss = 0.01461870
Iteration 21, loss = 0.01476332
Iteration 22, loss = 0.01427615
Iteration 22, loss = 0.01424047
Iteration 22, loss = 0.01427708
Iteration 22, loss = 0.01394555
Iteration 22, loss = 0.01403989
Iteration 23, loss = 0.01371252
Iteration 23, loss = 0.01366734
Iteration 23, loss = 0.01372321
Iteration 23, loss = 0.01336033
Iteration 23, loss = 0.01342069
Iteration 24, loss = 0.01306768
Iteration 24, loss = 0.01303281
Iteration 24, loss = 0.01306613
Iteration 24, loss = 0.01277927
Iteration 24, loss = 0.01288726
Iteration 25, loss = 0.01266895
Iteration 25, loss = 0.01263180
Iteration 25, loss = 0.01267635
Iteration 25, loss = 0.01234809
Iteration 25, loss = 0.01242632
Iteration 26, loss = 0.01221542
Iteration 26, loss = 0.01218699
Iteration 26, loss = 0.01221546
Iteration 26, loss = 0.01195786
Iteration 26, loss = 0.01202132
Iteration 27, loss = 0.01185605
Iteration 27, loss = 0.01182729
Iteration 27, loss = 0.01185051
Iteration 27, loss = 0.01160551
Iteration 27, loss = 0.01166279
Iteration 28, loss = 0.01157634
Iteration 28, loss = 0.01154264
Iteration 28, loss = 0.01157506
Iteration 28, loss = 0.01132187
Iteration 28, loss = 0.01134052
Iteration 29, loss = 0.01127090
Iteration 29, loss = 0.01124410
Iteration 29, loss = 0.01126088
Iteration 29, loss = 0.01105952
Iteration 29, loss = 0.01105187
Iteration 30, loss = 0.01101745
Iteration 30, loss = 0.01099497
Iteration 30, loss = 0.01100791
Iteration 30, loss = 0.01082103
Iteration 30, loss = 0.01078863
Iteration 31, loss = 0.01071656
Iteration 31, loss = 0.01069655
Iteration 31, loss = 0.01070553
Iteration 31, loss = 0.01052335
Iteration 31, loss = 0.01054752
Iteration 32, loss = 0.01052777
Iteration 32, loss = 0.01050874
Iteration 32, loss = 0.01051573
Iteration 32, loss = 0.01034513
Iteration 32, loss = 0.01032441
Iteration 33, loss = 0.01033866
Iteration 33, loss = 0.01031956
Iteration 33, loss = 0.01032801
Iteration 33, loss = 0.01015393
Iteration 33, loss = 0.01011907
Iteration 34, loss = 0.01006552
Iteration 34, loss = 0.01004519
Iteration 34, loss = 0.01005032
Iteration 34, loss = 0.00988616
Iteration 34, loss = 0.00992615
Iteration 35, loss = 0.00985833
Iteration 35, loss = 0.00984389
Iteration 35, loss = 0.00984991
Iteration 35, loss = 0.00968931
Iteration 35, loss = 0.00974376
Iteration 36, loss = 0.00979243
Iteration 36, loss = 0.00975800
Iteration 36, loss = 0.00978449
Iteration 36, loss = 0.00959063
Iteration 36, loss = 0.00957210
Iteration 37, loss = 0.00967095
Iteration 37, loss = 0.00957764
Iteration 37, loss = 0.00970059
Iteration 37, loss = 0.00940059
Iteration 37, loss = 0.00940732
Iteration 38, loss = 0.00967172
Iteration 38, loss = 0.00952710
Iteration 38, loss = 0.00963965
Iteration 38, loss = 0.00935192
Iteration 38, loss = 0.00925151
Iteration 39, loss = 0.00940632
Iteration 39, loss = 0.00936407
Iteration 39, loss = 0.00935953
Iteration 39, loss = 0.00912360
Iteration 39, loss = 0.00910059
Iteration 40, loss = 0.00932610
Iteration 40, loss = 0.00950703
Iteration 40, loss = 0.00926767
Iteration 40, loss = 0.00906053
Iteration 40, loss = 0.00895472
Iteration 41, loss = 0.00899395
Iteration 41, loss = 0.00903467
Iteration 41, loss = 0.00897309
Iteration 41, loss = 0.00883627
Iteration 41, loss = 0.00881462
Iteration 42, loss = 0.00895511
Iteration 42, loss = 0.00893894
Iteration 42, loss = 0.00893064
Iteration 42, loss = 0.00879802
Iteration 42, loss = 0.00867687
Iteration 43, loss = 0.00872384
Iteration 43, loss = 0.00871042
Iteration 43, loss = 0.00870550
Iteration 43, loss = 0.00857702
Iteration 43, loss = 0.00854448
Iteration 44, loss = 0.00868422
Iteration 44, loss = 0.00866712
Iteration 44, loss = 0.00866685
Iteration 44, loss = 0.00854118
Iteration 44, loss = 0.00841273
Iteration 45, loss = 0.00846118
Iteration 45, loss = 0.00844853
Iteration 45, loss = 0.00844497
Iteration 45, loss = 0.00832096
Iteration 45, loss = 0.00828493
Iteration 46, loss = 0.00845132
Iteration 46, loss = 0.00843297
Iteration 46, loss = 0.00844296
Iteration 46, loss = 0.00831072
Iteration 46, loss = 0.00816000
Iteration 47, loss = 0.00850093
Iteration 47, loss = 0.00847844
Iteration 47, loss = 0.00849808
Iteration 47, loss = 0.00836066
Iteration 47, loss = 0.00803588
Iteration 48, loss = 0.00806899
Iteration 48, loss = 0.00805592
Iteration 48, loss = 0.00805624
Iteration 48, loss = 0.00793184
Iteration 48, loss = 0.00791443
Iteration 49, loss = 0.00798719
Iteration 49, loss = 0.00798202
Iteration 49, loss = 0.00797376
Iteration 49, loss = 0.00786641
Iteration 49, loss = 0.00779379
Iteration 50, loss = 0.00796591
Iteration 50, loss = 0.00795619
Iteration 50, loss = 0.00795635
Iteration 50, loss = 0.00784755
Iteration 50, loss = 0.00767604
Iteration 51, loss = 0.00785099
Iteration 51, loss = 0.00784237
Iteration 51, loss = 0.00784092
Iteration 51, loss = 0.00773179
Iteration 51, loss = 0.00755953
Iteration 52, loss = 0.00772163
Iteration 52, loss = 0.00771239
Iteration 52, loss = 0.00771116
Iteration 52, loss = 0.00760263
Iteration 52, loss = 0.00744334
Iteration 53, loss = 0.00757713
Iteration 53, loss = 0.00757026
Iteration 53, loss = 0.00756934
Iteration 53, loss = 0.00746055
Iteration 53, loss = 0.00732962
Iteration 54, loss = 0.00739973
Iteration 54, loss = 0.00740639
Iteration 54, loss = 0.00739527
Iteration 54, loss = 0.00728495
Iteration 54, loss = 0.00721671
Iteration 55, loss = 0.00731517
Iteration 55, loss = 0.00732255
Iteration 55, loss = 0.00731061
Iteration 55, loss = 0.00720272
Iteration 55, loss = 0.00710547
Iteration 56, loss = 0.00729008
Iteration 56, loss = 0.00729585
Iteration 56, loss = 0.00729011
Iteration 56, loss = 0.00719227
Iteration 56, loss = 0.00699468
Iteration 57, loss = 0.00707409
Iteration 57, loss = 0.00707781
Iteration 57, loss = 0.00707095
Iteration 57, loss = 0.00696667
Iteration 57, loss = 0.00688549
Iteration 58, loss = 0.00703649
Iteration 58, loss = 0.00704340
Iteration 58, loss = 0.00703500
Iteration 58, loss = 0.00694038
Iteration 58, loss = 0.00677798
Iteration 59, loss = 0.00688947
Iteration 59, loss = 0.00689442
Iteration 59, loss = 0.00688583
Iteration 59, loss = 0.00678281
Iteration 59, loss = 0.00667156
Iteration 60, loss = 0.00677375
Iteration 60, loss = 0.00677830
Iteration 60, loss = 0.00677036
Iteration 60, loss = 0.00666866
Iteration 60, loss = 0.00656646
Iteration 61, loss = 0.00668368
Iteration 61, loss = 0.00668829
Iteration 61, loss = 0.00667884
Iteration 61, loss = 0.00657900
Iteration 61, loss = 0.00646299
Iteration 62, loss = 0.00659308
Iteration 62, loss = 0.00659038
Iteration 62, loss = 0.00658329
Iteration 62, loss = 0.00648304
Iteration 62, loss = 0.00635976
Iteration 63, loss = 0.00648749
Iteration 63, loss = 0.00648773
Iteration 63, loss = 0.00648251
Iteration 63, loss = 0.00638507
Iteration 63, loss = 0.00625906
Iteration 64, loss = 0.00638917
Iteration 64, loss = 0.00638744
Iteration 64, loss = 0.00638247
Iteration 64, loss = 0.00628447
Iteration 64, loss = 0.00615941
Iteration 65, loss = 0.00631431
Iteration 65, loss = 0.00631306
Iteration 65, loss = 0.00631271
Iteration 65, loss = 0.00621142
Iteration 65, loss = 0.00606170
Iteration 66, loss = 0.00611773
Iteration 66, loss = 0.00611758
Iteration 66, loss = 0.00611368
Iteration 66, loss = 0.00601545
Iteration 66, loss = 0.00596535
Iteration 67, loss = 0.00627436
Iteration 67, loss = 0.00628634
Iteration 67, loss = 0.00627344
Iteration 67, loss = 0.00618114
Iteration 67, loss = 0.00586974
Iteration 68, loss = 0.00618284
Iteration 68, loss = 0.00619090
Iteration 68, loss = 0.00617991
Iteration 68, loss = 0.00608650
Iteration 68, loss = 0.00577732
Iteration 69, loss = 0.00597999
Iteration 69, loss = 0.00598159
Iteration 69, loss = 0.00597788
Iteration 69, loss = 0.00588167
Iteration 69, loss = 0.00568505
Iteration 70, loss = 0.00583244
Iteration 70, loss = 0.00583415
Iteration 70, loss = 0.00583295
Iteration 70, loss = 0.00573431
Iteration 70, loss = 0.00559424
Iteration 71, loss = 0.00579479
Iteration 71, loss = 0.00579337
Iteration 71, loss = 0.00578943
Iteration 71, loss = 0.00569601
Iteration 71, loss = 0.00550661
Iteration 72, loss = 0.00572074
Iteration 72, loss = 0.00571794
Iteration 72, loss = 0.00574482
Iteration 72, loss = 0.00562218
Iteration 72, loss = 0.00542037
Iteration 73, loss = 0.00563590
Iteration 73, loss = 0.00563104
Iteration 73, loss = 0.00686690
Iteration 73, loss = 0.00553470
Iteration 73, loss = 0.00533352
Iteration 74, loss = 0.00574644
Iteration 74, loss = 0.00562406
Iteration 74, loss = 0.00580267
Iteration 74, loss = 0.00553185
Iteration 74, loss = 0.00525175
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 75, loss = 0.00757723
Iteration 75, loss = 0.00558645
Iteration 75, loss = 0.00582403
Iteration 75, loss = 0.00549409
Iteration 76, loss = 0.00547512
Iteration 76, loss = 0.00640592
Iteration 76, loss = 0.00555731
Iteration 76, loss = 0.00527879
Iteration 77, loss = 0.00548350
Iteration 77, loss = 0.00558774
Iteration 77, loss = 0.00549636
Iteration 77, loss = 0.00535142
Iteration 78, loss = 0.00562429
Iteration 78, loss = 0.00590326
Iteration 78, loss = 0.00540197
Iteration 78, loss = 0.00526653
Iteration 79, loss = 0.00629231
Iteration 79, loss = 0.00542830
Iteration 79, loss = 0.00525741
Iteration 79, loss = 0.00517403
Iteration 80, loss = 0.00530049
Iteration 80, loss = 0.00529935
Iteration 80, loss = 0.00521367
Iteration 80, loss = 0.00511543
Iteration 81, loss = 0.00524614
Iteration 81, loss = 0.00543902
Iteration 81, loss = 0.00522513
Iteration 81, loss = 0.00513647
Iteration 82, loss = 0.00553809
Iteration 82, loss = 0.00582652
Iteration 82, loss = 0.00573194
Iteration 82, loss = 0.00593886
Iteration 83, loss = 0.00554676
Iteration 83, loss = 0.00583639
Iteration 83, loss = 0.00546919
Iteration 83, loss = 0.00508651
Iteration 84, loss = 0.00510824
Iteration 84, loss = 0.00577228
Iteration 84, loss = 0.00511564
Iteration 84, loss = 0.00498765
Iteration 85, loss = 0.00501322
Iteration 85, loss = 0.00509668
Iteration 85, loss = 0.00505873
Iteration 85, loss = 0.00491921
Iteration 86, loss = 0.00481397
Iteration 86, loss = 0.00485211
Iteration 86, loss = 0.00582236
Iteration 86, loss = 0.00542878
Iteration 87, loss = 0.00476267
Iteration 87, loss = 0.00477759
Iteration 87, loss = 0.00479555
Iteration 87, loss = 0.00522733
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 88, loss = 0.00475799
Iteration 88, loss = 0.00475896
Iteration 88, loss = 0.00476140
Iteration 89, loss = 0.00470218
Iteration 89, loss = 0.00469614
Iteration 89, loss = 0.00468505
Iteration 90, loss = 0.00466797
Iteration 90, loss = 0.00466000
Iteration 90, loss = 0.00465040
Iteration 91, loss = 0.00465984
Iteration 91, loss = 0.00465122
Iteration 91, loss = 0.00464401
Iteration 92, loss = 0.00456750
Iteration 92, loss = 0.00455496
Iteration 92, loss = 0.00454812
Iteration 93, loss = 0.00473407
Iteration 93, loss = 0.00468096
Iteration 93, loss = 0.00468366
Iteration 94, loss = 0.00450317
Iteration 94, loss = 0.00448892
Iteration 94, loss = 0.00473593
Iteration 95, loss = 0.00447647
Iteration 95, loss = 0.00443612
Iteration 95, loss = 0.00508007
Iteration 96, loss = 0.00442637
Iteration 96, loss = 0.00440906
Iteration 96, loss = 0.00484614
Iteration 97, loss = 0.00456095
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 97, loss = 0.00534328
Iteration 97, loss = 0.00454200
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 98, loss = 0.00511085
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 60.0min finished
	accuracy: 5-fold cross validation: [0.3858 0.3784 0.3808 0.3732 0.371 ]
	test accuracy: 5-fold cross validation accuracy: 0.38 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.34468
	accuracy score (normalize=False):  8617

compute the precision
	precision score (average=macro):  0.2878720295131821
	precision score (average=micro):  0.34468
	precision score (average=weighted):  0.3339036429399172
	precision score (average=None):  [0.54722774 0.17453505 0.20388788 0.26243828 0.21177999 0.22483102
 0.20038351 0.47789276]
	precision score (average=None, zero_division=1):  [0.54722774 0.17453505 0.20388788 0.26243828 0.21177999 0.22483102
 0.20038351 0.47789276]

compute the precision
	recall score (average=macro):  0.290276621070899
	recall score (average=micro):  0.34468
	recall score (average=weighted):  0.34468
	recall score (average=None):  [0.60334528 0.15899218 0.17748918 0.26223909 0.21196359 0.22175439
 0.17832765 0.50810162]
	recall score (average=None, zero_division=1):  [0.60334528 0.15899218 0.17748918 0.26223909 0.21196359 0.22175439
 0.17832765 0.50810162]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.2886043200737112
	f1 score (average=micro):  0.34468
	f1 score (average=weighted):  0.3387367313380867
	f1 score (average=None):  [0.57391798 0.16640145 0.18977488 0.26233865 0.21187175 0.22328211
 0.18871332 0.49253442]

compute the F-beta score
	f beta score (average=macro):  0.2880500610233135
	f beta score (average=micro):  0.34467999999999993
	f beta score (average=weighted):  0.33570514019174036
	f beta score (average=None):  [0.55760029 0.17118803 0.19799807 0.26239842 0.21181669 0.22420888
 0.19554641 0.4836437 ]

compute the average Hamming loss
	hamming loss:  0.65532

jaccard similarity coefficient score
	jaccard score (average=macro):  0.17800988445007912
	jaccard score (average=None):  [0.40244388 0.0907513  0.10483496 0.15097225 0.11848801 0.12567111
 0.10418744 0.32673013]


================================================================================
Classifier.MULTINOMIAL_NB
________________________________________________________________________________
Training: 
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
train time: 0.031s
test time:  0.017s
accuracy:   0.349


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s finished
	accuracy: 5-fold cross validation: [0.3516 0.3552 0.3528 0.3494 0.3544]
	test accuracy: 5-fold cross validation accuracy: 0.35 (+/- 0.00)
dimensionality: 74535
density: 1.000000



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.34924
	accuracy score (normalize=False):  8731

compute the precision
	precision score (average=macro):  0.29209389818025067
	precision score (average=micro):  0.34924
/home/rpessoa/virtual_envs/comp551_p2/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
	precision score (average=weighted):  0.31926931735877995
	precision score (average=None):  [0.32342426 0.         0.         1.         0.         0.625
 0.         0.38832693]
	precision score (average=None, zero_division=1):  [0.32342426 1.         1.         1.         1.         0.625
 1.         0.38832693]

compute the precision
	recall score (average=macro):  0.21789837772820064
	recall score (average=micro):  0.34924
	recall score (average=weighted):  0.34924
	recall score (average=None):  [9.72720032e-01 0.00000000e+00 0.00000000e+00 7.59013283e-04
 0.00000000e+00 1.75438596e-03 0.00000000e+00 7.67953591e-01]
	recall score (average=None, zero_division=1):  [9.72720032e-01 0.00000000e+00 0.00000000e+00 7.59013283e-04
 0.00000000e+00 1.75438596e-03 0.00000000e+00 7.67953591e-01]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.12578485491762437
	f1 score (average=micro):  0.34924
	f1 score (average=weighted):  0.20121791760295146
	f1 score (average=None):  [0.48544172 0.         0.         0.00151688 0.         0.00349895
 0.         0.5158213 ]

compute the F-beta score
	f beta score (average=macro):  0.10208051148642841
	f beta score (average=micro):  0.34924
	f beta score (average=weighted):  0.1625361083717585
	f beta score (average=None):  [0.37325407 0.         0.         0.00378358 0.         0.00867453
 0.         0.43093191]

compute the average Hamming loss
	hamming loss:  0.65076

jaccard similarity coefficient score
	jaccard score (average=macro):  0.08382190051516342
	jaccard score (average=None):  [0.32051703 0.         0.         0.00075901 0.         0.00175254
 0.         0.34754662]


================================================================================
Classifier.NEAREST_CENTROID
________________________________________________________________________________
Training: 
NearestCentroid(metric='euclidean', shrink_threshold=None)
train time: 0.027s
test time:  0.021s
accuracy:   0.368


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished
	accuracy: 5-fold cross validation: [0.3718 0.377  0.3638 0.3736 0.3756]
	test accuracy: 5-fold cross validation accuracy: 0.37 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.36844
	accuracy score (normalize=False):  9211

compute the precision
	precision score (average=macro):  0.32251053942641617
	precision score (average=micro):  0.36844
	precision score (average=weighted):  0.3753277948275127
	precision score (average=None):  [0.62229905 0.20615243 0.23474369 0.27329624 0.26476339 0.23076923
 0.20385233 0.54420794]
	precision score (average=None, zero_division=1):  [0.62229905 0.20615243 0.23474369 0.27329624 0.26476339 0.23076923
 0.20385233 0.54420794]

compute the precision
	recall score (average=macro):  0.3216024250764673
	recall score (average=micro):  0.36844
	recall score (average=weighted):  0.36844
	recall score (average=None):  [0.5734767  0.19504778 0.22707595 0.29829222 0.2934547  0.23684211
 0.21672355 0.53190638]
	recall score (average=None, zero_division=1):  [0.5734767  0.19504778 0.22707595 0.29829222 0.2934547  0.23684211
 0.21672355 0.53190638]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.32170590666228027
	f1 score (average=micro):  0.36844
	f1 score (average=weighted):  0.37150039358722875
	f1 score (average=None):  [0.59689119 0.20044643 0.23084617 0.28524769 0.27837171 0.23376623
 0.21009098 0.53798685]

compute the F-beta score
	f beta score (average=macro):  0.3221054391980286
	f beta score (average=micro):  0.36844
	f beta score (average=weighted):  0.37370379952814314
	f beta score (average=None):  [0.61188068 0.20383149 0.233169   0.27795459 0.27004388 0.23195876
 0.20630279 0.54170232]

compute the average Hamming loss
	hamming loss:  0.63156

jaccard similarity coefficient score
	jaccard score (average=macro):  0.20162774722487914
	jaccard score (average=None):  [0.4254062  0.11138675 0.13048394 0.16634921 0.16169095 0.13235294
 0.11737523 0.36797675]


================================================================================
Classifier.NU_SVC
________________________________________________________________________________
Training: 
NuSVC(break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
      max_iter=-1, nu=0.5, probability=False, random_state=0, shrinking=True,
      tol=0.001, verbose=True)
..........*
optimization finished, #iter = 10639
C = 1.873700
obj = 3374.687991, rho = -0.294802
nSV = 7028, nBSV = 466
.........*
optimization finished, #iter = 9816
C = 1.626149
obj = 2937.502954, rho = -0.281922
nSV = 6752, nBSV = 626
.........*
optimization finished, #iter = 9273
C = 1.406271
obj = 2576.353944, rho = -0.176354
nSV = 6593, nBSV = 940
......*
optimization finished, #iter = 6569
C = 0.801415
obj = 1296.801867, rho = -0.279008
nSV = 5566, nBSV = 1914
.......*
optimization finished, #iter = 7178
C = 0.784839
obj = 1372.901620, rho = -0.129376
nSV = 5946, nBSV = 1998
.....*
optimization finished, #iter = 5804
C = 0.712190
obj = 1100.163805, rho = -0.281573
nSV = 5235, nBSV = 2060
.......*
optimization finished, #iter = 7746
C = 0.699238
obj = 1438.793268, rho = 0.127067
nSV = 7070, nBSV = 2708
.......*
optimization finished, #iter = 7200
C = 2.326902
obj = 2735.638171, rho = -0.021450
nSV = 4689, nBSV = 3
.......*
optimization finished, #iter = 7845
C = 2.123547
obj = 2641.054803, rho = 0.064896
nSV = 4939, nBSV = 17
......*
optimization finished, #iter = 6366
C = 1.439193
obj = 1686.639978, rho = -0.034043
nSV = 4463, nBSV = 270
......*
optimization finished, #iter = 6860
C = 1.262909
obj = 1611.918427, rho = 0.101039
nSV = 4823, nBSV = 505
.....*
optimization finished, #iter = 5710
C = 1.267520
obj = 1402.515370, rho = -0.003011
nSV = 4153, nBSV = 338
......*
optimization finished, #iter = 6712
C = 0.840687
obj = 1255.692341, rho = 0.384719
nSV = 5545, nBSV = 1638
.......*
optimization finished, #iter = 7906
C = 2.357481
obj = 3012.501338, rho = 0.094116
nSV = 5102, nBSV = 4
......*
optimization finished, #iter = 6915
C = 1.660735
obj = 2023.137506, rho = -0.054381
nSV = 4736, nBSV = 137
.......*
optimization finished, #iter = 7499
C = 1.446394
obj = 1924.123345, rho = 0.080891
nSV = 5095, nBSV = 328
......*
optimization finished, #iter = 6126
C = 1.401072
obj = 1609.874692, rho = -0.024211
nSV = 4335, nBSV = 266
.......*
optimization finished, #iter = 7371
C = 0.965383
obj = 1528.219746, rho = 0.339050
nSV = 5770, nBSV = 1431
.......*
optimization finished, #iter = 7506
C = 1.829153
obj = 2363.456723, rho = -0.160545
nSV = 5091, nBSV = 75
........*
optimization finished, #iter = 8050
C = 1.598838
obj = 2253.899881, rho = -0.007228
nSV = 5439, nBSV = 210
......*
optimization finished, #iter = 6791
C = 1.483856
obj = 1809.852492, rho = -0.117972
nSV = 4679, nBSV = 235
........*
optimization finished, #iter = 8153
C = 1.092947
obj = 1863.465955, rho = 0.246807
nSV = 6165, nBSV = 1212
........*
optimization finished, #iter = 8870
C = 2.329104
obj = 3204.665480, rho = 0.157410
nSV = 5496, nBSV = 3
.......*
optimization finished, #iter = 7166
C = 2.120370
obj = 2518.132400, rho = -0.003647
nSV = 4730, nBSV = 30
.........*
optimization finished, #iter = 9389
C = 1.541375
obj = 2651.112535, rho = 0.372311
nSV = 6538, nBSV = 740
........*
optimization finished, #iter = 8691
C = 2.335305
obj = 3076.598453, rho = -0.145412
nSV = 5267, nBSV = 10
...........*
optimization finished, #iter = 11806
C = 1.958312
obj = 3742.370511, rho = 0.264561
nSV = 7464, nBSV = 243
..........*
optimization finished, #iter = 10807
C = 2.010412
obj = 3472.032148, rho = 0.400780
nSV = 6855, nBSV = 261
Total nSV = 24683
[LibSVM]train time: 738.827s
test time:  364.280s
accuracy:   0.423


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
..................................*
optimization finished, #iter = 8627
C = 1.883466
obj = 2728.009775, rho = -0.303907
nSV = 5676, nBSV = 318
*
optimization finished, #iter = 8669
C = 1.888528
obj = 2732.530965, rho = -0.274521
nSV = 5686, nBSV = 305
..*
optimization finished, #iter = 8634
C = 1.880315
obj = 2720.209366, rho = -0.310090
nSV = 5672, nBSV = 319
....*
optimization finished, #iter = 8657
C = 1.884087
obj = 2728.016164, rho = -0.298321
nSV = 5682, nBSV = 301
*
optimization finished, #iter = 8612
C = 1.868355
obj = 2703.493195, rho = -0.289797
nSV = 5663, nBSV = 328
.................................*
optimization finished, #iter = 8132
C = 1.660518
obj = 2422.788309, rho = -0.300003
nSV = 5496, nBSV = 428
.*
optimization finished, #iter = 8053
C = 1.667147
obj = 2432.239502, rho = -0.290232
nSV = 5497, nBSV = 420
*
optimization finished, #iter = 8026
C = 1.651344
obj = 2403.919844, rho = -0.260624
nSV = 5479, nBSV = 438
...*
optimization finished, #iter = 8063
C = 1.668516
obj = 2435.394465, rho = -0.279583
nSV = 5505, nBSV = 422
...*
optimization finished, #iter = 8052
C = 1.661777
obj = 2424.269888, rho = -0.270712
nSV = 5493, nBSV = 415
...........................*
optimization finished, #iter = 7889
C = 1.464507
obj = 2184.251191, rho = -0.155489
nSV = 5440, nBSV = 599
..*
optimization finished, #iter = 7849
C = 1.470031
obj = 2191.628205, rho = -0.186188
nSV = 5428, nBSV = 604
..*
optimization finished, #iter = 7826
C = 1.468178
obj = 2186.456230, rho = -0.172579
nSV = 5434, nBSV = 612
..*
optimization finished, #iter = 7808
C = 1.471009
obj = 2193.103890, rho = -0.179658
nSV = 5429, nBSV = 608
..*
optimization finished, #iter = 7797
C = 1.461505
obj = 2177.282907, rho = -0.183943
nSV = 5432, nBSV = 623
.....................*
optimization finished, #iter = 5961
C = 0.909824
obj = 1227.795375, rho = -0.260333
nSV = 4670, nBSV = 1307
.*
optimization finished, #iter = 5987
C = 0.894656
obj = 1203.598584, rho = -0.264427
nSV = 4663, nBSV = 1309
.*
optimization finished, #iter = 5916
C = 0.896028
obj = 1205.368037, rho = -0.267212
nSV = 4643, nBSV = 1299
.*
optimization finished, #iter = 5918
C = 0.897959
obj = 1210.212890, rho = -0.269532
nSV = 4634, nBSV = 1309
.*
optimization finished, #iter = 5892
C = 0.897105
obj = 1206.449469, rho = -0.268337
nSV = 4651, nBSV = 1300
......................*
optimization finished, #iter = 6404
C = 0.885614
obj = 1287.220396, rho = -0.126915
nSV = 4972, nBSV = 1343
..*
optimization finished, #iter = 6356
C = 0.879458
obj = 1276.559500, rho = -0.125782
nSV = 4957, nBSV = 1380
..*
optimization finished, #iter = 6422
C = 0.883915
obj = 1283.724904, rho = -0.117758
nSV = 4973, nBSV = 1345
..*
optimization finished, #iter = 6492
C = 0.886236
obj = 1288.065741, rho = -0.107814
nSV = 4991, nBSV = 1339
..*
optimization finished, #iter = 6503
C = 0.884032
obj = 1286.807265, rho = -0.141639
nSV = 4973, nBSV = 1330
...................*
optimization finished, #iter = 5265
C = 0.805081
obj = 1034.120444, rho = -0.279289
nSV = 4396, nBSV = 1437
..*
optimization finished, #iter = 5374
C = 0.805123
obj = 1037.330697, rho = -0.261606
nSV = 4396, nBSV = 1413
..*
optimization finished, #iter = 5317
C = 0.818257
obj = 1053.347388, rho = -0.274043
nSV = 4404, nBSV = 1399
..*
optimization finished, #iter = 5297
C = 0.805862
obj = 1035.730265, rho = -0.270911
nSV = 4393, nBSV = 1409
*
optimization finished, #iter = 5276
C = 0.803609
obj = 1032.977976, rho = -0.248720
nSV = 4378, nBSV = 1431
...........................*
optimization finished, #iter = 6783
C = 0.793048
obj = 1360.679718, rho = 0.095639
nSV = 5908, nBSV = 1877
.*
optimization finished, #iter = 6912
C = 0.799428
obj = 1373.368829, rho = 0.134256
nSV = 5925, nBSV = 1857
.*
optimization finished, #iter = 6860
C = 0.803828
obj = 1383.782848, rho = 0.113627
nSV = 5931, nBSV = 1873
.*
optimization finished, #iter = 6820
C = 0.797790
obj = 1365.368523, rho = 0.137531
nSV = 5920, nBSV = 1869
*
optimization finished, #iter = 6851
C = 0.799838
obj = 1375.671566, rho = 0.121897
nSV = 5954, nBSV = 1825
.........*
optimization finished, #iter = 5626
C = 2.334993
obj = 2197.171863, rho = -0.026691
nSV = 3755, nBSV = 2
....*
optimization finished, #iter = 5673
C = 2.323938
obj = 2186.018211, rho = -0.046364
nSV = 3755, nBSV = 2
....*
optimization finished, #iter = 5607
C = 2.318421
obj = 2180.478763, rho = -0.003020
nSV = 3755, nBSV = 2
....*
optimization finished, #iter = 5627
C = 2.323551
obj = 2185.002995, rho = 0.008323
nSV = 3752, nBSV = 3
....*
optimization finished, #iter = 5649
C = 2.322022
obj = 2184.443310, rho = -0.014574
nSV = 3757, nBSV = 0
..........*
optimization finished, #iter = 6267
C = 2.140813
obj = 2131.600562, rho = 0.102228
nSV = 3968, nBSV = 6
.....*
optimization finished, #iter = 6251
C = 2.137988
obj = 2127.910845, rho = 0.036566
nSV = 3959, nBSV = 7
......*
optimization finished, #iter = 6357
C = 2.139790
obj = 2130.079016, rho = 0.079085
nSV = 3964, nBSV = 4
....*
optimization finished, #iter = 6210
C = 2.141725
obj = 2131.466324, rho = 0.078586
nSV = 3959, nBSV = 10
.....*
optimization finished, #iter = 6155
C = 2.133769
obj = 2123.814650, rho = 0.051999
nSV = 3959, nBSV = 4
.........*
optimization finished, #iter = 5510
C = 1.525401
obj = 1441.782996, rho = -0.011382
nSV = 3660, nBSV = 134
....*
optimization finished, #iter = 5330
C = 1.517138
obj = 1433.917006, rho = -0.040384
nSV = 3650, nBSV = 141
......*
optimization finished, #iter = 5447
C = 1.512860
obj = 1430.544465, rho = -0.012024
nSV = 3648, nBSV = 135
..*
optimization finished, #iter = 5388
C = 1.514685
obj = 1431.943956, rho = -0.028452
nSV = 3664, nBSV = 151
....*
optimization finished, #iter = 5339
C = 1.521431
obj = 1439.059456, rho = -0.032615
nSV = 3658, nBSV = 126
..........*
optimization finished, #iter = 5886
C = 1.350108
obj = 1398.552955, rho = 0.117305
nSV = 3963, nBSV = 296
....*
optimization finished, #iter = 5880
C = 1.358825
obj = 1407.181444, rho = 0.087587
nSV = 3959, nBSV = 273
.......*
optimization finished, #iter = 5778
C = 1.351495
obj = 1399.188538, rho = 0.131804
nSV = 3966, nBSV = 279
..*
optimization finished, #iter = 5880
C = 1.357006
obj = 1404.243356, rho = 0.120567
nSV = 3974, nBSV = 303
..*
optimization finished, #iter = 5867
C = 1.352151
obj = 1401.576294, rho = 0.075791
nSV = 3947, nBSV = 278
.......*
optimization finished, #iter = 4941
C = 1.354643
obj = 1212.582912, rho = 0.003188
nSV = 3414, nBSV = 171
....*
optimization finished, #iter = 4793
C = 1.361533
obj = 1219.258900, rho = -0.009328
nSV = 3418, nBSV = 170
.....*
optimization finished, #iter = 4848
C = 1.376809
obj = 1232.184195, rho = 0.014196
nSV = 3420, nBSV = 174
.*
optimization finished, #iter = 4697
C = 1.347113
obj = 1205.424424, rho = 0.020091
nSV = 3390, nBSV = 186
...*
optimization finished, #iter = 4801
C = 1.358073
obj = 1216.345661, rho = -0.010541
nSV = 3413, nBSV = 174
.................*
optimization finished, #iter = 5976
C = 0.929079
obj = 1148.749481, rho = 0.357226
nSV = 4633, nBSV = 1137
....*
optimization finished, #iter = 6046
C = 0.952437
obj = 1183.982968, rho = 0.372289
nSV = 4677, nBSV = 1124
....*
optimization finished, #iter = 5861
C = 0.937476
obj = 1159.203665, rho = 0.385854
nSV = 4633, nBSV = 1138
*
optimization finished, #iter = 5951
C = 0.937169
obj = 1159.416752, rho = 0.383117
nSV = 4659, nBSV = 1135
.*
optimization finished, #iter = 5993
C = 0.944350
obj = 1174.280161, rho = 0.362675
nSV = 4646, nBSV = 1138
..........*
optimization finished, #iter = 6219
C = 2.338872
obj = 2392.666742, rho = 0.134720
nSV = 4084, nBSV = 1
.....*
optimization finished, #iter = 6162
C = 2.348445
obj = 2400.430909, rho = 0.089395
nSV = 4084, nBSV = 4
......*
optimization finished, #iter = 6151
C = 2.359923
obj = 2412.349310, rho = 0.080792
nSV = 4084, nBSV = 4
....*
optimization finished, #iter = 6219
C = 2.344323
obj = 2396.314900, rho = 0.085043
nSV = 4083, nBSV = 3
.....*
optimization finished, #iter = 6177
C = 2.356454
obj = 2408.546380, rho = 0.072162
nSV = 4083, nBSV = 4
.........*
optimization finished, #iter = 5732
C = 1.731585
obj = 1694.487080, rho = -0.021611
nSV = 3856, nBSV = 64
....*
optimization finished, #iter = 5687
C = 1.715527
obj = 1679.408285, rho = -0.038099
nSV = 3839, nBSV = 60
.....*
optimization finished, #iter = 5647
C = 1.738464
obj = 1703.297051, rho = -0.064347
nSV = 3850, nBSV = 62
...*
optimization finished, #iter = 5586
C = 1.706626
obj = 1669.099417, rho = -0.040050
nSV = 3840, nBSV = 75
....*
optimization finished, #iter = 5606
C = 1.718805
obj = 1681.292736, rho = -0.046924
nSV = 3850, nBSV = 77
...........*
optimization finished, #iter = 6348
C = 1.519418
obj = 1630.040188, rho = 0.111121
nSV = 4159, nBSV = 165
......*
optimization finished, #iter = 6260
C = 1.525722
obj = 1639.887574, rho = 0.088887
nSV = 4150, nBSV = 163
......*
optimization finished, #iter = 6364
C = 1.550318
obj = 1667.086150, rho = 0.085691
nSV = 4170, nBSV = 146
..*
optimization finished, #iter = 6222
C = 1.508198
obj = 1616.725084, rho = 0.105801
nSV = 4156, nBSV = 189
.....*
optimization finished, #iter = 6328
C = 1.517495
obj = 1627.908852, rho = 0.055352
nSV = 4162, nBSV = 168
........*
optimization finished, #iter = 5053
C = 1.485951
obj = 1376.533698, rho = -0.006090
nSV = 3552, nBSV = 136
.....*
optimization finished, #iter = 5139
C = 1.481114
obj = 1374.021673, rho = -0.011111
nSV = 3551, nBSV = 125
......*
optimization finished, #iter = 5202
C = 1.490380
obj = 1382.329005, rho = -0.023151
nSV = 3554, nBSV = 125
..*
optimization finished, #iter = 5067
C = 1.481272
obj = 1370.590840, rho = -0.005401
nSV = 3575, nBSV = 146
....*
optimization finished, #iter = 5124
C = 1.474270
obj = 1366.941132, rho = -0.033467
nSV = 3564, nBSV = 132
....................*
optimization finished, #iter = 6443
C = 1.055665
obj = 1377.432455, rho = 0.321862
nSV = 4817, nBSV = 982
...*
optimization finished, #iter = 6466
C = 1.063338
obj = 1390.186216, rho = 0.347058
nSV = 4816, nBSV = 982
....*
optimization finished, #iter = 6532
C = 1.080633
obj = 1417.510606, rho = 0.327617
nSV = 4828, nBSV = 973
.*
optimization finished, #iter = 6494
C = 1.048835
obj = 1364.466627, rho = 0.339813
nSV = 4804, nBSV = 987
..*
optimization finished, #iter = 6442
C = 1.064192
obj = 1394.870553, rho = 0.317933
nSV = 4820, nBSV = 962
..........*
optimization finished, #iter = 6090
C = 1.889113
obj = 1957.266201, rho = -0.143618
nSV = 4106, nBSV = 33
.......*
optimization finished, #iter = 6054
C = 1.869935
obj = 1937.063367, rho = -0.125134
nSV = 4104, nBSV = 35
...*
optimization finished, #iter = 5979
C = 1.884693
obj = 1952.441407, rho = -0.155609
nSV = 4111, nBSV = 36
....*
optimization finished, #iter = 6089
C = 1.876072
obj = 1943.590339, rho = -0.152990
nSV = 4104, nBSV = 33
....*
optimization finished, #iter = 5914
C = 1.874784
obj = 1942.237382, rho = -0.145138
nSV = 4099, nBSV = 39
..........*
optimization finished, #iter = 6570
C = 1.678503
obj = 1903.249452, rho = 0.000474
nSV = 4439, nBSV = 102
...........*
optimization finished, #iter = 6712
C = 1.662087
obj = 1884.689801, rho = 0.018319
nSV = 4429, nBSV = 111
....*
optimization finished, #iter = 6624
C = 1.675025
obj = 1900.728938, rho = 0.011000
nSV = 4432, nBSV = 92
*
optimization finished, #iter = 6651
C = 1.673195
obj = 1897.419822, rho = 0.011119
nSV = 4424, nBSV = 101
.....*
optimization finished, #iter = 6689
C = 1.655788
obj = 1877.481273, rho = -0.020421
nSV = 4418, nBSV = 98
........*
optimization finished, #iter = 5637
C = 1.571771
obj = 1544.491070, rho = -0.113125
nSV = 3822, nBSV = 119
..........*
optimization finished, #iter = 5674
C = 1.550799
obj = 1524.457003, rho = -0.086662
nSV = 3825, nBSV = 128
..*
optimization finished, #iter = 5624
C = 1.563919
obj = 1536.413055, rho = -0.099085
nSV = 3810, nBSV = 128
.*
optimization finished, #iter = 5685
C = 1.575097
obj = 1548.737229, rho = -0.111462
nSV = 3819, nBSV = 112
....*
optimization finished, #iter = 5691
C = 1.548566
obj = 1520.701469, rho = -0.116175
nSV = 3801, nBSV = 126
................*
optimization finished, #iter = 7169
C = 1.184118
obj = 1654.937350, rho = 0.212702
nSV = 5114, nBSV = 787
............*
optimization finished, #iter = 7059
C = 1.191716
obj = 1667.272469, rho = 0.252469
nSV = 5135, nBSV = 796
.*
optimization finished, #iter = 7171
C = 1.181364
obj = 1648.493995, rho = 0.271535
nSV = 5148, nBSV = 775
....*
optimization finished, #iter = 7084
C = 1.186079
obj = 1660.421013, rho = 0.240592
nSV = 5119, nBSV = 772
..*
optimization finished, #iter = 7079
C = 1.180946
obj = 1651.881865, rho = 0.246275
nSV = 5103, nBSV = 786
..........*
optimization finished, #iter = 6991
C = 2.319629
obj = 2553.338930, rho = 0.145072
nSV = 4398, nBSV = 2
............*
optimization finished, #iter = 6958
C = 2.316009
obj = 2549.716316, rho = 0.182471
nSV = 4398, nBSV = 1
..*
optimization finished, #iter = 6921
C = 2.333108
obj = 2568.691892, rho = 0.150145
nSV = 4401, nBSV = 1
.*
optimization finished, #iter = 6905
C = 2.318121
obj = 2552.438636, rho = 0.171927
nSV = 4398, nBSV = 1
.....*
optimization finished, #iter = 6942
C = 2.318213
obj = 2551.001481, rho = 0.125211
nSV = 4400, nBSV = 3
........*
optimization finished, #iter = 5838
C = 2.133489
obj = 2029.026688, rho = -0.014172
nSV = 3793, nBSV = 9
.........*
optimization finished, #iter = 5771
C = 2.112932
obj = 2007.920167, rho = 0.020190
nSV = 3794, nBSV = 18
..*
optimization finished, #iter = 5747
C = 2.147541
obj = 2043.450653, rho = 0.000532
nSV = 3790, nBSV = 12
..*
optimization finished, #iter = 5885
C = 2.137409
obj = 2032.903416, rho = 0.003523
nSV = 3789, nBSV = 12
....*
optimization finished, #iter = 5743
C = 2.136957
obj = 2031.397398, rho = -0.013100
nSV = 3795, nBSV = 13
..............*
optimization finished, #iter = 7957
C = 1.597329
obj = 2233.779904, rho = 0.332232
nSV = 5344, nBSV = 441
............*
optimization finished, #iter = 7760
C = 1.579216
obj = 2202.024700, rho = 0.393936
nSV = 5328, nBSV = 495
....*
optimization finished, #iter = 7913
C = 1.612122
obj = 2251.764333, rho = 0.386631
nSV = 5354, nBSV = 450
*
optimization finished, #iter = 7863
C = 1.575418
obj = 2194.064646, rho = 0.363130
nSV = 5332, nBSV = 479
.....*
optimization finished, #iter = 7888
C = 1.601773
obj = 2233.919699, rho = 0.368887
nSV = 5361, nBSV = 489
.........*
optimization finished, #iter = 6818
C = 2.329402
obj = 2454.787170, rho = -0.146024
nSV = 4214, nBSV = 5
...........*
optimization finished, #iter = 6827
C = 2.313231
obj = 2437.670149, rho = -0.140498
nSV = 4214, nBSV = 5
.*
optimization finished, #iter = 6767
C = 2.325213
obj = 2451.030783, rho = -0.152648
nSV = 4215, nBSV = 4
....*
optimization finished, #iter = 6780
C = 2.322385
obj = 2448.658613, rho = -0.135710
nSV = 4214, nBSV = 4
.....*
optimization finished, #iter = 6832
C = 2.324773
obj = 2449.965496, rho = -0.133989
nSV = 4212, nBSV = 7
.................*
optimization finished, #iter = 9573
C = 1.979515
obj = 3038.370997, rho = 0.234948
nSV = 6033, nBSV = 147
.................*
optimization finished, #iter = 9602
C = 1.971125
obj = 3022.226981, rho = 0.239068
nSV = 6020, nBSV = 156
..*
optimization finished, #iter = 9496
C = 1.967692
obj = 3018.784519, rho = 0.275246
nSV = 6007, nBSV = 161
...*
optimization finished, #iter = 9602
C = 1.976365
obj = 3030.037732, rho = 0.282905
nSV = 6019, nBSV = 153
......*
optimization finished, #iter = 9484
C = 1.970413
obj = 3024.519758, rho = 0.288330
nSV = 6023, nBSV = 146
...............*
optimization finished, #iter = 8716
C = 2.008735
obj = 2782.447949, rho = 0.379030
nSV = 5502, nBSV = 180
Total nSV = 19803
.............*
optimization finished, #iter = 8745
C = 2.005999
obj = 2777.433939, rho = 0.394633
nSV = 5520, nBSV = 196
Total nSV = 19814
.*
optimization finished, #iter = 8658
C = 2.015155
obj = 2792.973943, rho = 0.402026
nSV = 5522, nBSV = 177
Total nSV = 19801
.....*
optimization finished, #iter = 8697
C = 2.011501
obj = 2786.118646, rho = 0.409587
nSV = 5514, nBSV = 180
Total nSV = 19809
......*
optimization finished, #iter = 8636
C = 1.996608
obj = 2764.359537, rho = 0.402170
nSV = 5507, nBSV = 177
Total nSV = 19781
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  9.4min finished
	accuracy: 5-fold cross validation: [0.4302 0.436  0.4248 0.4384 0.4282]
	test accuracy: 5-fold cross validation accuracy: 0.43 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.4232
	accuracy score (normalize=False):  10580

compute the precision
	precision score (average=macro):  0.3296542547227407
	precision score (average=micro):  0.4232
	precision score (average=weighted):  0.36441787605127834
	precision score (average=None):  [0.52356154 0.23905724 0.26017558 0.32552783 0.32323857 0.27209031
 0.217119   0.47646398]
	precision score (average=None, zero_division=1):  [0.52356154 0.23905724 0.26017558 0.32552783 0.32323857 0.27209031
 0.217119   0.47646398]

compute the precision
	recall score (average=macro):  0.3265346715684577
	recall score (average=micro):  0.4232
	recall score (average=weighted):  0.4232
	recall score (average=None):  [0.82078853 0.06168549 0.12829595 0.32182163 0.22670134 0.24526316
 0.0443686  0.76335267]
	recall score (average=None, zero_division=1):  [0.82078853 0.06168549 0.12829595 0.32182163 0.22670134 0.24526316
 0.0443686  0.76335267]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.3022215771125444
	f1 score (average=micro):  0.4232
	f1 score (average=weighted):  0.36726728623786187
	f1 score (average=None):  [0.63931756 0.0980663  0.17185029 0.32366412 0.26649682 0.25798118
 0.07368048 0.58671587]

compute the F-beta score
	f beta score (average=macro):  0.3072740965101899
	f beta score (average=micro):  0.4232
	f beta score (average=weighted):  0.35583046927256007
	f beta score (average=None):  [0.56444104 0.15177426 0.21580829 0.32477978 0.29786992 0.26626543
 0.12206573 0.51518834]

compute the average Hamming loss
	hamming loss:  0.5768

jaccard similarity coefficient score
	jaccard score (average=macro):  0.19546399407072007
	jaccard score (average=None):  [0.46985068 0.05156137 0.09400231 0.19307832 0.1537331  0.14809322
 0.03824936 0.4151436 ]


================================================================================
Classifier.PASSIVE_AGGRESSIVE_CLASSIFIER
________________________________________________________________________________
Training: 
PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,
                            early_stopping=False, fit_intercept=True,
                            loss='hinge', max_iter=1000, n_iter_no_change=5,
                            n_jobs=-1, random_state=0, shuffle=True, tol=0.001,
                            validation_fraction=0.1, verbose=True,
                            warm_start=False)
-- Epoch 1
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 44.37, NNZs: 53609, Bias: -0.984532, T: 25000, Avg. loss: 0.266433
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 52.82, NNZs: 58416, Bias: -1.047044, T: 25000, Avg. loss: 0.339288
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 50.97, NNZs: 57829, Bias: -0.905457, T: 25000, Avg. loss: 0.304023
Total training time: 0.10 seconds.
-- Epoch 2
Norm: 67.55, NNZs: 56155, Bias: -0.569719, T: 25000, Avg. loss: 0.403276
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 45.94, NNZs: 56463, Bias: -1.007615, T: 25000, Avg. loss: 0.281790
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 44.18, NNZs: 54761, Bias: -1.035598, T: 25000, Avg. loss: 0.263862
Total training time: 0.10 seconds.
-- Epoch 2
Norm: 48.51, NNZs: 56734, Bias: -1.144661, T: 25000, Avg. loss: 0.283122
Total training time: 0.11 seconds.
-- Epoch 2
Norm: 73.53, NNZs: 57920, Bias: -0.938501, T: 50000, Avg. loss: 0.202666
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 66.69, NNZs: 54100, Bias: -0.661343, T: 25000, Avg. loss: 0.375045
Total training time: 0.10 seconds.
-- Epoch 2
Norm: 83.65, NNZs: 61895, Bias: -1.010183, T: 50000, Avg. loss: 0.246052
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 94.48, NNZs: 59098, Bias: -0.697991, T: 50000, Avg. loss: 0.250326
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 72.81, NNZs: 58568, Bias: -1.125855, T: 50000, Avg. loss: 0.199903
Total training time: 0.12 seconds.
-- Epoch 3
Norm: 75.86, NNZs: 60536, Bias: -1.028257, T: 50000, Avg. loss: 0.211677
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 76.81, NNZs: 60280, Bias: -1.073611, T: 50000, Avg. loss: 0.203767
Total training time: 0.13 seconds.
-- Epoch 3
Norm: 92.40, NNZs: 56598, Bias: -0.661227, T: 50000, Avg. loss: 0.225275
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 105.47, NNZs: 62876, Bias: -1.069766, T: 75000, Avg. loss: 0.176436
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 81.18, NNZs: 61643, Bias: -0.960147, T: 50000, Avg. loss: 0.217893
Total training time: 0.14 seconds.
-- Epoch 3
Norm: 93.93, NNZs: 59112, Bias: -1.049628, T: 75000, Avg. loss: 0.146813
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 111.30, NNZs: 57594, Bias: -0.611261, T: 75000, Avg. loss: 0.159518
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 121.52, NNZs: 63256, Bias: -1.086505, T: 100000, Avg. loss: 0.129288
Total training time: 0.12 seconds.
Norm: 114.38, NNZs: 59904, Bias: -0.655942, T: 75000, Avg. loss: 0.178840
Total training time: 0.11 seconds.
-- Epoch 5
-- Epoch 4
Norm: 93.45, NNZs: 59685, Bias: -1.158776, T: 75000, Avg. loss: 0.145836
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 96.67, NNZs: 61541, Bias: -1.190784, T: 75000, Avg. loss: 0.151132
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 96.66, NNZs: 61367, Bias: -1.198449, T: 75000, Avg. loss: 0.144352
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 101.26, NNZs: 62604, Bias: -1.042872, T: 75000, Avg. loss: 0.149572
Total training time: 0.15 seconds.
-- Epoch 4
Norm: 109.01, NNZs: 59662, Bias: -1.048799, T: 100000, Avg. loss: 0.106225
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 133.48, NNZs: 63507, Bias: -1.175799, T: 125000, Avg. loss: 0.096032
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 125.24, NNZs: 57990, Bias: -0.688941, T: 100000, Avg. loss: 0.115627
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 129.37, NNZs: 60293, Bias: -0.645908, T: 100000, Avg. loss: 0.132230
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 108.46, NNZs: 60182, Bias: -1.183592, T: 100000, Avg. loss: 0.105009
Total training time: 0.15 seconds.
-- Epoch 5
Norm: 111.88, NNZs: 61937, Bias: -1.172707, T: 100000, Avg. loss: 0.108835
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 111.11, NNZs: 61716, Bias: -1.259510, T: 100000, Avg. loss: 0.103871
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 115.74, NNZs: 62990, Bias: -1.001357, T: 100000, Avg. loss: 0.106194
Total training time: 0.17 seconds.
-- Epoch 5
Norm: 142.81, NNZs: 63603, Bias: -1.173085, T: 150000, Avg. loss: 0.072149
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 120.08, NNZs: 59885, Bias: -1.121855, T: 125000, Avg. loss: 0.077664
Total training time: 0.16 seconds.
-- Epoch 6
Norm: 135.93, NNZs: 58223, Bias: -0.715602, T: 125000, Avg. loss: 0.085999
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 141.01, NNZs: 60520, Bias: -0.680312, T: 125000, Avg. loss: 0.100588
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 119.37, NNZs: 60382, Bias: -1.281028, T: 125000, Avg. loss: 0.075526
Total training time: 0.17 seconds.
-- Epoch 6
Norm: 123.01, NNZs: 62161, Bias: -1.210542, T: 125000, Avg. loss: 0.078914
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 121.90, NNZs: 61876, Bias: -1.329958, T: 125000, Avg. loss: 0.076225
Total training time: 0.17 seconds.
-- Epoch 6
Norm: 149.84, NNZs: 63650, Bias: -1.234977, T: 175000, Avg. loss: 0.054087
Total training time: 0.16 seconds.
-- Epoch 8
Norm: 144.22, NNZs: 58339, Bias: -0.741086, T: 150000, Avg. loss: 0.065265
Total training time: 0.16 seconds.
-- Epoch 7
Norm: 126.58, NNZs: 63187, Bias: -1.079443, T: 125000, Avg. loss: 0.077796
Total training time: 0.18 seconds.
-- Epoch 6
Norm: 128.46, NNZs: 60009, Bias: -1.208620, T: 150000, Avg. loss: 0.057727
Total training time: 0.17 seconds.
-- Epoch 7
Norm: 155.38, NNZs: 63666, Bias: -1.246735, T: 200000, Avg. loss: 0.041632
Total training time: 0.17 seconds.
-- Epoch 9
Norm: 150.63, NNZs: 58391, Bias: -0.693349, T: 175000, Avg. loss: 0.049350
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 150.04, NNZs: 60639, Bias: -0.741573, T: 150000, Avg. loss: 0.076718
Total training time: 0.15 seconds.
Norm: 127.70, NNZs: 60475, Bias: -1.295125, T: 150000, Avg. loss: 0.055920
Total training time: 0.18 seconds.
-- Epoch 7
-- Epoch 7
Norm: 131.40, NNZs: 62264, Bias: -1.266068, T: 150000, Avg. loss: 0.058550
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 129.89, NNZs: 61974, Bias: -1.375051, T: 150000, Avg. loss: 0.055249
Total training time: 0.19 seconds.
-- Epoch 7
Norm: 134.51, NNZs: 63272, Bias: -1.148457, T: 150000, Avg. loss: 0.055759
Total training time: 0.20 seconds.
-- Epoch 7
Norm: 134.74, NNZs: 60069, Bias: -1.234996, T: 175000, Avg. loss: 0.042397
Total training time: 0.19 seconds.
-- Epoch 8
Norm: 159.54, NNZs: 63672, Bias: -1.246878, T: 225000, Avg. loss: 0.030871
Total training time: 0.18 seconds.
-- Epoch 10
Norm: 155.47, NNZs: 58433, Bias: -0.751235, T: 200000, Avg. loss: 0.037036
Total training time: 0.18 seconds.
-- Epoch 9
Norm: 133.78, NNZs: 60559, Bias: -1.316626, T: 175000, Avg. loss: 0.040656
Total training time: 0.20 seconds.
-- Epoch 8
Norm: 157.22, NNZs: 60727, Bias: -0.721577, T: 175000, Avg. loss: 0.059681
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 137.59, NNZs: 62314, Bias: -1.344487, T: 175000, Avg. loss: 0.042798
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 135.95, NNZs: 62030, Bias: -1.428890, T: 175000, Avg. loss: 0.041091
Total training time: 0.20 seconds.
-- Epoch 8
Norm: 162.72, NNZs: 63683, Bias: -1.280622, T: 250000, Avg. loss: 0.023558
Total training time: 0.19 seconds.
Norm: 140.45, NNZs: 63326, Bias: -1.148691, T: 175000, Avg. loss: 0.040761
Total training time: 0.21 seconds.
-- Epoch 11
-- Epoch 8
Norm: 159.34, NNZs: 58471, Bias: -0.767768, T: 225000, Avg. loss: 0.029169
Total training time: 0.19 seconds.
-- Epoch 10
Norm: 139.41, NNZs: 60122, Bias: -1.251294, T: 200000, Avg. loss: 0.031100
Total training time: 0.20 seconds.
-- Epoch 9
Norm: 138.38, NNZs: 60586, Bias: -1.324314, T: 200000, Avg. loss: 0.030027
Total training time: 0.21 seconds.
-- Epoch 9
Norm: 165.17, NNZs: 63691, Bias: -1.269310, T: 275000, Avg. loss: 0.017856
Total training time: 0.20 seconds.
Norm: 162.89, NNZs: 60777, Bias: -0.760201, T: 200000, Avg. loss: 0.046528
Total training time: 0.18 seconds.
-- Epoch 12
-- Epoch 9
Norm: 142.29, NNZs: 62346, Bias: -1.362235, T: 200000, Avg. loss: 0.031691
Total training time: 0.18 seconds.
-- Epoch 9
Norm: 140.61, NNZs: 62048, Bias: -1.444587, T: 200000, Avg. loss: 0.030934
Total training time: 0.21 seconds.
-- Epoch 9
Norm: 162.29, NNZs: 58487, Bias: -0.758277, T: 250000, Avg. loss: 0.021995
Total training time: 0.20 seconds.
-- Epoch 11
Norm: 144.85, NNZs: 63339, Bias: -1.178205, T: 200000, Avg. loss: 0.029855
Total training time: 0.23 seconds.
-- Epoch 9
Norm: 142.95, NNZs: 60136, Bias: -1.253382, T: 225000, Avg. loss: 0.023281
Total training time: 0.22 seconds.
-- Epoch 10
Norm: 141.83, NNZs: 60606, Bias: -1.355584, T: 225000, Avg. loss: 0.022404
Total training time: 0.22 seconds.
Norm: 167.10, NNZs: 63697, Bias: -1.299269, T: 300000, Avg. loss: 0.014122
Total training time: 0.21 seconds.
-- Epoch 10
-- Epoch 13
Norm: 167.35, NNZs: 60798, Bias: -0.775682, T: 225000, Avg. loss: 0.036210
Total training time: 0.19 seconds.
-- Epoch 10
Norm: 164.57, NNZs: 58489, Bias: -0.771938, T: 275000, Avg. loss: 0.016963
Total training time: 0.21 seconds.
-- Epoch 12
Norm: 145.91, NNZs: 62375, Bias: -1.387993, T: 225000, Avg. loss: 0.024160
Total training time: 0.20 seconds.
Norm: 144.00, NNZs: 62070, Bias: -1.467357, T: 225000, Avg. loss: 0.022224
Total training time: 0.23 seconds.
-- Epoch 10
-- Epoch 10
Norm: 148.08, NNZs: 63372, Bias: -1.189445, T: 225000, Avg. loss: 0.021709
Total training time: 0.24 seconds.
-- Epoch 10
Norm: 144.47, NNZs: 60612, Bias: -1.377214, T: 250000, Avg. loss: 0.016966
Total training time: 0.23 seconds.
-- Epoch 11
Norm: 168.59, NNZs: 63702, Bias: -1.304289, T: 325000, Avg. loss: 0.010705
Total training time: 0.22 seconds.
-- Epoch 14
Norm: 145.62, NNZs: 60158, Bias: -1.260868, T: 250000, Avg. loss: 0.017391
Total training time: 0.23 seconds.
Norm: 170.84, NNZs: 60821, Bias: -0.788623, T: 250000, Avg. loss: 0.028126
Total training time: 0.20 seconds.
-- Epoch 11
-- Epoch 11
Norm: 166.44, NNZs: 58491, Bias: -0.761781, T: 300000, Avg. loss: 0.013690
Total training time: 0.22 seconds.
-- Epoch 13
Norm: 148.54, NNZs: 62379, Bias: -1.365987, T: 250000, Avg. loss: 0.017219
Total training time: 0.21 seconds.
Norm: 146.50, NNZs: 62082, Bias: -1.486625, T: 250000, Avg. loss: 0.016214
Total training time: 0.24 seconds.
-- Epoch 11
-- Epoch 11
Norm: 146.40, NNZs: 60619, Bias: -1.373181, T: 275000, Avg. loss: 0.012232
Total training time: 0.25 seconds.
-- Epoch 12
Norm: 169.73, NNZs: 63703, Bias: -1.302317, T: 350000, Avg. loss: 0.008148
Total training time: 0.23 seconds.
-- Epoch 15
Norm: 173.71, NNZs: 60827, Bias: -0.779406, T: 275000, Avg. loss: 0.022679
Total training time: 0.22 seconds.
Norm: 167.88, NNZs: 58497, Bias: -0.770383, T: 325000, Avg. loss: 0.010576
Total training time: 0.23 seconds.
-- Epoch 12
-- Epoch 14
Norm: 150.47, NNZs: 63385, Bias: -1.192607, T: 250000, Avg. loss: 0.015845
Total training time: 0.26 seconds.
-- Epoch 11
Norm: 147.65, NNZs: 60166, Bias: -1.275404, T: 275000, Avg. loss: 0.013202
Total training time: 0.25 seconds.
-- Epoch 12
Norm: 147.85, NNZs: 60622, Bias: -1.387326, T: 300000, Avg. loss: 0.009203
Total training time: 0.26 seconds.
-- Epoch 13
Norm: 170.62, NNZs: 63709, Bias: -1.307324, T: 375000, Avg. loss: 0.006437
Total training time: 0.24 seconds.
-- Epoch 16
Norm: 176.00, NNZs: 60834, Bias: -0.801260, T: 300000, Avg. loss: 0.018039
Total training time: 0.23 seconds.
Norm: 169.10, NNZs: 58500, Bias: -0.763403, T: 350000, Avg. loss: 0.008807
Total training time: 0.24 seconds.
-- Epoch 13
-- Epoch 15
Norm: 148.38, NNZs: 62094, Bias: -1.488577, T: 275000, Avg. loss: 0.011993
Total training time: 0.26 seconds.
Norm: 150.53, NNZs: 62388, Bias: -1.399973, T: 275000, Avg. loss: 0.013217
Total training time: 0.23 seconds.
-- Epoch 12
-- Epoch 12
Norm: 152.23, NNZs: 63390, Bias: -1.206707, T: 275000, Avg. loss: 0.011627
Total training time: 0.27 seconds.
-- Epoch 12
Norm: 149.16, NNZs: 60172, Bias: -1.280567, T: 300000, Avg. loss: 0.009733
Total training time: 0.26 seconds.
-- Epoch 13
Norm: 148.93, NNZs: 60630, Bias: -1.399331, T: 325000, Avg. loss: 0.006859
Total training time: 0.27 seconds.
-- Epoch 14
Norm: 171.29, NNZs: 63709, Bias: -1.309807, T: 400000, Avg. loss: 0.004820
Total training time: 0.25 seconds.
-- Epoch 17
Norm: 170.03, NNZs: 58505, Bias: -0.774497, T: 375000, Avg. loss: 0.006787
Total training time: 0.25 seconds.
Norm: 177.86, NNZs: 60840, Bias: -0.809747, T: 325000, Avg. loss: 0.014505
Total training time: 0.24 seconds.
-- Epoch 16
-- Epoch 14
Norm: 149.81, NNZs: 62100, Bias: -1.495285, T: 300000, Avg. loss: 0.009079
Total training time: 0.27 seconds.
Norm: 152.00, NNZs: 62389, Bias: -1.410984, T: 300000, Avg. loss: 0.009708
Total training time: 0.24 seconds.
-- Epoch 13
-- Epoch 13
Norm: 153.58, NNZs: 63393, Bias: -1.213941, T: 300000, Avg. loss: 0.008890
Total training time: 0.29 seconds.
-- Epoch 13
Norm: 149.74, NNZs: 60633, Bias: -1.405013, T: 350000, Avg. loss: 0.005068
Total training time: 0.28 seconds.
-- Epoch 15
Norm: 171.82, NNZs: 63709, Bias: -1.306075, T: 425000, Avg. loss: 0.003719
Total training time: 0.26 seconds.
-- Epoch 18
Norm: 179.34, NNZs: 60847, Bias: -0.806133, T: 350000, Avg. loss: 0.011388
Total training time: 0.25 seconds.
-- Epoch 15
Norm: 170.77, NNZs: 58506, Bias: -0.768129, T: 400000, Avg. loss: 0.005286
Total training time: 0.27 seconds.
-- Epoch 17
Norm: 150.33, NNZs: 60185, Bias: -1.299024, T: 325000, Avg. loss: 0.007606
Total training time: 0.28 seconds.
-- Epoch 14
Norm: 150.81, NNZs: 62100, Bias: -1.508151, T: 325000, Avg. loss: 0.006373
Total training time: 0.29 seconds.
Norm: 153.09, NNZs: 62389, Bias: -1.413208, T: 325000, Avg. loss: 0.007174
Total training time: 0.26 seconds.
-- Epoch 14
-- Epoch 14
Norm: 150.36, NNZs: 60633, Bias: -1.405020, T: 375000, Avg. loss: 0.003842
Total training time: 0.29 seconds.
-- Epoch 16
Norm: 172.23, NNZs: 63711, Bias: -1.312792, T: 450000, Avg. loss: 0.003027
Total training time: 0.28 seconds.
-- Epoch 19
Norm: 180.51, NNZs: 60850, Bias: -0.817402, T: 375000, Avg. loss: 0.008972
Total training time: 0.26 seconds.
Norm: 171.36, NNZs: 58515, Bias: -0.777129, T: 425000, Avg. loss: 0.004258
Total training time: 0.28 seconds.
-- Epoch 16
-- Epoch 18
Norm: 154.54, NNZs: 63397, Bias: -1.218618, T: 325000, Avg. loss: 0.006359
Total training time: 0.30 seconds.
-- Epoch 14
Norm: 151.21, NNZs: 60191, Bias: -1.309047, T: 350000, Avg. loss: 0.005676
Total training time: 0.30 seconds.
-- Epoch 15
Norm: 150.84, NNZs: 60634, Bias: -1.405323, T: 400000, Avg. loss: 0.002974
Total training time: 0.30 seconds.
-- Epoch 17
Norm: 172.55, NNZs: 63711, Bias: -1.311724, T: 475000, Avg. loss: 0.002312
Total training time: 0.29 seconds.
-- Epoch 20
Norm: 171.89, NNZs: 58524, Bias: -0.777365, T: 450000, Avg. loss: 0.003732
Total training time: 0.29 seconds.
-- Epoch 19
Norm: 181.44, NNZs: 60854, Bias: -0.811399, T: 400000, Avg. loss: 0.007060
Total training time: 0.27 seconds.
-- Epoch 17
Norm: 151.58, NNZs: 62107, Bias: -1.511959, T: 350000, Avg. loss: 0.004799
Total training time: 0.30 seconds.
Norm: 153.97, NNZs: 62395, Bias: -1.426439, T: 350000, Avg. loss: 0.005821
Total training time: 0.27 seconds.
-- Epoch 15
-- Epoch 15
Norm: 155.28, NNZs: 63397, Bias: -1.218586, T: 350000, Avg. loss: 0.004820
Total training time: 0.32 seconds.
-- Epoch 15
Norm: 151.21, NNZs: 60634, Bias: -1.407957, T: 425000, Avg. loss: 0.002320
Total training time: 0.31 seconds.
-- Epoch 18
Norm: 172.81, NNZs: 63713, Bias: -1.314728, T: 500000, Avg. loss: 0.001849
Total training time: 0.30 seconds.
-- Epoch 21
Norm: 151.93, NNZs: 60191, Bias: -1.302366, T: 375000, Avg. loss: 0.004547
Total training time: 0.31 seconds.
Norm: 172.25, NNZs: 58525, Bias: -0.779455, T: 475000, Avg. loss: 0.002556
Total training time: 0.30 seconds.
-- Epoch 16
-- Epoch 20
Norm: 182.18, NNZs: 60855, Bias: -0.819656, T: 425000, Avg. loss: 0.005631
Total training time: 0.28 seconds.
-- Epoch 18
Norm: 152.15, NNZs: 62114, Bias: -1.513443, T: 375000, Avg. loss: 0.003586
Total training time: 0.32 seconds.
Norm: 154.64, NNZs: 62398, Bias: -1.425536, T: 375000, Avg. loss: 0.004363
Total training time: 0.29 seconds.
-- Epoch 16
-- Epoch 16
Norm: 155.84, NNZs: 63397, Bias: -1.220694, T: 375000, Avg. loss: 0.003712
Total training time: 0.33 seconds.
Norm: 151.48, NNZs: 60634, Bias: -1.410524, T: 450000, Avg. loss: 0.001707
Total training time: 0.32 seconds.
-- Epoch 16
-- Epoch 19
Norm: 173.03, NNZs: 63713, Bias: -1.313003, T: 525000, Avg. loss: 0.001626
Total training time: 0.31 seconds.
-- Epoch 22
Norm: 172.57, NNZs: 58525, Bias: -0.778819, T: 500000, Avg. loss: 0.002277
Total training time: 0.31 seconds.
-- Epoch 21
Norm: 182.75, NNZs: 60855, Bias: -0.822961, T: 450000, Avg. loss: 0.004338
Total training time: 0.30 seconds.
-- Epoch 19
Norm: 152.45, NNZs: 60191, Bias: -1.306593, T: 400000, Avg. loss: 0.003403
Total training time: 0.33 seconds.
-- Epoch 17
Norm: 152.57, NNZs: 62114, Bias: -1.517006, T: 400000, Avg. loss: 0.002630
Total training time: 0.34 seconds.
-- Epoch 17
Norm: 155.12, NNZs: 62398, Bias: -1.429033, T: 400000, Avg. loss: 0.003204
Total training time: 0.30 seconds.
-- Epoch 17
Norm: 151.69, NNZs: 60634, Bias: -1.412248, T: 475000, Avg. loss: 0.001300
Total training time: 0.34 seconds.
-- Epoch 20
Norm: 173.22, NNZs: 63713, Bias: -1.314154, T: 550000, Avg. loss: 0.001352
Total training time: 0.32 seconds.
Convergence after 22 epochs took 0.32 seconds
Norm: 172.81, NNZs: 58525, Bias: -0.782249, T: 525000, Avg. loss: 0.001683
Total training time: 0.32 seconds.
-- Epoch 22
Norm: 156.27, NNZs: 63398, Bias: -1.225491, T: 400000, Avg. loss: 0.002874
Total training time: 0.35 seconds.
Norm: 183.23, NNZs: 60855, Bias: -0.822736, T: 475000, Avg. loss: 0.003647
Total training time: 0.31 seconds.
-- Epoch 17
-- Epoch 20
Norm: 152.92, NNZs: 60197, Bias: -1.309796, T: 425000, Avg. loss: 0.002964
Total training time: 0.34 seconds.
-- Epoch 18
Norm: 151.87, NNZs: 60634, Bias: -1.413340, T: 500000, Avg. loss: 0.001087
Total training time: 0.35 seconds.
Convergence after 20 epochs took 0.35 seconds
Norm: 173.01, NNZs: 58528, Bias: -0.782915, T: 550000, Avg. loss: 0.001403
Total training time: 0.33 seconds.
-- Epoch 23
Norm: 152.89, NNZs: 62114, Bias: -1.517878, T: 425000, Avg. loss: 0.002007
Total training time: 0.35 seconds.
-- Epoch 18
Norm: 155.52, NNZs: 62398, Bias: -1.430571, T: 425000, Avg. loss: 0.002665
Total training time: 0.32 seconds.
-- Epoch 18
Norm: 183.61, NNZs: 60855, Bias: -0.824509, T: 500000, Avg. loss: 0.002829
Total training time: 0.32 seconds.
-- Epoch 21
Norm: 156.62, NNZs: 63398, Bias: -1.223090, T: 425000, Avg. loss: 0.002293
Total training time: 0.36 seconds.
-- Epoch 18
Norm: 173.16, NNZs: 58528, Bias: -0.782353, T: 575000, Avg. loss: 0.001058
Total training time: 0.34 seconds.
-- Epoch 24
Norm: 153.28, NNZs: 60197, Bias: -1.309382, T: 450000, Avg. loss: 0.002301
Total training time: 0.36 seconds.
-- Epoch 19
Norm: 183.90, NNZs: 60855, Bias: -0.827256, T: 525000, Avg. loss: 0.002267
Total training time: 0.33 seconds.
-- Epoch 22
Norm: 153.12, NNZs: 62114, Bias: -1.519271, T: 450000, Avg. loss: 0.001441
Total training time: 0.36 seconds.
-- Epoch 19
Norm: 155.84, NNZs: 62402, Bias: -1.431580, T: 450000, Avg. loss: 0.002121
Total training time: 0.33 seconds.
-- Epoch 19
Norm: 156.86, NNZs: 63398, Bias: -1.229054, T: 450000, Avg. loss: 0.001676
Total training time: 0.37 seconds.
-- Epoch 19
Norm: 173.29, NNZs: 58528, Bias: -0.782487, T: 600000, Avg. loss: 0.000886
Total training time: 0.35 seconds.
Convergence after 24 epochs took 0.35 seconds
Norm: 184.13, NNZs: 60863, Bias: -0.831908, T: 550000, Avg. loss: 0.001708
Total training time: 0.34 seconds.
-- Epoch 23
Norm: 153.54, NNZs: 60198, Bias: -1.310341, T: 475000, Avg. loss: 0.001695
Total training time: 0.37 seconds.
-- Epoch 20
Norm: 153.30, NNZs: 62114, Bias: -1.519685, T: 475000, Avg. loss: 0.001094
Total training time: 0.37 seconds.
-- Epoch 20
Norm: 156.08, NNZs: 62402, Bias: -1.432737, T: 475000, Avg. loss: 0.001639
Total training time: 0.34 seconds.
-- Epoch 20
Norm: 184.34, NNZs: 60863, Bias: -0.826838, T: 575000, Avg. loss: 0.001632
Total training time: 0.35 seconds.
Norm: 157.05, NNZs: 63398, Bias: -1.226084, T: 475000, Avg. loss: 0.001287
Total training time: 0.39 seconds.
Convergence after 23 epochs took 0.35 seconds
-- Epoch 20
Norm: 153.81, NNZs: 60198, Bias: -1.308723, T: 500000, Avg. loss: 0.001701
Total training time: 0.38 seconds.
-- Epoch 21
Norm: 153.43, NNZs: 62114, Bias: -1.522205, T: 500000, Avg. loss: 0.000800
Total training time: 0.39 seconds.
Convergence after 20 epochs took 0.39 seconds
Norm: 156.25, NNZs: 62402, Bias: -1.434435, T: 500000, Avg. loss: 0.001173
Total training time: 0.36 seconds.
-- Epoch 21
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.4s remaining:    0.6s
Norm: 157.19, NNZs: 63398, Bias: -1.226327, T: 500000, Avg. loss: 0.000999
Total training time: 0.40 seconds.
Convergence after 20 epochs took 0.40 seconds
Norm: 154.03, NNZs: 60199, Bias: -1.312583, T: 525000, Avg. loss: 0.001406
Total training time: 0.39 seconds.
Convergence after 21 epochs took 0.39 seconds
Norm: 156.41, NNZs: 62402, Bias: -1.435662, T: 525000, Avg. loss: 0.001177
Total training time: 0.37 seconds.
Convergence after 21 epochs took 0.37 seconds
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.4s finished
train time: 0.509s
test time:  0.056s
accuracy:   0.331


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[LibSVM]-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 40.79, NNZs: 50010, Bias: -0.972328, T: 20000, Avg. loss: 0.266715
Total training time: 0.02 seconds.
Norm: 46.61, NNZs: 53108, Bias: -0.905558, T: 20000, Avg. loss: 0.303940
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 48.06, NNZs: 53508, Bias: -0.969197, T: 20000, Avg. loss: 0.339644
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 2
Norm: 61.60, NNZs: 49928, Bias: -0.635239, T: 20000, Avg. loss: 0.379431
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 1
Norm: 44.22, NNZs: 52520, Bias: -1.088445, T: 20000, Avg. loss: 0.284524
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 74.18, NNZs: 56509, Bias: -0.826120, T: 40000, Avg. loss: 0.210522
Total training time: 0.03 seconds.
Norm: 67.35, NNZs: 53501, Bias: -0.942554, T: 40000, Avg. loss: 0.198449
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 3
Norm: 76.83, NNZs: 56879, Bias: -1.012985, T: 40000, Avg. loss: 0.240160
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 85.08, NNZs: 52199, Bias: -0.626590, T: 40000, Avg. loss: 0.218057
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 62.33, NNZs: 52323, Bias: -0.677934, T: 20000, Avg. loss: 0.407558
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 42.14, NNZs: 51530, Bias: -1.080486, T: 20000, Avg. loss: 0.282042
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 85.78, NNZs: 54601, Bias: -0.918278, T: 60000, Avg. loss: 0.136514
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 40.46, NNZs: 50257, Bias: -1.019462, T: 20000, Avg. loss: 0.263937
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 92.12, NNZs: 57454, Bias: -1.020201, T: 60000, Avg. loss: 0.139657
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 69.72, NNZs: 55419, Bias: -1.079943, T: 40000, Avg. loss: 0.207418
Total training time: 0.05 seconds.
-- Epoch 3
Norm: 101.13, NNZs: 53073, Bias: -0.627214, T: 60000, Avg. loss: 0.144139
Total training time: 0.05 seconds.
-- Epoch 4
Norm: 70.90, NNZs: 56088, Bias: -1.156372, T: 40000, Avg. loss: 0.199281
Total training time: 0.05 seconds.
Norm: 98.40, NNZs: 54964, Bias: -1.066791, T: 80000, Avg. loss: 0.093698
Total training time: 0.06 seconds.
-- Epoch 3
-- Epoch 5
Norm: 96.15, NNZs: 57563, Bias: -1.070622, T: 60000, Avg. loss: 0.163927
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 66.96, NNZs: 54180, Bias: -1.022969, T: 40000, Avg. loss: 0.195169
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 86.85, NNZs: 54674, Bias: -0.613868, T: 40000, Avg. loss: 0.242709
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 113.15, NNZs: 53405, Bias: -0.675457, T: 80000, Avg. loss: 0.101923
Total training time: 0.07 seconds.
-- Epoch 5
Norm: 107.33, NNZs: 55116, Bias: -1.061606, T: 100000, Avg. loss: 0.064429
Total training time: 0.08 seconds.
-- Epoch 6
Norm: 85.01, NNZs: 55173, Bias: -1.106167, T: 60000, Avg. loss: 0.133069
Total training time: 0.08 seconds.
-- Epoch 4
Norm: 121.76, NNZs: 53543, Bias: -0.709202, T: 100000, Avg. loss: 0.071865
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 109.79, NNZs: 57872, Bias: -1.049634, T: 80000, Avg. loss: 0.114007
Total training time: 0.09 seconds.
Norm: 113.59, NNZs: 55188, Bias: -1.146057, T: 120000, Avg. loss: 0.044600
Total training time: 0.09 seconds.
-- Epoch 5
-- Epoch 7
Norm: 104.34, NNZs: 57804, Bias: -0.982987, T: 80000, Avg. loss: 0.091616
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 88.65, NNZs: 56977, Bias: -1.126117, T: 60000, Avg. loss: 0.133781
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 97.66, NNZs: 55536, Bias: -1.176288, T: 80000, Avg. loss: 0.091641
Total training time: 0.09 seconds.
Norm: 104.35, NNZs: 55451, Bias: -0.697780, T: 60000, Avg. loss: 0.166056
Total training time: 0.08 seconds.
-- Epoch 5
-- Epoch 4
Norm: 128.03, NNZs: 53618, Bias: -0.713112, T: 120000, Avg. loss: 0.050639
Total training time: 0.10 seconds.
-- Epoch 7
Norm: 88.18, NNZs: 56379, Bias: -1.139349, T: 60000, Avg. loss: 0.141327
Total training time: 0.10 seconds.
-- Epoch 4
Norm: 119.55, NNZs: 58007, Bias: -1.080943, T: 100000, Avg. loss: 0.080032
Total training time: 0.10 seconds.
Norm: 118.22, NNZs: 55245, Bias: -1.097306, T: 140000, Avg. loss: 0.031786
Total training time: 0.10 seconds.
-- Epoch 6
-- Epoch 8
Norm: 106.52, NNZs: 55723, Bias: -1.152247, T: 100000, Avg. loss: 0.062654
Total training time: 0.11 seconds.
-- Epoch 6
Norm: 132.56, NNZs: 53663, Bias: -0.709605, T: 140000, Avg. loss: 0.036013
Total training time: 0.11 seconds.
-- Epoch 8
Norm: 112.97, NNZs: 57941, Bias: -0.995371, T: 100000, Avg. loss: 0.062667
Total training time: 0.12 seconds.
-- Epoch 6
Norm: 121.36, NNZs: 55263, Bias: -1.135004, T: 160000, Avg. loss: 0.021718
Total training time: 0.12 seconds.
-- Epoch 9
Norm: 126.64, NNZs: 58087, Bias: -1.152080, T: 120000, Avg. loss: 0.056704
Total training time: 0.12 seconds.
-- Epoch 7
Norm: 100.51, NNZs: 57268, Bias: -1.299389, T: 80000, Avg. loss: 0.090316
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 112.69, NNZs: 55776, Bias: -1.209115, T: 120000, Avg. loss: 0.043220
Total training time: 0.12 seconds.
-- Epoch 7
Norm: 117.24, NNZs: 55706, Bias: -0.702216, T: 80000, Avg. loss: 0.118207
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 135.81, NNZs: 53691, Bias: -0.744574, T: 160000, Avg. loss: 0.025638
Total training time: 0.13 seconds.
-- Epoch 9
Norm: 123.63, NNZs: 55270, Bias: -1.157002, T: 180000, Avg. loss: 0.015458
Total training time: 0.13 seconds.
-- Epoch 10
Norm: 101.19, NNZs: 56729, Bias: -1.168589, T: 80000, Avg. loss: 0.096580
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 131.81, NNZs: 58107, Bias: -1.160342, T: 140000, Avg. loss: 0.040225
Total training time: 0.14 seconds.
-- Epoch 8
Norm: 117.07, NNZs: 55790, Bias: -1.241945, T: 140000, Avg. loss: 0.030145
Total training time: 0.13 seconds.
-- Epoch 8
Norm: 138.20, NNZs: 53705, Bias: -0.749149, T: 180000, Avg. loss: 0.018547
Total training time: 0.14 seconds.
-- Epoch 10
Norm: 118.78, NNZs: 58002, Bias: -1.091871, T: 120000, Avg. loss: 0.042002
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 125.22, NNZs: 55272, Bias: -1.162110, T: 200000, Avg. loss: 0.010795
Total training time: 0.15 seconds.
-- Epoch 11
Norm: 109.15, NNZs: 57378, Bias: -1.257374, T: 100000, Avg. loss: 0.062092
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 120.10, NNZs: 55805, Bias: -1.280465, T: 160000, Avg. loss: 0.020675
Total training time: 0.15 seconds.
-- Epoch 9
Norm: 135.53, NNZs: 58116, Bias: -1.147256, T: 160000, Avg. loss: 0.028302
Total training time: 0.15 seconds.
-- Epoch 9
Norm: 126.70, NNZs: 55906, Bias: -0.738967, T: 100000, Avg. loss: 0.084914
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 140.01, NNZs: 53717, Bias: -0.746958, T: 200000, Avg. loss: 0.013894
Total training time: 0.16 seconds.
-- Epoch 11
Norm: 126.33, NNZs: 55273, Bias: -1.168319, T: 220000, Avg. loss: 0.007512
Total training time: 0.16 seconds.
-- Epoch 12
Norm: 122.31, NNZs: 55827, Bias: -1.258058, T: 180000, Avg. loss: 0.014533
Total training time: 0.16 seconds.
-- Epoch 10
Norm: 138.14, NNZs: 58121, Bias: -1.162824, T: 180000, Avg. loss: 0.019757
Total training time: 0.17 seconds.
-- Epoch 10
Norm: 115.16, NNZs: 57489, Bias: -1.289069, T: 120000, Avg. loss: 0.043051
Total training time: 0.16 seconds.
Norm: 133.75, NNZs: 56010, Bias: -0.720872, T: 120000, Avg. loss: 0.061289
Total training time: 0.16 seconds.
-- Epoch 7
-- Epoch 7
Norm: 110.18, NNZs: 56844, Bias: -1.213817, T: 100000, Avg. loss: 0.066001
Total training time: 0.17 seconds.
-- Epoch 6
Norm: 141.38, NNZs: 53725, Bias: -0.748162, T: 220000, Avg. loss: 0.010411
Total training time: 0.17 seconds.
-- Epoch 12
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
Norm: 122.82, NNZs: 58009, Bias: -1.099223, T: 140000, Avg. loss: 0.028250
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 127.15, NNZs: 55273, Bias: -1.170361, T: 240000, Avg. loss: 0.005531
Total training time: 0.18 seconds.
-- Epoch 13
Norm: 123.84, NNZs: 55827, Bias: -1.283339, T: 200000, Avg. loss: 0.010276
Total training time: 0.18 seconds.
-- Epoch 11
Norm: 139.98, NNZs: 58130, Bias: -1.178287, T: 200000, Avg. loss: 0.013908
Total training time: 0.18 seconds.
-- Epoch 11
Norm: 142.47, NNZs: 53726, Bias: -0.757869, T: 240000, Avg. loss: 0.008279
Total training time: 0.18 seconds.
-- Epoch 13
Norm: 119.37, NNZs: 57525, Bias: -1.345603, T: 140000, Avg. loss: 0.029572
Total training time: 0.18 seconds.
-- Epoch 8
Norm: 125.58, NNZs: 58030, Bias: -1.118121, T: 160000, Avg. loss: 0.019136
Total training time: 0.19 seconds.
-- Epoch 9
Norm: 127.79, NNZs: 55283, Bias: -1.176324, T: 260000, Avg. loss: 0.004268
Total training time: 0.19 seconds.
-- Epoch 14
Norm: 116.53, NNZs: 56905, Bias: -1.245198, T: 120000, Avg. loss: 0.045545
Total training time: 0.19 seconds.
-- Epoch 7
Norm: 124.92, NNZs: 55828, Bias: -1.281186, T: 220000, Avg. loss: 0.007035
Total training time: 0.19 seconds.
-- Epoch 12
Norm: 141.32, NNZs: 58131, Bias: -1.189269, T: 220000, Avg. loss: 0.009986
Total training time: 0.20 seconds.
-- Epoch 12
Norm: 143.27, NNZs: 53726, Bias: -0.756281, T: 260000, Avg. loss: 0.006023
Total training time: 0.20 seconds.
-- Epoch 14
Norm: 128.24, NNZs: 55283, Bias: -1.180019, T: 280000, Avg. loss: 0.003032
Total training time: 0.20 seconds.
-- Epoch 15
Norm: 122.39, NNZs: 57549, Bias: -1.364459, T: 160000, Avg. loss: 0.020679
Total training time: 0.20 seconds.
-- Epoch 9
Norm: 125.66, NNZs: 55828, Bias: -1.290587, T: 240000, Avg. loss: 0.004939
Total training time: 0.20 seconds.
-- Epoch 13
Norm: 142.25, NNZs: 58131, Bias: -1.192506, T: 240000, Avg. loss: 0.006878
Total training time: 0.21 seconds.
-- Epoch 13
Norm: 139.00, NNZs: 56075, Bias: -0.785199, T: 140000, Avg. loss: 0.045224
Total training time: 0.20 seconds.
-- Epoch 8
Norm: 143.88, NNZs: 53726, Bias: -0.761010, T: 280000, Avg. loss: 0.004509
Total training time: 0.21 seconds.
-- Epoch 15
Norm: 120.96, NNZs: 56925, Bias: -1.262052, T: 140000, Avg. loss: 0.031126
Total training time: 0.21 seconds.
-- Epoch 8
Norm: 128.57, NNZs: 55283, Bias: -1.185097, T: 300000, Avg. loss: 0.002194
Total training time: 0.22 seconds.
-- Epoch 16
Norm: 124.45, NNZs: 57556, Bias: -1.374329, T: 180000, Avg. loss: 0.013833
Total training time: 0.21 seconds.
-- Epoch 10
Norm: 127.47, NNZs: 58049, Bias: -1.113898, T: 180000, Avg. loss: 0.012820
Total training time: 0.22 seconds.
-- Epoch 10
Norm: 126.20, NNZs: 55839, Bias: -1.293586, T: 260000, Avg. loss: 0.003537
Total training time: 0.22 seconds.
-- Epoch 14
Norm: 142.93, NNZs: 58131, Bias: -1.191717, T: 260000, Avg. loss: 0.005010
Total training time: 0.22 seconds.
-- Epoch 14
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
Norm: 144.31, NNZs: 53726, Bias: -0.759318, T: 300000, Avg. loss: 0.003186
Total training time: 0.23 seconds.
-- Epoch 16
Norm: 124.04, NNZs: 56949, Bias: -1.299152, T: 160000, Avg. loss: 0.021634
Total training time: 0.23 seconds.
-- Epoch 9
Norm: 128.87, NNZs: 55286, Bias: -1.184820, T: 320000, Avg. loss: 0.002018
Total training time: 0.23 seconds.
-- Epoch 17
Norm: 125.87, NNZs: 57565, Bias: -1.377132, T: 200000, Avg. loss: 0.009442
Total training time: 0.23 seconds.
-- Epoch 11
Norm: 128.78, NNZs: 58053, Bias: -1.126278, T: 200000, Avg. loss: 0.008877
Total training time: 0.23 seconds.
Norm: 126.59, NNZs: 55839, Bias: -1.298384, T: 280000, Avg. loss: 0.002562
Total training time: 0.23 seconds.
-- Epoch 11
-- Epoch 15
Norm: 143.43, NNZs: 58131, Bias: -1.197446, T: 280000, Avg. loss: 0.003696
Total training time: 0.23 seconds.
-- Epoch 15
Norm: 142.97, NNZs: 56091, Bias: -0.809838, T: 160000, Avg. loss: 0.033436
Total training time: 0.23 seconds.
-- Epoch 9
Norm: 144.64, NNZs: 53726, Bias: -0.760367, T: 320000, Avg. loss: 0.002488
Total training time: 0.24 seconds.
-- Epoch 17
Norm: 126.17, NNZs: 56960, Bias: -1.308418, T: 180000, Avg. loss: 0.014641
Total training time: 0.24 seconds.
Norm: 129.11, NNZs: 55286, Bias: -1.184973, T: 340000, Avg. loss: 0.001585
Total training time: 0.24 seconds.
-- Epoch 10
-- Epoch 18
Norm: 126.88, NNZs: 55839, Bias: -1.299333, T: 300000, Avg. loss: 0.001875
Total training time: 0.24 seconds.
-- Epoch 16
Norm: 126.88, NNZs: 57567, Bias: -1.375455, T: 220000, Avg. loss: 0.006642
Total training time: 0.24 seconds.
Norm: 143.78, NNZs: 58132, Bias: -1.198554, T: 300000, Avg. loss: 0.002595
Total training time: 0.25 seconds.
-- Epoch 12
-- Epoch 16
Norm: 129.66, NNZs: 58060, Bias: -1.126549, T: 220000, Avg. loss: 0.005933
Total training time: 0.25 seconds.
-- Epoch 12
Norm: 146.00, NNZs: 56109, Bias: -0.790718, T: 180000, Avg. loss: 0.024987
Total training time: 0.24 seconds.
-- Epoch 10
Norm: 144.88, NNZs: 53726, Bias: -0.764958, T: 340000, Avg. loss: 0.001779
Total training time: 0.25 seconds.
-- Epoch 18
Norm: 129.27, NNZs: 55286, Bias: -1.185486, T: 360000, Avg. loss: 0.001042
Total training time: 0.26 seconds.
-- Epoch 19
Norm: 127.09, NNZs: 55839, Bias: -1.298998, T: 320000, Avg. loss: 0.001328
Total training time: 0.26 seconds.
-- Epoch 17
Norm: 127.74, NNZs: 56964, Bias: -1.311281, T: 200000, Avg. loss: 0.010686
Total training time: 0.26 seconds.
-- Epoch 11
Norm: 144.04, NNZs: 58132, Bias: -1.199806, T: 320000, Avg. loss: 0.001871
Total training time: 0.26 seconds.
-- Epoch 17
Norm: 130.27, NNZs: 58060, Bias: -1.129950, T: 240000, Avg. loss: 0.004089
Total training time: 0.26 seconds.
Norm: 127.55, NNZs: 57569, Bias: -1.385724, T: 240000, Avg. loss: 0.004496
Total training time: 0.26 seconds.
-- Epoch 13
Norm: 148.25, NNZs: 56120, Bias: -0.788277, T: 200000, Avg. loss: 0.018406
Total training time: 0.26 seconds.
-- Epoch 11
Norm: 145.07, NNZs: 53726, Bias: -0.764203, T: 360000, Avg. loss: 0.001398
Total training time: 0.27 seconds.
-- Epoch 19
Norm: 129.41, NNZs: 55286, Bias: -1.187605, T: 380000, Avg. loss: 0.000920
Total training time: 0.27 seconds.
Convergence after 19 epochs took 0.27 seconds
Norm: 127.24, NNZs: 55839, Bias: -1.299524, T: 340000, Avg. loss: 0.000967
Total training time: 0.27 seconds.
-- Epoch 18
-- Epoch 13
Norm: 144.22, NNZs: 58132, Bias: -1.202337, T: 340000, Avg. loss: 0.001337
Total training time: 0.28 seconds.
Norm: 128.84, NNZs: 56970, Bias: -1.321969, T: 220000, Avg. loss: 0.007498
Total training time: 0.27 seconds.
-- Epoch 18
-- Epoch 12
Norm: 130.69, NNZs: 58060, Bias: -1.128211, T: 260000, Avg. loss: 0.002772
Total training time: 0.28 seconds.
-- Epoch 14
Norm: 149.97, NNZs: 56132, Bias: -0.800554, T: 220000, Avg. loss: 0.013925
Total training time: 0.27 seconds.
-- Epoch 12
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
Norm: 145.21, NNZs: 53726, Bias: -0.766109, T: 380000, Avg. loss: 0.001054
Total training time: 0.28 seconds.
-- Epoch 20
Norm: 127.35, NNZs: 55839, Bias: -1.301166, T: 360000, Avg. loss: 0.000775
Total training time: 0.28 seconds.
Convergence after 18 epochs took 0.28 seconds
Norm: 128.02, NNZs: 57569, Bias: -1.388564, T: 260000, Avg. loss: 0.003093
Total training time: 0.28 seconds.
-- Epoch 14
Norm: 144.37, NNZs: 58135, Bias: -1.200277, T: 360000, Avg. loss: 0.001072
Total training time: 0.29 seconds.
-- Epoch 19
Norm: 130.99, NNZs: 58061, Bias: -1.131413, T: 280000, Avg. loss: 0.002022
Total training time: 0.29 seconds.
Norm: 129.61, NNZs: 56975, Bias: -1.324186, T: 240000, Avg. loss: 0.005237
Total training time: 0.29 seconds.
-- Epoch 15
-- Epoch 13
Norm: 151.24, NNZs: 56139, Bias: -0.813914, T: 240000, Avg. loss: 0.010234
Total training time: 0.28 seconds.
[LibSVM]-- Epoch 1
-- Epoch 13
[LibSVM]-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 145.32, NNZs: 53726, Bias: -0.766110, T: 400000, Avg. loss: 0.000769
Total training time: 0.29 seconds.
Convergence after 20 epochs took 0.29 seconds
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 128.34, NNZs: 57569, Bias: -1.392682, T: 280000, Avg. loss: 0.002102
Total training time: 0.30 seconds.
-- Epoch 15
-- Epoch 1
Norm: 144.47, NNZs: 58137, Bias: -1.201513, T: 380000, Avg. loss: 0.000780
-- Epoch 1
Total training time: 0.30 seconds.
-- Epoch 20
Norm: 131.19, NNZs: 58061, Bias: -1.131320, T: 300000, Avg. loss: 0.001365
Total training time: 0.30 seconds.
-- Epoch 16
Norm: 130.16, NNZs: 56975, Bias: -1.329472, T: 260000, Avg. loss: 0.003683
Total training time: 0.30 seconds.
-- Epoch 14
-- Epoch 1
Norm: 152.19, NNZs: 56139, Bias: -0.819218, T: 260000, Avg. loss: 0.007503
Total training time: 0.30 seconds.
-- Epoch 14
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.3s remaining:    0.5s
-- Epoch 1
Norm: 61.83, NNZs: 50199, Bias: -0.562737, T: 20000, Avg. loss: 0.380705
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 1
Norm: 128.57, NNZs: 57569, Bias: -1.392679, T: 300000, Avg. loss: 0.001487
Total training time: 0.31 seconds.
Norm: 144.55, NNZs: 58137, Bias: -1.202535, T: 400000, Avg. loss: 0.000608
Total training time: 0.32 seconds.
-- Epoch 16
Convergence after 20 epochs took 0.32 seconds
-- Epoch 1
Norm: 40.42, NNZs: 49810, Bias: -0.983139, T: 20000, Avg. loss: 0.267378
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 1
Norm: 130.54, NNZs: 56977, Bias: -1.330664, T: 280000, Avg. loss: 0.002566
Total training time: 0.32 seconds.
-- Epoch 15
-- Epoch 1
Norm: 40.32, NNZs: 50895, Bias: -1.020162, T: 20000, Avg. loss: 0.264129
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 85.15, NNZs: 52665, Bias: -0.650798, T: 40000, Avg. loss: 0.216020
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 44.42, NNZs: 52416, Bias: -1.026460, T: 20000, Avg. loss: 0.283868
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 152.90, NNZs: 56139, Bias: -0.814014, T: 280000, Avg. loss: 0.005567
Total training time: 0.33 seconds.
-- Epoch 15
Norm: 67.33, NNZs: 53540, Bias: -0.947196, T: 40000, Avg. loss: 0.198151
Total training time: 0.04 seconds.
-- Epoch 3
Norm: 130.84, NNZs: 56977, Bias: -1.334106, T: 300000, Avg. loss: 0.001980
Total training time: 0.34 seconds.
-- Epoch 16
Norm: 131.34, NNZs: 58061, Bias: -1.133146, T: 320000, Avg. loss: 0.000963
Total training time: 0.34 seconds.
-- Epoch 17
Norm: 40.44, NNZs: 50752, Bias: -0.994468, T: 20000, Avg. loss: 0.264087
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 62.70, NNZs: 51881, Bias: -0.606049, T: 20000, Avg. loss: 0.405533
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 101.35, NNZs: 53346, Bias: -0.684599, T: 60000, Avg. loss: 0.144906
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 40.42, NNZs: 49743, Bias: -0.941535, T: 20000, Avg. loss: 0.267387
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 128.72, NNZs: 57569, Bias: -1.394648, T: 320000, Avg. loss: 0.000995
Total training time: 0.35 seconds.
-- Epoch 17
Norm: 131.03, NNZs: 56977, Bias: -1.333610, T: 320000, Avg. loss: 0.001262
Total training time: 0.36 seconds.
-- Epoch 17
Norm: 131.44, NNZs: 58061, Bias: -1.133509, T: 340000, Avg. loss: 0.000669
Total training time: 0.36 seconds.
-- Epoch 18
Norm: 48.44, NNZs: 53743, Bias: -0.931102, T: 20000, Avg. loss: 0.339198
Total training time: 0.06 seconds.
-- Epoch 2
Norm: 42.37, NNZs: 51465, Bias: -1.017304, T: 20000, Avg. loss: 0.280916
Total training time: 0.07 seconds.
Norm: 46.18, NNZs: 54161, Bias: -0.873271, T: 20000, Avg. loss: 0.305636
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 2
Norm: 85.89, NNZs: 54646, Bias: -1.019058, T: 60000, Avg. loss: 0.137481
Total training time: 0.06 seconds.
-- Epoch 4
Norm: 112.99, NNZs: 53672, Bias: -0.670817, T: 80000, Avg. loss: 0.100768
Total training time: 0.08 seconds.
-- Epoch 5
Norm: 87.04, NNZs: 54016, Bias: -0.615972, T: 40000, Avg. loss: 0.238494
Total training time: 0.05 seconds.
-- Epoch 3
Norm: 61.27, NNZs: 49965, Bias: -0.566032, T: 20000, Avg. loss: 0.380509
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 71.04, NNZs: 55719, Bias: -1.157508, T: 40000, Avg. loss: 0.197401
Total training time: 0.07 seconds.
Norm: 153.42, NNZs: 56139, Bias: -0.814212, T: 300000, Avg. loss: 0.004103
Total training time: 0.36 seconds.
-- Epoch 3
-- Epoch 16
Norm: 67.59, NNZs: 53912, Bias: -0.953606, T: 40000, Avg. loss: 0.198262
Total training time: 0.05 seconds.
-- Epoch 3
Norm: 66.96, NNZs: 54549, Bias: -0.996922, T: 40000, Avg. loss: 0.195183
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 62.40, NNZs: 52100, Bias: -0.570206, T: 20000, Avg. loss: 0.406928
Total training time: 0.06 seconds.
Norm: 48.46, NNZs: 54344, Bias: -0.991933, T: 20000, Avg. loss: 0.338973
Total training time: 0.07 seconds.
-- Epoch 2
-- Epoch 2
Norm: 131.18, NNZs: 56977, Bias: -1.333065, T: 340000, Avg. loss: 0.000982
Total training time: 0.37 seconds.
-- Epoch 18
Norm: 46.26, NNZs: 53814, Bias: -0.907533, T: 20000, Avg. loss: 0.305111
Total training time: 0.09 seconds.
-- Epoch 2
Norm: 128.83, NNZs: 57569, Bias: -1.393132, T: 340000, Avg. loss: 0.000696
Total training time: 0.38 seconds.
-- Epoch 18
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
Norm: 98.63, NNZs: 54984, Bias: -1.094408, T: 80000, Avg. loss: 0.093975
Total training time: 0.08 seconds.
-- Epoch 5
Norm: 121.55, NNZs: 53813, Bias: -0.715138, T: 100000, Avg. loss: 0.072091
Total training time: 0.09 seconds.
-- Epoch 6
Norm: 44.16, NNZs: 52644, Bias: -1.100877, T: 20000, Avg. loss: 0.284433
Total training time: 0.09 seconds.
-- Epoch 2
Norm: 66.87, NNZs: 54379, Bias: -1.013293, T: 40000, Avg. loss: 0.195747
Total training time: 0.08 seconds.
Norm: 42.23, NNZs: 52076, Bias: -1.067525, T: 20000, Avg. loss: 0.281724
Total training time: 0.08 seconds.
-- Epoch 3
-- Epoch 2
Norm: 153.81, NNZs: 56139, Bias: -0.816363, T: 320000, Avg. loss: 0.003093
Total training time: 0.38 seconds.
-- Epoch 17
Norm: 104.20, NNZs: 54862, Bias: -0.709990, T: 60000, Avg. loss: 0.162337
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 85.53, NNZs: 54859, Bias: -1.071524, T: 60000, Avg. loss: 0.135440
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 131.28, NNZs: 56977, Bias: -1.333766, T: 360000, Avg. loss: 0.000675
Total training time: 0.39 seconds.
-- Epoch 19
Norm: 88.54, NNZs: 56567, Bias: -1.151823, T: 60000, Avg. loss: 0.131701
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 74.24, NNZs: 57266, Bias: -0.844231, T: 40000, Avg. loss: 0.212857
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 128.07, NNZs: 53888, Bias: -0.691759, T: 120000, Avg. loss: 0.052916
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 154.09, NNZs: 56140, Bias: -0.814835, T: 340000, Avg. loss: 0.002203
Total training time: 0.39 seconds.
-- Epoch 18
Norm: 76.82, NNZs: 56973, Bias: -1.005050, T: 40000, Avg. loss: 0.238097
Total training time: 0.10 seconds.
Norm: 107.47, NNZs: 55194, Bias: -1.073490, T: 100000, Avg. loss: 0.063633
Total training time: 0.10 seconds.
-- Epoch 3
-- Epoch 6
Norm: 116.82, NNZs: 55206, Bias: -0.661161, T: 80000, Avg. loss: 0.115102
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 98.14, NNZs: 55158, Bias: -1.102209, T: 80000, Avg. loss: 0.092993
Total training time: 0.09 seconds.
-- Epoch 5
[LibSVM]-- Epoch 1
Norm: 131.36, NNZs: 56977, Bias: -1.334225, T: 380000, Avg. loss: 0.000502
Total training time: 0.41 seconds.
Convergence after 19 epochs took 0.41 seconds
-- Epoch 1
Norm: 69.85, NNZs: 56050, Bias: -1.012758, T: 40000, Avg. loss: 0.206184
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 84.58, NNZs: 52459, Bias: -0.615867, T: 40000, Avg. loss: 0.217049
Total training time: 0.12 seconds.
-- Epoch 3
Norm: 132.71, NNZs: 53949, Bias: -0.703179, T: 140000, Avg. loss: 0.037339
Total training time: 0.13 seconds.
-- Epoch 8
Norm: 131.51, NNZs: 58061, Bias: -1.133697, T: 360000, Avg. loss: 0.000466
Total training time: 0.42 seconds.
Convergence after 18 epochs took 0.42 seconds
Norm: 128.90, NNZs: 57569, Bias: -1.394314, T: 360000, Avg. loss: 0.000488
Total training time: 0.42 seconds.
Convergence after 18 epochs took 0.42 seconds
Norm: 100.51, NNZs: 56948, Bias: -1.239124, T: 80000, Avg. loss: 0.088564
Total training time: 0.12 seconds.
Norm: 87.17, NNZs: 54702, Bias: -0.564389, T: 40000, Avg. loss: 0.242549
Total training time: 0.11 seconds.
-- Epoch 5
-- Epoch 3
-- Epoch 1
Norm: 126.06, NNZs: 55407, Bias: -0.704350, T: 100000, Avg. loss: 0.082532
Total training time: 0.11 seconds.
-- Epoch 1
-- Epoch 6
-- Epoch 1
Norm: 107.08, NNZs: 55243, Bias: -1.118457, T: 100000, Avg. loss: 0.064530
Total training time: 0.10 seconds.
-- Epoch 6
Norm: 84.99, NNZs: 55424, Bias: -1.108111, T: 60000, Avg. loss: 0.134503
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 88.22, NNZs: 57010, Bias: -1.099095, T: 60000, Avg. loss: 0.140746
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 92.37, NNZs: 57997, Bias: -0.966928, T: 60000, Avg. loss: 0.140673
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 85.00, NNZs: 55529, Bias: -1.125354, T: 60000, Avg. loss: 0.133794
Norm: 136.12, NNZs: 53988, Bias: -0.709848, T: 160000, Avg. loss: 0.026867
Total training time: 0.15 seconds.
Total training time: 0.15 seconds.
-- Epoch 9
-- Epoch 4
Norm: 108.90, NNZs: 57104, Bias: -1.255253, T: 100000, Avg. loss: 0.060523
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 69.94, NNZs: 55449, Bias: -0.970670, T: 40000, Avg. loss: 0.205742
Total training time: 0.15 seconds.
-- Epoch 3
Norm: 132.90, NNZs: 55492, Bias: -0.728914, T: 120000, Avg. loss: 0.059700
Total training time: 0.13 seconds.
Norm: 101.03, NNZs: 53347, Bias: -0.587984, T: 60000, Avg. loss: 0.146666
Total training time: 0.15 seconds.
-- Epoch 4
-- Epoch 1
Norm: 40.68, NNZs: 50670, Bias: -1.065460, T: 20000, Avg. loss: 0.263971
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 70.96, NNZs: 56163, Bias: -1.161192, T: 40000, Avg. loss: 0.196926
Total training time: 0.16 seconds.
-- Epoch 3
Norm: 100.87, NNZs: 57346, Bias: -1.157918, T: 80000, Avg. loss: 0.095116
Total training time: 0.14 seconds.
Norm: 104.80, NNZs: 55508, Bias: -0.650873, T: 60000, Avg. loss: 0.165328
Total training time: 0.14 seconds.
-- Epoch 5
-- Epoch 4
Norm: 113.64, NNZs: 55270, Bias: -1.199502, T: 120000, Avg. loss: 0.044451
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 114.75, NNZs: 57149, Bias: -1.284523, T: 120000, Avg. loss: 0.041213
Total training time: 0.16 seconds.
-- Epoch 7
Norm: 97.73, NNZs: 55906, Bias: -1.181525, T: 80000, Avg. loss: 0.092603
Total training time: 0.17 seconds.
-- Epoch 5
Norm: 76.63, NNZs: 57408, Bias: -0.994520, T: 40000, Avg. loss: 0.236799
Total training time: 0.16 seconds.
Norm: 154.31, NNZs: 56140, Bias: -0.817233, T: 360000, Avg. loss: 0.001686
Total training time: 0.45 seconds.
Norm: 97.67, NNZs: 55708, Bias: -1.136655, T: 80000, Avg. loss: 0.092570
Total training time: 0.15 seconds.
-- Epoch 3
-- Epoch 19
-- Epoch 5
-- Epoch 7
Norm: 113.38, NNZs: 55330, Bias: -1.213506, T: 120000, Avg. loss: 0.044970
Total training time: 0.14 seconds.
-- Epoch 7
Norm: 138.66, NNZs: 53991, Bias: -0.728032, T: 180000, Avg. loss: 0.019896
Total training time: 0.17 seconds.
-- Epoch 10
Norm: 41.98, NNZs: 51810, Bias: -0.980292, T: 20000, Avg. loss: 0.281933
Norm: 43.96, NNZs: 52591, Bias: -1.072486, T: 20000, Avg. loss: 0.285204
Total training time: 0.06 seconds.
-- Epoch 2
-- Epoch 1
Total training time: 0.04 seconds.
Norm: 96.04, NNZs: 57705, Bias: -1.005011, T: 60000, Avg. loss: 0.163318
Total training time: 0.16 seconds.
-- Epoch 4
-- Epoch 2
Norm: 73.89, NNZs: 56947, Bias: -0.838967, T: 40000, Avg. loss: 0.212317
Total training time: 0.17 seconds.
-- Epoch 1
Norm: 61.42, NNZs: 49845, Bias: -0.580556, T: 20000, Avg. loss: 0.380525
Norm: 104.63, NNZs: 58316, Bias: -1.019415, T: 80000, Avg. loss: 0.093247
Total training time: 0.14 seconds.
-- Epoch 3
-- Epoch 5
Norm: 109.71, NNZs: 57450, Bias: -1.191940, T: 100000, Avg. loss: 0.065506
Total training time: 0.16 seconds.
-- Epoch 6
Norm: 118.92, NNZs: 57172, Bias: -1.321343, T: 140000, Avg. loss: 0.028863
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 118.16, NNZs: 55298, Bias: -1.152796, T: 140000, Avg. loss: 0.031034
Total training time: 0.17 seconds.
-- Epoch 8
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 62.45, NNZs: 52314, Bias: -0.528323, T: 20000, Avg. loss: 0.409685
Total training time: 0.04 seconds.
Norm: 67.27, NNZs: 54272, Bias: -1.026967, T: 40000, Avg. loss: 0.195136
Total training time: 0.06 seconds.
-- Epoch 3
-- Epoch 2
Norm: 109.49, NNZs: 57968, Bias: -1.065609, T: 80000, Avg. loss: 0.113139
Total training time: 0.18 seconds.
Norm: 70.71, NNZs: 55958, Bias: -1.141475, T: 40000, Avg. loss: 0.199927
Total training time: 0.08 seconds.
-- Epoch 5
-- Epoch 3
Norm: 112.81, NNZs: 53637, Bias: -0.599932, T: 80000, Avg. loss: 0.102637
Total training time: 0.19 seconds.
-- Epoch 5
Norm: 121.82, NNZs: 57209, Bias: -1.345992, T: 160000, Avg. loss: 0.019717
Total training time: 0.19 seconds.
-- Epoch 9
Norm: 117.48, NNZs: 55772, Bias: -0.631125, T: 80000, Avg. loss: 0.115718
Total training time: 0.17 seconds.
-- Epoch 5
Norm: 106.64, NNZs: 56047, Bias: -1.184723, T: 100000, Avg. loss: 0.063204
Total training time: 0.20 seconds.
-- Epoch 6
Norm: 140.52, NNZs: 54000, Bias: -0.730470, T: 200000, Avg. loss: 0.014357
Total training time: 0.20 seconds.
Norm: 40.36, NNZs: 49341, Bias: -0.951016, T: 20000, Avg. loss: 0.267508
Total training time: 0.08 seconds.
-- Epoch 11
-- Epoch 2
Norm: 117.96, NNZs: 55388, Bias: -1.175301, T: 140000, Avg. loss: 0.031368
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 106.58, NNZs: 55845, Bias: -1.196323, T: 100000, Avg. loss: 0.063955
Total training time: 0.19 seconds.
-- Epoch 6
Norm: 119.29, NNZs: 58104, Bias: -1.052393, T: 100000, Avg. loss: 0.079605
Total training time: 0.20 seconds.
-- Epoch 6
Norm: 69.89, NNZs: 55734, Bias: -0.981409, T: 40000, Avg. loss: 0.205916
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 88.33, NNZs: 56910, Bias: -1.203400, T: 60000, Avg. loss: 0.134158
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 115.94, NNZs: 57501, Bias: -1.235462, T: 120000, Avg. loss: 0.045049
Total training time: 0.20 seconds.
Norm: 113.23, NNZs: 58410, Bias: -1.049776, T: 100000, Avg. loss: 0.063576
Total training time: 0.18 seconds.
-- Epoch 7
-- Epoch 6
Norm: 123.80, NNZs: 57220, Bias: -1.353797, T: 180000, Avg. loss: 0.013194
Total training time: 0.21 seconds.
-- Epoch 10
Norm: 85.20, NNZs: 52245, Bias: -0.641999, T: 40000, Avg. loss: 0.217494
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 141.91, NNZs: 54011, Bias: -0.738531, T: 220000, Avg. loss: 0.010659
Total training time: 0.22 seconds.
-- Epoch 12
Norm: 121.27, NNZs: 55318, Bias: -1.184116, T: 160000, Avg. loss: 0.021622
Total training time: 0.21 seconds.
Norm: 48.04, NNZs: 53774, Bias: -0.962002, T: 20000, Avg. loss: 0.339611
Total training time: 0.05 seconds.
-- Epoch 9
-- Epoch 2
Norm: 121.52, NNZs: 53823, Bias: -0.658765, T: 100000, Avg. loss: 0.073648
Total training time: 0.22 seconds.
-- Epoch 6
Norm: 126.27, NNZs: 58154, Bias: -1.126365, T: 120000, Avg. loss: 0.056270
Total training time: 0.22 seconds.
-- Epoch 7
Norm: 87.99, NNZs: 56283, Bias: -1.101378, T: 60000, Avg. loss: 0.138643
Total training time: 0.23 seconds.
-- Epoch 4
Norm: 46.19, NNZs: 53361, Bias: -0.864169, T: 20000, Avg. loss: 0.305352
Total training time: 0.06 seconds.
-- Epoch 2
Norm: 100.57, NNZs: 57196, Bias: -1.224587, T: 80000, Avg. loss: 0.091465
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 121.06, NNZs: 55414, Bias: -1.230390, T: 160000, Avg. loss: 0.021524
Total training time: 0.21 seconds.
Norm: 112.82, NNZs: 55893, Bias: -1.224957, T: 120000, Avg. loss: 0.043554
Total training time: 0.22 seconds.
-- Epoch 9
-- Epoch 7
Norm: 142.99, NNZs: 54011, Bias: -0.738479, T: 240000, Avg. loss: 0.008148
Total training time: 0.24 seconds.
-- Epoch 13
Norm: 88.50, NNZs: 56935, Bias: -1.165505, T: 60000, Avg. loss: 0.131949
Total training time: 0.24 seconds.
-- Epoch 4
Norm: 112.78, NNZs: 56121, Bias: -1.254368, T: 120000, Avg. loss: 0.043171
Total training time: 0.24 seconds.
-- Epoch 7
Norm: 126.75, NNZs: 55889, Bias: -0.684448, T: 100000, Avg. loss: 0.082623
Total training time: 0.21 seconds.
Norm: 138.09, NNZs: 55545, Bias: -0.748049, T: 140000, Avg. loss: 0.044283
Total training time: 0.22 seconds.
-- Epoch 6
-- Epoch 8
Norm: 91.89, NNZs: 57870, Bias: -1.005824, T: 60000, Avg. loss: 0.142108
Total training time: 0.24 seconds.
-- Epoch 4
Norm: 88.32, NNZs: 56598, Bias: -1.092066, T: 60000, Avg. loss: 0.139449
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 154.47, NNZs: 56140, Bias: -0.817610, T: 380000, Avg. loss: 0.001244
Total training time: 0.53 seconds.
Norm: 131.46, NNZs: 58201, Bias: -1.142884, T: 140000, Avg. loss: 0.040546
Total training time: 0.23 seconds.
-- Epoch 20
Norm: 101.75, NNZs: 52992, Bias: -0.666336, T: 60000, Avg. loss: 0.146060
Total training time: 0.13 seconds.
-- Epoch 8
-- Epoch 4
Norm: 119.04, NNZs: 58462, Bias: -1.097940, T: 120000, Avg. loss: 0.042360
Total training time: 0.21 seconds.
-- Epoch 7
Norm: 125.13, NNZs: 57232, Bias: -1.372819, T: 200000, Avg. loss: 0.008920
Total training time: 0.24 seconds.
-- Epoch 11
Norm: 85.23, NNZs: 55186, Bias: -1.167791, T: 60000, Avg. loss: 0.134210
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 120.38, NNZs: 57516, Bias: -1.253418, T: 140000, Avg. loss: 0.031455
Total training time: 0.23 seconds.
-- Epoch 8
Norm: 95.86, NNZs: 58189, Bias: -1.069382, T: 60000, Avg. loss: 0.162966
Total training time: 0.24 seconds.
-- Epoch 4
Norm: 87.25, NNZs: 54508, Bias: -0.588705, T: 40000, Avg. loss: 0.242382
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 128.10, NNZs: 53898, Bias: -0.693016, T: 120000, Avg. loss: 0.053748
Total training time: 0.26 seconds.
-- Epoch 7
Norm: 67.62, NNZs: 53454, Bias: -0.908608, T: 40000, Avg. loss: 0.198572
Total training time: 0.14 seconds.
-- Epoch 3
Norm: 143.76, NNZs: 54016, Bias: -0.749009, T: 260000, Avg. loss: 0.005895
Total training time: 0.27 seconds.
Norm: 76.39, NNZs: 57088, Bias: -1.056171, T: 40000, Avg. loss: 0.240594
Total training time: 0.09 seconds.
-- Epoch 14
-- Epoch 3
Norm: 74.01, NNZs: 56763, Bias: -0.917257, T: 40000, Avg. loss: 0.211996
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 123.50, NNZs: 57543, Bias: -1.282894, T: 160000, Avg. loss: 0.021958
Total training time: 0.25 seconds.
-- Epoch 9
Norm: 123.54, NNZs: 55322, Bias: -1.200250, T: 180000, Avg. loss: 0.015495
Total training time: 0.26 seconds.
-- Epoch 10
Norm: 141.98, NNZs: 55579, Bias: -0.765109, T: 160000, Avg. loss: 0.032864
Total training time: 0.25 seconds.
-- Epoch 9
Norm: 113.46, NNZs: 53262, Bias: -0.663949, T: 80000, Avg. loss: 0.100122
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 117.22, NNZs: 55918, Bias: -1.265339, T: 140000, Avg. loss: 0.030255
Total training time: 0.26 seconds.
Norm: 100.97, NNZs: 56879, Bias: -1.129157, T: 80000, Avg. loss: 0.093937
Total training time: 0.15 seconds.
Norm: 135.23, NNZs: 58246, Bias: -1.148796, T: 160000, Avg. loss: 0.029016
Total training time: 0.27 seconds.
-- Epoch 8
-- Epoch 5
-- Epoch 9
Norm: 109.14, NNZs: 57331, Bias: -1.291004, T: 100000, Avg. loss: 0.062485
Total training time: 0.16 seconds.
-- Epoch 6
Norm: 117.22, NNZs: 56147, Bias: -1.254082, T: 140000, Avg. loss: 0.030292
Total training time: 0.28 seconds.
-- Epoch 8
Norm: 123.32, NNZs: 55439, Bias: -1.226357, T: 180000, Avg. loss: 0.015251
Total training time: 0.25 seconds.
-- Epoch 10
Norm: 109.55, NNZs: 58494, Bias: -1.095787, T: 80000, Avg. loss: 0.113531
Total training time: 0.27 seconds.
-- Epoch 5
Norm: 100.29, NNZs: 57267, Bias: -1.272340, T: 80000, Avg. loss: 0.088173
Total training time: 0.28 seconds.
-- Epoch 5
Norm: 125.69, NNZs: 57549, Bias: -1.302841, T: 180000, Avg. loss: 0.015277
Total training time: 0.27 seconds.
-- Epoch 10
Norm: 133.61, NNZs: 55942, Bias: -0.680700, T: 120000, Avg. loss: 0.059172
Total training time: 0.26 seconds.
-- Epoch 7
Norm: 100.61, NNZs: 56669, Bias: -1.132385, T: 80000, Avg. loss: 0.094163
Total training time: 0.29 seconds.
-- Epoch 5
Norm: 154.59, NNZs: 56140, Bias: -0.818295, T: 400000, Avg. loss: 0.000925
Total training time: 0.57 seconds.
-- Epoch 21
Norm: 123.17, NNZs: 58498, Bias: -1.099886, T: 140000, Avg. loss: 0.029258
Total training time: 0.26 seconds.
Norm: 97.91, NNZs: 55556, Bias: -1.166782, T: 80000, Avg. loss: 0.091763
Total training time: 0.16 seconds.
Norm: 126.07, NNZs: 57232, Bias: -1.366688, T: 220000, Avg. loss: 0.006099
-- Epoch 8
Norm: 144.37, NNZs: 54039, Bias: -0.745390, T: 280000, Avg. loss: 0.004489
Total training time: 0.30 seconds.
-- Epoch 15
Norm: 104.07, NNZs: 58166, Bias: -1.009318, T: 80000, Avg. loss: 0.093972
Total training time: 0.29 seconds.
-- Epoch 5
Norm: 125.14, NNZs: 55330, Bias: -1.197759, T: 200000, Avg. loss: 0.010874
Total training time: 0.29 seconds.
-- Epoch 5
Norm: 132.91, NNZs: 53930, Bias: -0.646823, T: 140000, Avg. loss: 0.038411
Total training time: 0.30 seconds.
-- Epoch 8
[LibSVM]-- Epoch 1
Norm: 92.01, NNZs: 57517, Bias: -0.944595, T: 60000, Avg. loss: 0.140987
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 115.21, NNZs: 57401, Bias: -1.315455, T: 120000, Avg. loss: 0.043002
Total training time: 0.19 seconds.
-- Epoch 7
-- Epoch 11
Norm: 127.27, NNZs: 57552, Bias: -1.307494, T: 200000, Avg. loss: 0.010877
Total training time: 0.29 seconds.
-- Epoch 11
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 144.93, NNZs: 55591, Bias: -0.751307, T: 180000, Avg. loss: 0.024367
Total training time: 0.29 seconds.
-- Epoch 1
Norm: 95.76, NNZs: 57803, Bias: -1.123407, T: 60000, Avg. loss: 0.164532
Total training time: 0.13 seconds.
-- Epoch 10
-- Epoch 4
Norm: 137.89, NNZs: 58256, Bias: -1.157387, T: 180000, Avg. loss: 0.020272
Total training time: 0.30 seconds.
-- Epoch 10
Total training time: 0.30 seconds.
-- Epoch 12
-- Epoch 1
-- Epoch 1
Norm: 121.88, NNZs: 53463, Bias: -0.696104, T: 100000, Avg. loss: 0.070389
Total training time: 0.20 seconds.
-- Epoch 6
-- Epoch 1
Norm: 104.79, NNZs: 55277, Bias: -0.656189, T: 60000, Avg. loss: 0.164685
Total training time: 0.17 seconds.
-- Epoch 4
Norm: 120.18, NNZs: 56161, Bias: -1.315254, T: 160000, Avg. loss: 0.020424
Total training time: 0.32 seconds.
-- Epoch 9
Norm: 112.89, NNZs: 58302, Bias: -1.046513, T: 100000, Avg. loss: 0.066069
Total training time: 0.33 seconds.
-- Epoch 6
Norm: 139.82, NNZs: 58269, Bias: -1.162991, T: 200000, Avg. loss: 0.014504
Total training time: 0.32 seconds.
-- Epoch 11
Norm: 85.79, NNZs: 54510, Bias: -0.961205, T: 60000, Avg. loss: 0.136295
Total training time: 0.21 seconds.
Norm: 136.36, NNZs: 53971, Bias: -0.674994, T: 160000, Avg. loss: 0.027557
Total training time: 0.33 seconds.
-- Epoch 4
Norm: 46.24, NNZs: 53489, Bias: -0.899251, T: 20000, Avg. loss: 0.305223
Total training time: 0.03 seconds.
-- Epoch 9
-- Epoch 2
Norm: 104.22, NNZs: 57776, Bias: -1.000135, T: 80000, Avg. loss: 0.094237
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 126.26, NNZs: 55334, Bias: -1.210054, T: 220000, Avg. loss: 0.007588
Total training time: 0.33 seconds.
-- Epoch 12
Norm: 119.24, NNZs: 58625, Bias: -1.070361, T: 100000, Avg. loss: 0.078626
Total training time: 0.33 seconds.
-- Epoch 6
Norm: 138.80, NNZs: 56008, Bias: -0.727354, T: 140000, Avg. loss: 0.043858
Total training time: 0.32 seconds.
-- Epoch 8
Norm: 144.81, NNZs: 54040, Bias: -0.746134, T: 300000, Avg. loss: 0.003322
Total training time: 0.35 seconds.
-- Epoch 16
Norm: 109.44, NNZs: 56825, Bias: -1.173237, T: 100000, Avg. loss: 0.064976
Total training time: 0.35 seconds.
Norm: 106.70, NNZs: 55667, Bias: -1.266913, T: 100000, Avg. loss: 0.063132
Total training time: 0.22 seconds.
-- Epoch 6
Norm: 154.67, NNZs: 56140, Bias: -0.818920, T: 420000, Avg. loss: 0.000675
Total training time: 0.63 seconds.
-- Epoch 6
Norm: 109.43, NNZs: 58109, Bias: -1.142297, T: 80000, Avg. loss: 0.113949
Total training time: 0.18 seconds.
-- Epoch 5
Norm: 141.20, NNZs: 58269, Bias: -1.166693, T: 220000, Avg. loss: 0.010432
Total training time: 0.34 seconds.
-- Epoch 12
Convergence after 21 epochs took 0.63 seconds
Norm: 147.20, NNZs: 55592, Bias: -0.777354, T: 200000, Avg. loss: 0.018617
Total training time: 0.33 seconds.
Norm: 118.95, NNZs: 58359, Bias: -1.122169, T: 120000, Avg. loss: 0.044800
Total training time: 0.35 seconds.
-- Epoch 11
-- Epoch 7
Norm: 119.57, NNZs: 57429, Bias: -1.355583, T: 140000, Avg. loss: 0.030205
Total training time: 0.24 seconds.
-- Epoch 8
Norm: 122.42, NNZs: 56168, Bias: -1.292130, T: 180000, Avg. loss: 0.014692
Total training time: 0.35 seconds.
-- Epoch 10
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.6s finished
Norm: 128.35, NNZs: 57568, Bias: -1.309877, T: 220000, Avg. loss: 0.007444
Total training time: 0.34 seconds.
Norm: 138.91, NNZs: 53986, Bias: -0.676874, T: 180000, Avg. loss: 0.020097
Total training time: 0.36 seconds.
-- Epoch 10
Norm: 128.19, NNZs: 53572, Bias: -0.691544, T: 120000, Avg. loss: 0.051061
Total training time: 0.25 seconds.
-- Epoch 7
Norm: 142.18, NNZs: 58271, Bias: -1.182645, T: 240000, Avg. loss: 0.007423
Total training time: 0.36 seconds.
Norm: 126.68, NNZs: 57232, Bias: -1.376032, T: 240000, Avg. loss: 0.004041
Total training time: 0.36 seconds.
-- Epoch 13
-- Epoch 13
Norm: 123.23, NNZs: 58390, Bias: -1.123144, T: 140000, Avg. loss: 0.030682
Total training time: 0.37 seconds.
-- Epoch 8
Norm: 44.16, NNZs: 52225, Bias: -1.087674, T: 20000, Avg. loss: 0.285026
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 117.46, NNZs: 55644, Bias: -0.622579, T: 80000, Avg. loss: 0.116191
Total training time: 0.22 seconds.
-- Epoch 5
Norm: 108.67, NNZs: 57397, Bias: -1.269300, T: 100000, Avg. loss: 0.060017
Total training time: 0.38 seconds.
-- Epoch 6
Norm: 112.84, NNZs: 57916, Bias: -1.061537, T: 100000, Avg. loss: 0.064908
Total training time: 0.21 seconds.
-- Epoch 6
-- Epoch 12
Norm: 98.52, NNZs: 54893, Bias: -1.070706, T: 80000, Avg. loss: 0.094305
Total training time: 0.26 seconds.
-- Epoch 5
Norm: 148.91, NNZs: 55596, Bias: -0.774309, T: 220000, Avg. loss: 0.013709
Total training time: 0.36 seconds.
-- Epoch 12
Norm: 40.68, NNZs: 50629, Bias: -1.067134, T: 20000, Avg. loss: 0.263611
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 127.09, NNZs: 55338, Bias: -1.211868, T: 240000, Avg. loss: 0.005623
Total training time: 0.38 seconds.
Norm: 122.53, NNZs: 57432, Bias: -1.373395, T: 160000, Avg. loss: 0.020163
Total training time: 0.27 seconds.
-- Epoch 13
-- Epoch 9
Norm: 142.90, NNZs: 58277, Bias: -1.177577, T: 260000, Avg. loss: 0.005367
Total training time: 0.38 seconds.
-- Epoch 14
Norm: 109.79, NNZs: 56977, Bias: -1.184221, T: 100000, Avg. loss: 0.064603
Total training time: 0.26 seconds.
-- Epoch 6
Norm: 48.33, NNZs: 53550, Bias: -1.017887, T: 20000, Avg. loss: 0.338916
Total training time: 0.09 seconds.
-- Epoch 2
Norm: 123.91, NNZs: 56169, Bias: -1.317869, T: 200000, Avg. loss: 0.009975
Total training time: 0.39 seconds.
-- Epoch 11
Norm: 120.24, NNZs: 55927, Bias: -1.274662, T: 160000, Avg. loss: 0.020466
Total training time: 0.38 seconds.
Norm: 119.29, NNZs: 58270, Bias: -1.152804, T: 100000, Avg. loss: 0.080522
Total training time: 0.22 seconds.
-- Epoch 6
Norm: 40.92, NNZs: 49621, Bias: -0.964417, T: 20000, Avg. loss: 0.266167
Total training time: 0.09 seconds.
-- Epoch 2
Norm: 124.95, NNZs: 55445, Bias: -1.226369, T: 200000, Avg. loss: 0.010929
Total training time: 0.37 seconds.
-- Epoch 11
Norm: 115.66, NNZs: 56882, Bias: -1.236846, T: 120000, Avg. loss: 0.044795
Total training time: 0.40 seconds.
-- Epoch 7
-- Epoch 9
Norm: 107.46, NNZs: 55030, Bias: -1.069720, T: 100000, Avg. loss: 0.064334
Total training time: 0.28 seconds.
Norm: 61.70, NNZs: 50386, Bias: -0.600993, T: 20000, Avg. loss: 0.379373
Total training time: 0.11 seconds.
-- Epoch 6
-- Epoch 2
Norm: 142.55, NNZs: 56033, Bias: -0.741896, T: 160000, Avg. loss: 0.031274
Total training time: 0.38 seconds.
Norm: 126.20, NNZs: 58685, Bias: -1.159038, T: 120000, Avg. loss: 0.056007
Total training time: 0.40 seconds.
-- Epoch 9
-- Epoch 7
Norm: 143.44, NNZs: 58278, Bias: -1.178680, T: 280000, Avg. loss: 0.004014
Total training time: 0.40 seconds.
-- Epoch 15
Norm: 126.22, NNZs: 58401, Bias: -1.131000, T: 160000, Avg. loss: 0.021168
Total training time: 0.41 seconds.
Norm: 132.65, NNZs: 53644, Bias: -0.716702, T: 140000, Avg. loss: 0.035927
Total training time: 0.29 seconds.
-- Epoch 9
-- Epoch 8
Norm: 41.74, NNZs: 51687, Bias: -0.979986, T: 20000, Avg. loss: 0.282388
Total training time: 0.09 seconds.
-- Epoch 2
Norm: 127.72, NNZs: 55341, Bias: -1.210096, T: 260000, Avg. loss: 0.004195
Total training time: 0.40 seconds.
-- Epoch 14
Norm: 74.05, NNZs: 56939, Bias: -0.907747, T: 40000, Avg. loss: 0.212871
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 127.12, NNZs: 57232, Bias: -1.374518, T: 260000, Avg. loss: 0.002844
Total training time: 0.41 seconds.
-- Epoch 14
Norm: 125.96, NNZs: 58505, Bias: -1.117404, T: 160000, Avg. loss: 0.019650
Total training time: 0.38 seconds.
-- Epoch 9
Norm: 62.66, NNZs: 51828, Bias: -0.520623, T: 20000, Avg. loss: 0.407700
Total training time: 0.09 seconds.
-- Epoch 2
Norm: 145.13, NNZs: 54040, Bias: -0.748405, T: 320000, Avg. loss: 0.002348
Total training time: 0.42 seconds.
-- Epoch 17
Norm: 140.82, NNZs: 53999, Bias: -0.678799, T: 200000, Avg. loss: 0.014924
Total training time: 0.42 seconds.
-- Epoch 11
Norm: 113.00, NNZs: 55707, Bias: -1.261210, T: 120000, Avg. loss: 0.043269
Total training time: 0.29 seconds.
-- Epoch 7
Norm: 124.60, NNZs: 57441, Bias: -1.377903, T: 180000, Avg. loss: 0.013882
Total training time: 0.31 seconds.
-- Epoch 10
Norm: 143.85, NNZs: 58278, Bias: -1.179751, T: 300000, Avg. loss: 0.003113
Total training time: 0.42 seconds.
-- Epoch 16
Norm: 126.40, NNZs: 58363, Bias: -1.196033, T: 120000, Avg. loss: 0.057417
Total training time: 0.26 seconds.
-- Epoch 7
Norm: 129.14, NNZs: 57571, Bias: -1.319597, T: 240000, Avg. loss: 0.005511
Total training time: 0.41 seconds.
-- Epoch 13
Norm: 118.85, NNZs: 57979, Bias: -1.083481, T: 120000, Avg. loss: 0.043971
Total training time: 0.26 seconds.
-- Epoch 7
Norm: 124.98, NNZs: 56173, Bias: -1.317989, T: 220000, Avg. loss: 0.007063
Total training time: 0.43 seconds.
-- Epoch 12
Norm: 114.60, NNZs: 57438, Bias: -1.282222, T: 120000, Avg. loss: 0.041795
Norm: 120.09, NNZs: 56936, Bias: -1.242814, T: 140000, Avg. loss: 0.031254
Total training time: 0.44 seconds.
Total training time: 0.43 seconds.
-- Epoch 7
-- Epoch 8
Norm: 135.85, NNZs: 53659, Bias: -0.722933, T: 160000, Avg. loss: 0.025380
Total training time: 0.33 seconds.
Norm: 144.15, NNZs: 58278, Bias: -1.181008, T: 320000, Avg. loss: 0.002217
Total training time: 0.43 seconds.
-- Epoch 9
-- Epoch 17
Norm: 131.22, NNZs: 58728, Bias: -1.189365, T: 140000, Avg. loss: 0.039643
Total training time: 0.44 seconds.
-- Epoch 8
Norm: 128.20, NNZs: 55343, Bias: -1.216727, T: 280000, Avg. loss: 0.003260
Total training time: 0.44 seconds.
-- Epoch 15
Norm: 150.17, NNZs: 55616, Bias: -0.788346, T: 240000, Avg. loss: 0.010194
Total training time: 0.43 seconds.
-- Epoch 13
Norm: 145.49, NNZs: 56046, Bias: -0.737608, T: 180000, Avg. loss: 0.023859
Total training time: 0.43 seconds.
-- Epoch 10
Norm: 126.87, NNZs: 55821, Bias: -0.632223, T: 100000, Avg. loss: 0.084050
Total training time: 0.30 seconds.
Norm: 117.33, NNZs: 55768, Bias: -1.271222, T: 140000, Avg. loss: 0.029494
Total training time: 0.32 seconds.
-- Epoch 8
Norm: 126.05, NNZs: 55445, Bias: -1.236467, T: 220000, Avg. loss: 0.007438
Total training time: 0.43 seconds.
-- Epoch 12
Norm: 128.33, NNZs: 58408, Bias: -1.143484, T: 180000, Avg. loss: 0.014856
Total training time: 0.46 seconds.
-- Epoch 10
Norm: 70.64, NNZs: 55722, Bias: -1.152647, T: 40000, Avg. loss: 0.199940
Total training time: 0.15 seconds.
-- Epoch 3
-- Epoch 6
Norm: 127.41, NNZs: 57232, Bias: -1.376931, T: 280000, Avg. loss: 0.001877
Total training time: 0.46 seconds.
Norm: 115.91, NNZs: 57041, Bias: -1.257854, T: 120000, Avg. loss: 0.043945
Total training time: 0.33 seconds.
-- Epoch 15
-- Epoch 7
Norm: 92.02, NNZs: 57797, Bias: -1.016390, T: 60000, Avg. loss: 0.140734
Total training time: 0.16 seconds.
-- Epoch 4
Norm: 113.87, NNZs: 55108, Bias: -1.140595, T: 120000, Avg. loss: 0.045623
Total training time: 0.34 seconds.
-- Epoch 7
Norm: 122.43, NNZs: 55944, Bias: -1.263588, T: 180000, Avg. loss: 0.014468
Norm: 125.98, NNZs: 57443, Bias: -1.400362, T: 200000, Avg. loss: 0.009331
Total training time: 0.36 seconds.
Total training time: 0.46 seconds.
-- Epoch 10
-- Epoch 11
Norm: 142.24, NNZs: 53999, Bias: -0.685804, T: 220000, Avg. loss: 0.011055
Total training time: 0.47 seconds.
-- Epoch 12
Norm: 129.71, NNZs: 57571, Bias: -1.320962, T: 260000, Avg. loss: 0.003945
Total training time: 0.46 seconds.
-- Epoch 14
Norm: 138.29, NNZs: 53669, Bias: -0.744400, T: 180000, Avg. loss: 0.019276
Total training time: 0.36 seconds.
-- Epoch 10
Norm: 86.87, NNZs: 54272, Bias: -0.556922, T: 40000, Avg. loss: 0.238280
Total training time: 0.16 seconds.
Norm: 144.38, NNZs: 58278, Bias: -1.183583, T: 340000, Avg. loss: 0.001771
Total training time: 0.47 seconds.
-- Epoch 3
-- Epoch 18
Norm: 145.36, NNZs: 54040, Bias: -0.749689, T: 340000, Avg. loss: 0.001772
Total training time: 0.48 seconds.
-- Epoch 18
Norm: 68.15, NNZs: 53658, Bias: -0.925686, T: 40000, Avg. loss: 0.195625
Total training time: 0.17 seconds.
-- Epoch 3
Norm: 134.90, NNZs: 58746, Bias: -1.195699, T: 160000, Avg. loss: 0.028571
Total training time: 0.47 seconds.
-- Epoch 9
Norm: 67.12, NNZs: 54123, Bias: -1.067187, T: 40000, Avg. loss: 0.192988
Total training time: 0.17 seconds.
-- Epoch 3
Norm: 125.72, NNZs: 56176, Bias: -1.324248, T: 240000, Avg. loss: 0.004846
Total training time: 0.48 seconds.
-- Epoch 13
Norm: 131.57, NNZs: 58386, Bias: -1.213698, T: 140000, Avg. loss: 0.040933
Total training time: 0.31 seconds.
-- Epoch 8
Norm: 123.17, NNZs: 57991, Bias: -1.105585, T: 140000, Avg. loss: 0.031061
Total training time: 0.31 seconds.
Norm: 123.17, NNZs: 56953, Bias: -1.285971, T: 160000, Avg. loss: 0.021783
Total training time: 0.48 seconds.
-- Epoch 8
-- Epoch 9
Norm: 76.82, NNZs: 56894, Bias: -1.024468, T: 40000, Avg. loss: 0.238297
Total training time: 0.18 seconds.
-- Epoch 3
Norm: 69.83, NNZs: 55639, Bias: -0.897933, T: 40000, Avg. loss: 0.207422
Total training time: 0.17 seconds.
-- Epoch 3
Norm: 120.34, NNZs: 55787, Bias: -1.300774, T: 160000, Avg. loss: 0.020357
Total training time: 0.36 seconds.
-- Epoch 9
Norm: 147.60, NNZs: 56063, Bias: -0.743906, T: 200000, Avg. loss: 0.017124
Total training time: 0.47 seconds.
-- Epoch 11
Norm: 127.93, NNZs: 58505, Bias: -1.122538, T: 180000, Avg. loss: 0.013667
Total training time: 0.46 seconds.
-- Epoch 10
Norm: 118.69, NNZs: 57460, Bias: -1.328056, T: 140000, Avg. loss: 0.028440
Total training time: 0.50 seconds.
-- Epoch 8
Norm: 85.17, NNZs: 52493, Bias: -0.676605, T: 40000, Avg. loss: 0.216604
Total training time: 0.20 seconds.
-- Epoch 3
Norm: 118.37, NNZs: 55145, Bias: -1.135526, T: 140000, Avg. loss: 0.031276
Total training time: 0.38 seconds.
Norm: 126.25, NNZs: 56176, Bias: -1.328349, T: 260000, Avg. loss: 0.003482
-- Epoch 8
Total training time: 0.50 seconds.
-- Epoch 14
Norm: 151.18, NNZs: 55621, Bias: -0.796152, T: 260000, Avg. loss: 0.008045
Total training time: 0.48 seconds.
-- Epoch 14
Norm: 129.76, NNZs: 58410, Bias: -1.155197, T: 200000, Avg. loss: 0.010057
Norm: 130.14, NNZs: 57571, Bias: -1.321252, T: 280000, Avg. loss: 0.002964
Total training time: 0.49 seconds.
-- Epoch 15
Norm: 144.57, NNZs: 58278, Bias: -1.182372, T: 360000, Avg. loss: 0.001450
Total training time: 0.50 seconds.
-- Epoch 19
Norm: 127.62, NNZs: 57232, Bias: -1.379678, T: 300000, Avg. loss: 0.001338
Total training time: 0.51 seconds.
Norm: 104.32, NNZs: 58084, Bias: -1.000416, T: 80000, Avg. loss: 0.093895
Total training time: 0.21 seconds.
-- Epoch 5
-- Epoch 16
Norm: 126.97, NNZs: 57446, Bias: -1.393369, T: 220000, Avg. loss: 0.006436
Total training time: 0.41 seconds.
Norm: 120.27, NNZs: 57050, Bias: -1.240797, T: 140000, Avg. loss: 0.030416
Total training time: 0.39 seconds.
Norm: 145.55, NNZs: 54040, Bias: -0.750790, T: 360000, Avg. loss: 0.001413
Total training time: 0.53 seconds.
-- Epoch 19
Norm: 88.01, NNZs: 56552, Bias: -1.224796, T: 60000, Avg. loss: 0.133576
Total training time: 0.22 seconds.
-- Epoch 4
Norm: 104.37, NNZs: 55109, Bias: -0.614789, T: 60000, Avg. loss: 0.164091
Total training time: 0.21 seconds.
Norm: 123.93, NNZs: 55945, Bias: -1.284956, T: 200000, Avg. loss: 0.010037
Total training time: 0.52 seconds.
-- Epoch 4
-- Epoch 11
Norm: 129.28, NNZs: 58510, Bias: -1.133538, T: 200000, Avg. loss: 0.009394
Total training time: 0.50 seconds.
-- Epoch 11
Norm: 125.38, NNZs: 56959, Bias: -1.294838, T: 180000, Avg. loss: 0.015338
Total training time: 0.53 seconds.
Norm: 140.16, NNZs: 53675, Bias: -0.738679, T: 200000, Avg. loss: 0.014505
Total training time: 0.41 seconds.
-- Epoch 10
-- Epoch 11
Norm: 121.50, NNZs: 55167, Bias: -1.161164, T: 160000, Avg. loss: 0.021663
Total training time: 0.41 seconds.
-- Epoch 9
Norm: 112.98, NNZs: 58209, Bias: -1.080789, T: 100000, Avg. loss: 0.064908
Total training time: 0.23 seconds.
-- Epoch 6
Norm: 88.18, NNZs: 56653, Bias: -1.057136, T: 60000, Avg. loss: 0.141410
Total training time: 0.22 seconds.
Norm: 128.55, NNZs: 55343, Bias: -1.220737, T: 300000, Avg. loss: 0.002393
Total training time: 0.54 seconds.
Total training time: 0.53 seconds.
-- Epoch 4
-- Epoch 16
Norm: 149.18, NNZs: 56069, Bias: -0.745674, T: 220000, Avg. loss: 0.012623
-- Epoch 11
Norm: 122.46, NNZs: 55799, Bias: -1.295638, T: 180000, Avg. loss: 0.013963
Total training time: 0.40 seconds.
-- Epoch 10
Norm: 143.32, NNZs: 54004, Bias: -0.686413, T: 240000, Avg. loss: 0.008337
Total training time: 0.54 seconds.
Norm: 126.86, NNZs: 55448, Bias: -1.243585, T: 240000, Avg. loss: 0.005442
Total training time: 0.51 seconds.
Norm: 137.54, NNZs: 58751, Bias: -1.193643, T: 180000, Avg. loss: 0.020268
Total training time: 0.53 seconds.
-- Epoch 13
-- Epoch 10
Norm: 135.31, NNZs: 58407, Bias: -1.211601, T: 160000, Avg. loss: 0.029062
Total training time: 0.37 seconds.
-- Epoch 9
Norm: 126.11, NNZs: 58015, Bias: -1.123239, T: 160000, Avg. loss: 0.020907
Norm: 133.77, NNZs: 55894, Bias: -0.647110, T: 120000, Avg. loss: 0.060584
Total training time: 0.39 seconds.
-- Epoch 7
-- Epoch 12
Norm: 121.59, NNZs: 57470, Bias: -1.356421, T: 160000, Avg. loss: 0.019848
Total training time: 0.55 seconds.
-- Epoch 9
Norm: 86.11, NNZs: 54655, Bias: -0.998770, T: 60000, Avg. loss: 0.132929
Total training time: 0.24 seconds.
-- Epoch 4
Total training time: 0.37 seconds.
-- Epoch 9
Norm: 84.92, NNZs: 55142, Bias: -1.210113, T: 60000, Avg. loss: 0.132234
Total training time: 0.24 seconds.
-- Epoch 4
Norm: 144.73, NNZs: 58283, Bias: -1.182712, T: 380000, Avg. loss: 0.001219
Total training time: 0.54 seconds.
Convergence after 19 epochs took 0.54 seconds
Total training time: 0.51 seconds.
-- Epoch 8
-- Epoch 12
Norm: 95.96, NNZs: 57648, Bias: -1.077327, T: 60000, Avg. loss: 0.163197
Total training time: 0.25 seconds.
-- Epoch 4
Norm: 101.67, NNZs: 53243, Bias: -0.681725, T: 60000, Avg. loss: 0.146275
Total training time: 0.26 seconds.
-- Epoch 4
Norm: 130.45, NNZs: 57571, Bias: -1.324798, T: 300000, Avg. loss: 0.002227
Total training time: 0.55 seconds.
-- Epoch 16
Norm: 100.45, NNZs: 56917, Bias: -1.226114, T: 80000, Avg. loss: 0.092555
Total training time: 0.26 seconds.
Norm: 139.07, NNZs: 55958, Bias: -0.651736, T: 140000, Avg. loss: 0.045419
Total training time: 0.42 seconds.
-- Epoch 5
-- Epoch 8
Norm: 141.51, NNZs: 53685, Bias: -0.743812, T: 220000, Avg. loss: 0.010447
Total training time: 0.45 seconds.
-- Epoch 12
Norm: 126.64, NNZs: 56182, Bias: -1.330616, T: 280000, Avg. loss: 0.002528
Total training time: 0.57 seconds.
-- Epoch 15
Norm: 130.83, NNZs: 58411, Bias: -1.157071, T: 220000, Avg. loss: 0.007431
Total training time: 0.57 seconds.
-- Epoch 12
Norm: 117.03, NNZs: 55399, Bias: -0.622127, T: 80000, Avg. loss: 0.115843
Total training time: 0.25 seconds.
-- Epoch 5
Norm: 118.88, NNZs: 58258, Bias: -1.129752, T: 120000, Avg. loss: 0.043154
Total training time: 0.27 seconds.
-- Epoch 7
Norm: 151.92, NNZs: 55624, Bias: -0.790302, T: 280000, Avg. loss: 0.005817
Total training time: 0.56 seconds.
-- Epoch 15
Norm: 128.86, NNZs: 55346, Bias: -1.220313, T: 320000, Avg. loss: 0.002063
Total training time: 0.57 seconds.
-- Epoch 17
Norm: 97.61, NNZs: 55506, Bias: -1.145639, T: 80000, Avg. loss: 0.091327
Total training time: 0.27 seconds.
-- Epoch 5
-- Epoch 13
Norm: 126.98, NNZs: 56961, Bias: -1.304028, T: 200000, Avg. loss: 0.010997
Total training time: 0.58 seconds.
-- Epoch 11
Norm: 145.70, NNZs: 54040, Bias: -0.750593, T: 380000, Avg. loss: 0.001057
Total training time: 0.58 seconds.
Norm: 127.76, NNZs: 57232, Bias: -1.379475, T: 320000, Avg. loss: 0.000893
Total training time: 0.57 seconds.
-- Epoch 17
Norm: 139.52, NNZs: 58756, Bias: -1.214842, T: 200000, Avg. loss: 0.015117
-- Epoch 20
Total training time: 0.58 seconds.
-- Epoch 11
Norm: 123.75, NNZs: 55186, Bias: -1.178930, T: 180000, Avg. loss: 0.015416
Total training time: 0.46 seconds.
-- Epoch 10
Norm: 127.63, NNZs: 57446, Bias: -1.400768, T: 240000, Avg. loss: 0.004373
Total training time: 0.47 seconds.
-- Epoch 13
Norm: 123.57, NNZs: 57473, Bias: -1.369587, T: 180000, Avg. loss: 0.013280
Total training time: 0.59 seconds.
-- Epoch 10
Norm: 123.94, NNZs: 55800, Bias: -1.306688, T: 200000, Avg. loss: 0.009809
Total training time: 0.46 seconds.
-- Epoch 11
Norm: 131.55, NNZs: 58412, Bias: -1.157705, T: 240000, Avg. loss: 0.005019
Total training time: 0.59 seconds.
-- Epoch 13
Norm: 123.23, NNZs: 57057, Bias: -1.281952, T: 160000, Avg. loss: 0.020925
Total training time: 0.46 seconds.
-- Epoch 9
Norm: 138.05, NNZs: 58418, Bias: -1.226884, T: 180000, Avg. loss: 0.021179
Total training time: 0.42 seconds.
Norm: 144.16, NNZs: 54010, Bias: -0.691398, T: 260000, Avg. loss: 0.006507
-- Epoch 10
Total training time: 0.59 seconds.
-- Epoch 14
Norm: 127.47, NNZs: 55450, Bias: -1.246403, T: 260000, Avg. loss: 0.004042
Total training time: 0.57 seconds.
Norm: 125.02, NNZs: 55951, Bias: -1.285348, T: 220000, Avg. loss: 0.007213
Total training time: 0.59 seconds.
-- Epoch 12
Norm: 98.42, NNZs: 54955, Bias: -1.033664, T: 80000, Avg. loss: 0.089622
Total training time: 0.30 seconds.
-- Epoch 5
Norm: 128.19, NNZs: 58017, Bias: -1.130209, T: 180000, Avg. loss: 0.014596
Total training time: 0.43 seconds.
-- Epoch 10
Norm: 130.67, NNZs: 57571, Bias: -1.327328, T: 320000, Avg. loss: 0.001649
Total training time: 0.59 seconds.
-- Epoch 17
Norm: 126.92, NNZs: 56183, Bias: -1.335631, T: 300000, Avg. loss: 0.001893
Total training time: 0.61 seconds.
-- Epoch 16
-- Epoch 14
Norm: 130.23, NNZs: 58512, Bias: -1.137513, T: 220000, Avg. loss: 0.006599
Total training time: 0.58 seconds.
Norm: 150.34, NNZs: 56072, Bias: -0.761596, T: 240000, Avg. loss: 0.009364
Total training time: 0.59 seconds.
-- Epoch 13
Norm: 100.70, NNZs: 57027, Bias: -1.118896, T: 80000, Avg. loss: 0.095330
Total training time: 0.29 seconds.
-- Epoch 5
Norm: 128.11, NNZs: 56964, Bias: -1.305398, T: 220000, Avg. loss: 0.007759
Total training time: 0.61 seconds.
-- Epoch 12
-- Epoch 12
Norm: 124.94, NNZs: 55820, Bias: -1.317479, T: 220000, Avg. loss: 0.006577
Total training time: 0.48 seconds.
-- Epoch 12
Norm: 113.31, NNZs: 53527, Bias: -0.700447, T: 80000, Avg. loss: 0.100292
Total training time: 0.32 seconds.
-- Epoch 5
Norm: 127.85, NNZs: 57232, Bias: -1.380798, T: 340000, Avg. loss: 0.000625
Total training time: 0.62 seconds.
-- Epoch 18
Norm: 125.38, NNZs: 55187, Bias: -1.171877, T: 200000, Avg. loss: 0.011034
Total training time: 0.50 seconds.
-- Epoch 11
Norm: 109.48, NNZs: 57995, Bias: -1.152615, T: 80000, Avg. loss: 0.112466
Total training time: 0.32 seconds.
Norm: 132.06, NNZs: 58416, Bias: -1.156923, T: 260000, Avg. loss: 0.003553
-- Epoch 5
Total training time: 0.63 seconds.
-- Epoch 14
Norm: 123.12, NNZs: 58285, Bias: -1.126171, T: 140000, Avg. loss: 0.030189
Total training time: 0.32 seconds.
Norm: 126.46, NNZs: 55591, Bias: -0.644634, T: 100000, Avg. loss: 0.084210
Total training time: 0.31 seconds.
-- Epoch 8
-- Epoch 6
Norm: 145.81, NNZs: 54040, Bias: -0.752385, T: 400000, Avg. loss: 0.000871
Total training time: 0.63 seconds.
Convergence after 20 epochs took 0.63 seconds
Norm: 124.96, NNZs: 57473, Bias: -1.381868, T: 200000, Avg. loss: 0.009244
Total training time: 0.63 seconds.
-- Epoch 11
Norm: 109.24, NNZs: 57036, Bias: -1.304522, T: 100000, Avg. loss: 0.063937
Total training time: 0.33 seconds.
-- Epoch 6
Norm: 144.85, NNZs: 54013, Bias: -0.692497, T: 280000, Avg. loss: 0.005216
Total training time: 0.64 seconds.
-- Epoch 15
Norm: 152.45, NNZs: 55625, Bias: -0.793640, T: 300000, Avg. loss: 0.004284
Total training time: 0.62 seconds.
-- Epoch 16
Norm: 142.53, NNZs: 53688, Bias: -0.749188, T: 240000, Avg. loss: 0.007847
Total training time: 0.53 seconds.
-- Epoch 13
Norm: 129.10, NNZs: 55346, Bias: -1.221010, T: 340000, Avg. loss: 0.001585
Total training time: 0.64 seconds.
-- Epoch 18
Norm: 128.08, NNZs: 57447, Bias: -1.402717, T: 260000, Avg. loss: 0.003015
Total training time: 0.53 seconds.
Norm: 127.91, NNZs: 55454, Bias: -1.249583, T: 280000, Avg. loss: 0.002985
Total training time: 0.62 seconds.
-- Epoch 14
-- Epoch 15
Norm: 140.94, NNZs: 58767, Bias: -1.216089, T: 220000, Avg. loss: 0.010757
Total training time: 0.64 seconds.
-- Epoch 12
Norm: 127.92, NNZs: 57232, Bias: -1.380649, T: 360000, Avg. loss: 0.000416
Total training time: 0.64 seconds.
Norm: 125.37, NNZs: 57060, Bias: -1.288951, T: 180000, Avg. loss: 0.014808
Total training time: 0.52 seconds.
Convergence after 18 epochs took 0.64 seconds
-- Epoch 10
Norm: 129.63, NNZs: 58027, Bias: -1.143930, T: 200000, Avg. loss: 0.010137
Total training time: 0.48 seconds.
Norm: 142.87, NNZs: 56027, Bias: -0.649530, T: 160000, Avg. loss: 0.032373
Total training time: 0.50 seconds.
Norm: 125.96, NNZs: 57475, Bias: -1.373006, T: 220000, Avg. loss: 0.006533
Total training time: 0.65 seconds.
-- Epoch 11
-- Epoch 9
-- Epoch 12
Norm: 128.94, NNZs: 56967, Bias: -1.313768, T: 240000, Avg. loss: 0.005768
Total training time: 0.65 seconds.
-- Epoch 13
Norm: 140.02, NNZs: 58425, Bias: -1.238281, T: 200000, Avg. loss: 0.015082
Total training time: 0.48 seconds.
-- Epoch 11
Norm: 130.87, NNZs: 58512, Bias: -1.135686, T: 240000, Avg. loss: 0.004443
Total training time: 0.62 seconds.
-- Epoch 13
Norm: 125.78, NNZs: 55951, Bias: -1.294261, T: 240000, Avg. loss: 0.005001
Total training time: 0.64 seconds.
-- Epoch 13
Norm: 126.48, NNZs: 55193, Bias: -1.182136, T: 220000, Avg. loss: 0.007471
Total training time: 0.53 seconds.
-- Epoch 12
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.7s remaining:    1.2s
Norm: 151.23, NNZs: 56075, Bias: -0.769184, T: 260000, Avg. loss: 0.007073
Total training time: 0.64 seconds.
-- Epoch 14
Norm: 132.42, NNZs: 58416, Bias: -1.163045, T: 280000, Avg. loss: 0.002626
Total training time: 0.67 seconds.
Norm: 125.67, NNZs: 55836, Bias: -1.323989, T: 240000, Avg. loss: 0.004747
Total training time: 0.53 seconds.
-- Epoch 15
-- Epoch 13
Norm: 127.14, NNZs: 56183, Bias: -1.334280, T: 320000, Avg. loss: 0.001387
Total training time: 0.67 seconds.
-- Epoch 17
Norm: 126.02, NNZs: 58300, Bias: -1.156562, T: 160000, Avg. loss: 0.020683
Norm: 129.54, NNZs: 56967, Bias: -1.317645, T: 260000, Avg. loss: 0.004109
Total training time: 0.67 seconds.
-- Epoch 14
Norm: 130.86, NNZs: 57571, Bias: -1.325561, T: 340000, Avg. loss: 0.001356
Total training time: 0.66 seconds.
-- Epoch 18
Norm: 131.34, NNZs: 58513, Bias: -1.137926, T: 260000, Avg. loss: 0.003285
Total training time: 0.64 seconds.
-- Epoch 14
Total training time: 0.37 seconds.
-- Epoch 9
Norm: 107.14, NNZs: 55016, Bias: -1.058766, T: 100000, Avg. loss: 0.061478
Total training time: 0.37 seconds.
-- Epoch 6
Norm: 119.24, NNZs: 58144, Bias: -1.149583, T: 100000, Avg. loss: 0.079324
Total training time: 0.37 seconds.
-- Epoch 6
Norm: 152.85, NNZs: 55625, Bias: -0.794183, T: 320000, Avg. loss: 0.003181
Total training time: 0.66 seconds.
-- Epoch 17
Norm: 121.85, NNZs: 53670, Bias: -0.739504, T: 100000, Avg. loss: 0.070405
Total training time: 0.38 seconds.
-- Epoch 6
Norm: 126.64, NNZs: 57477, Bias: -1.380547, T: 240000, Avg. loss: 0.004464
Total training time: 0.69 seconds.
-- Epoch 13
Norm: 106.27, NNZs: 55645, Bias: -1.254884, T: 100000, Avg. loss: 0.062218
Total training time: 0.37 seconds.
-- Epoch 6
Norm: 141.95, NNZs: 58767, Bias: -1.228648, T: 240000, Avg. loss: 0.007601
Total training time: 0.68 seconds.
-- Epoch 13
Norm: 127.31, NNZs: 55193, Bias: -1.192266, T: 240000, Avg. loss: 0.005649
Total training time: 0.56 seconds.
-- Epoch 13
Norm: 109.87, NNZs: 57236, Bias: -1.134347, T: 100000, Avg. loss: 0.067421
Total training time: 0.37 seconds.
-- Epoch 6
Norm: 143.33, NNZs: 53688, Bias: -0.755074, T: 260000, Avg. loss: 0.006160
Total training time: 0.57 seconds.
-- Epoch 14
Norm: 115.35, NNZs: 57120, Bias: -1.329479, T: 120000, Avg. loss: 0.043377
Total training time: 0.39 seconds.
-- Epoch 7
Norm: 130.67, NNZs: 58027, Bias: -1.148995, T: 220000, Avg. loss: 0.007225
Total training time: 0.52 seconds.
Norm: 145.83, NNZs: 56040, Bias: -0.684187, T: 180000, Avg. loss: 0.025017
Total training time: 0.54 seconds.
-- Epoch 12
-- Epoch 10
Norm: 129.26, NNZs: 55346, Bias: -1.222558, T: 360000, Avg. loss: 0.001110
Total training time: 0.68 seconds.
Convergence after 18 epochs took 0.68 seconds
Norm: 131.66, NNZs: 58515, Bias: -1.140132, T: 280000, Avg. loss: 0.002316
Total training time: 0.66 seconds.
-- Epoch 15
Norm: 128.07, NNZs: 58307, Bias: -1.139502, T: 180000, Avg. loss: 0.014311
Total training time: 0.39 seconds.
-- Epoch 10
Norm: 133.39, NNZs: 55659, Bias: -0.681643, T: 120000, Avg. loss: 0.060508
Total training time: 0.38 seconds.
-- Epoch 7
Norm: 126.34, NNZs: 55951, Bias: -1.294448, T: 260000, Avg. loss: 0.003633
Total training time: 0.69 seconds.
-- Epoch 14
Norm: 141.44, NNZs: 58430, Bias: -1.248361, T: 220000, Avg. loss: 0.010875
Total training time: 0.53 seconds.
-- Epoch 12
Norm: 126.18, NNZs: 55836, Bias: -1.324279, T: 260000, Avg. loss: 0.003309
Total training time: 0.57 seconds.
-- Epoch 14
Norm: 127.95, NNZs: 55194, Bias: -1.192536, T: 260000, Avg. loss: 0.004333
Total training time: 0.58 seconds.
-- Epoch 14
Norm: 145.35, NNZs: 54014, Bias: -0.694245, T: 300000, Avg. loss: 0.003813
Total training time: 0.70 seconds.
-- Epoch 16
Norm: 132.69, NNZs: 58416, Bias: -1.163338, T: 300000, Avg. loss: 0.001964
Total training time: 0.71 seconds.
-- Epoch 16
Norm: 128.41, NNZs: 57447, Bias: -1.406852, T: 280000, Avg. loss: 0.002127
Total training time: 0.59 seconds.
-- Epoch 15
Norm: 151.92, NNZs: 56075, Bias: -0.767785, T: 280000, Avg. loss: 0.005429
Total training time: 0.69 seconds.
Norm: 128.22, NNZs: 55454, Bias: -1.252679, T: 300000, Avg. loss: 0.002087
Total training time: 0.68 seconds.
Norm: 126.82, NNZs: 57063, Bias: -1.296445, T: 200000, Avg. loss: 0.010044
Norm: 127.12, NNZs: 57480, Bias: -1.384509, T: 260000, Avg. loss: 0.003154
Total training time: 0.71 seconds.
Total training time: 0.58 seconds.
-- Epoch 16
-- Epoch 14
-- Epoch 11
Norm: 131.00, NNZs: 57571, Bias: -1.327127, T: 360000, Avg. loss: 0.001082
Total training time: 0.70 seconds.
Convergence after 18 epochs took 0.70 seconds
-- Epoch 15
Norm: 113.17, NNZs: 55063, Bias: -1.136536, T: 120000, Avg. loss: 0.042220
Total training time: 0.41 seconds.
-- Epoch 7
Norm: 128.15, NNZs: 53743, Bias: -0.731167, T: 120000, Avg. loss: 0.050533
Total training time: 0.42 seconds.
-- Epoch 7
Norm: 143.97, NNZs: 53691, Bias: -0.755632, T: 280000, Avg. loss: 0.004852
Total training time: 0.61 seconds.
Norm: 153.16, NNZs: 55625, Bias: -0.796604, T: 340000, Avg. loss: 0.002416
Total training time: 0.70 seconds.
Norm: 128.45, NNZs: 55196, Bias: -1.192396, T: 280000, Avg. loss: 0.003353
Total training time: 0.60 seconds.
-- Epoch 18
-- Epoch 15
Norm: 148.14, NNZs: 56054, Bias: -0.684229, T: 200000, Avg. loss: 0.019096
Total training time: 0.58 seconds.
-- Epoch 11
Norm: 116.13, NNZs: 57300, Bias: -1.235318, T: 120000, Avg. loss: 0.045699
Total training time: 0.41 seconds.
-- Epoch 7
Norm: 112.44, NNZs: 55709, Bias: -1.248044, T: 120000, Avg. loss: 0.042885
Total training time: 0.41 seconds.
Norm: 126.25, NNZs: 58240, Bias: -1.166040, T: 120000, Avg. loss: 0.056091
Total training time: 0.42 seconds.
-- Epoch 7
-- Epoch 7
Norm: 127.44, NNZs: 57480, Bias: -1.384812, T: 280000, Avg. loss: 0.002074
Total training time: 0.73 seconds.
-- Epoch 15
Norm: 152.40, NNZs: 56077, Bias: -0.769363, T: 300000, Avg. loss: 0.003841
Total training time: 0.71 seconds.
-- Epoch 16
Norm: 129.95, NNZs: 56967, Bias: -1.318675, T: 280000, Avg. loss: 0.002887
Total training time: 0.73 seconds.
-- Epoch 15
Norm: 131.90, NNZs: 58515, Bias: -1.141453, T: 300000, Avg. loss: 0.001759
Total training time: 0.70 seconds.
-- Epoch 16
-- Epoch 15
Norm: 126.53, NNZs: 55836, Bias: -1.325417, T: 280000, Avg. loss: 0.002283
Total training time: 0.60 seconds.
-- Epoch 15
Norm: 128.80, NNZs: 55196, Bias: -1.199160, T: 300000, Avg. loss: 0.002412
Total training time: 0.62 seconds.
-- Epoch 16
Norm: 126.72, NNZs: 55951, Bias: -1.295634, T: 280000, Avg. loss: 0.002515
Total training time: 0.73 seconds.
-- Epoch 15
Norm: 132.89, NNZs: 58416, Bias: -1.170790, T: 320000, Avg. loss: 0.001504
Total training time: 0.75 seconds.
-- Epoch 17
Norm: 127.67, NNZs: 57480, Bias: -1.387653, T: 300000, Avg. loss: 0.001527
Total training time: 0.75 seconds.
-- Epoch 16
Norm: 138.60, NNZs: 55705, Bias: -0.646919, T: 140000, Avg. loss: 0.044364
Total training time: 0.43 seconds.
-- Epoch 8
Norm: 152.76, NNZs: 56078, Bias: -0.773343, T: 320000, Avg. loss: 0.002906
Total training time: 0.73 seconds.
Norm: 129.49, NNZs: 58317, Bias: -1.161564, T: 200000, Avg. loss: 0.009994
Total training time: 0.45 seconds.
-- Epoch 17
Norm: 119.64, NNZs: 57168, Bias: -1.332700, T: 140000, Avg. loss: 0.029887
Total training time: 0.45 seconds.
Norm: 127.29, NNZs: 56183, Bias: -1.336841, T: 340000, Avg. loss: 0.000970
Total training time: 0.75 seconds.
-- Epoch 8
-- Epoch 18
Norm: 142.72, NNZs: 58768, Bias: -1.225774, T: 260000, Avg. loss: 0.005725
Total training time: 0.75 seconds.
-- Epoch 14
Norm: 128.47, NNZs: 55454, Bias: -1.253527, T: 320000, Avg. loss: 0.001670
Total training time: 0.73 seconds.
-- Epoch 17
Norm: 128.62, NNZs: 57447, Bias: -1.407813, T: 300000, Avg. loss: 0.001376
Total training time: 0.64 seconds.
-- Epoch 16
Norm: 129.11, NNZs: 55207, Bias: -1.199621, T: 320000, Avg. loss: 0.002108
Total training time: 0.63 seconds.
-- Epoch 17
Norm: 149.91, NNZs: 56060, Bias: -0.670523, T: 220000, Avg. loss: 0.014449
Total training time: 0.61 seconds.
-- Epoch 12
Norm: 131.30, NNZs: 58313, Bias: -1.230762, T: 140000, Avg. loss: 0.039971
Total training time: 0.45 seconds.
-- Epoch 8
Norm: 127.95, NNZs: 57088, Bias: -1.292500, T: 220000, Avg. loss: 0.007699
Total training time: 0.63 seconds.
-- Epoch 12
-- Epoch 11
Norm: 145.74, NNZs: 54014, Bias: -0.692104, T: 320000, Avg. loss: 0.002879
Total training time: 0.76 seconds.
-- Epoch 17
Norm: 131.39, NNZs: 58027, Bias: -1.148490, T: 240000, Avg. loss: 0.005016
Total training time: 0.59 seconds.
-- Epoch 13
Norm: 144.48, NNZs: 53691, Bias: -0.755644, T: 300000, Avg. loss: 0.003793
Total training time: 0.65 seconds.
-- Epoch 16
Norm: 142.51, NNZs: 58430, Bias: -1.255747, T: 240000, Avg. loss: 0.008107
Total training time: 0.60 seconds.
-- Epoch 13
Norm: 117.44, NNZs: 55087, Bias: -1.119302, T: 140000, Avg. loss: 0.028911
Total training time: 0.47 seconds.
-- Epoch 8
Norm: 126.79, NNZs: 55836, Bias: -1.326179, T: 300000, Avg. loss: 0.001668
Total training time: 0.64 seconds.
Norm: 153.03, NNZs: 56078, Bias: -0.773445, T: 340000, Avg. loss: 0.002155
Total training time: 0.75 seconds.
-- Epoch 16
-- Epoch 18
Norm: 129.38, NNZs: 55207, Bias: -1.202479, T: 340000, Avg. loss: 0.001767
Total training time: 0.65 seconds.
-- Epoch 18
Norm: 120.77, NNZs: 57337, Bias: -1.221794, T: 140000, Avg. loss: 0.032736
Total training time: 0.46 seconds.
Norm: 127.83, NNZs: 57480, Bias: -1.388182, T: 320000, Avg. loss: 0.001001
Total training time: 0.78 seconds.
-- Epoch 17
Norm: 132.09, NNZs: 58515, Bias: -1.148342, T: 320000, Avg. loss: 0.001372
Norm: 127.40, NNZs: 56183, Bias: -1.339092, T: 360000, Avg. loss: 0.000746
Total training time: 0.79 seconds.
Convergence after 18 epochs took 0.79 seconds
Total training time: 0.75 seconds.
-- Epoch 17
Norm: 128.77, NNZs: 57447, Bias: -1.408388, T: 320000, Avg. loss: 0.000994
Total training time: 0.68 seconds.
Norm: 132.66, NNZs: 53796, Bias: -0.727193, T: 140000, Avg. loss: 0.035771
Total training time: 0.49 seconds.
-- Epoch 17
-- Epoch 8
Norm: 143.27, NNZs: 58777, Bias: -1.227973, T: 280000, Avg. loss: 0.004179
Total training time: 0.79 seconds.
-- Epoch 15
Norm: 129.56, NNZs: 55207, Bias: -1.201751, T: 360000, Avg. loss: 0.001216
Total training time: 0.67 seconds.
Norm: 116.78, NNZs: 55774, Bias: -1.275464, T: 140000, Avg. loss: 0.029752
Total training time: 0.48 seconds.
Convergence after 18 epochs took 0.67 seconds
-- Epoch 8
Norm: 122.62, NNZs: 57184, Bias: -1.371176, T: 160000, Avg. loss: 0.020616
Total training time: 0.49 seconds.
-- Epoch 9
-- Epoch 8
Norm: 153.41, NNZs: 55625, Bias: -0.797654, T: 360000, Avg. loss: 0.002062
Total training time: 0.78 seconds.
-- Epoch 19
Norm: 144.84, NNZs: 53692, Bias: -0.760391, T: 320000, Avg. loss: 0.002761
Total training time: 0.68 seconds.
-- Epoch 17
Norm: 151.24, NNZs: 56062, Bias: -0.687263, T: 240000, Avg. loss: 0.010955
Total training time: 0.65 seconds.
-- Epoch 13
Norm: 130.52, NNZs: 58329, Bias: -1.167271, T: 220000, Avg. loss: 0.007176
Total training time: 0.50 seconds.
-- Epoch 12
Norm: 133.05, NNZs: 58416, Bias: -1.167553, T: 340000, Avg. loss: 0.001165
Total training time: 0.80 seconds.
-- Epoch 18
Norm: 126.97, NNZs: 55838, Bias: -1.328661, T: 320000, Avg. loss: 0.001183
Total training time: 0.67 seconds.
-- Epoch 17
Norm: 135.00, NNZs: 58344, Bias: -1.223230, T: 160000, Avg. loss: 0.028580
Total training time: 0.50 seconds.
-- Epoch 9
Norm: 130.25, NNZs: 56967, Bias: -1.319761, T: 300000, Avg. loss: 0.002129
Total training time: 0.80 seconds.
-- Epoch 16
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.9s remaining:    1.5s
Norm: 146.02, NNZs: 54014, Bias: -0.696724, T: 340000, Avg. loss: 0.002132
Total training time: 0.81 seconds.
-- Epoch 18
Norm: 127.94, NNZs: 57480, Bias: -1.388205, T: 340000, Avg. loss: 0.000718
Total training time: 0.81 seconds.
Norm: 127.02, NNZs: 55951, Bias: -1.297318, T: 300000, Avg. loss: 0.001909
Total training time: 0.80 seconds.
-- Epoch 18
-- Epoch 16
Norm: 128.73, NNZs: 57088, Bias: -1.303686, T: 240000, Avg. loss: 0.005478
Total training time: 0.68 seconds.
-- Epoch 13
Norm: 142.58, NNZs: 55748, Bias: -0.650977, T: 160000, Avg. loss: 0.033430
Total training time: 0.49 seconds.
-- Epoch 9
Norm: 120.37, NNZs: 55110, Bias: -1.159485, T: 160000, Avg. loss: 0.019997
Total training time: 0.51 seconds.
-- Epoch 9
Norm: 152.32, NNZs: 56063, Bias: -0.694117, T: 260000, Avg. loss: 0.008661
Total training time: 0.67 seconds.
-- Epoch 14
Norm: 153.26, NNZs: 56078, Bias: -0.774595, T: 360000, Avg. loss: 0.001850
Total training time: 0.80 seconds.
Norm: 131.89, NNZs: 58027, Bias: -1.151606, T: 260000, Avg. loss: 0.003545
Total training time: 0.65 seconds.
-- Epoch 19
-- Epoch 14
Norm: 128.67, NNZs: 55459, Bias: -1.254155, T: 340000, Avg. loss: 0.001324
Total training time: 0.79 seconds.
-- Epoch 18
Norm: 123.91, NNZs: 57359, Bias: -1.247348, T: 160000, Avg. loss: 0.022159
Total training time: 0.51 seconds.
-- Epoch 9
Norm: 127.10, NNZs: 55838, Bias: -1.329841, T: 340000, Avg. loss: 0.000820
Total training time: 0.69 seconds.
-- Epoch 18
Norm: 128.87, NNZs: 57447, Bias: -1.410422, T: 340000, Avg. loss: 0.000653
Total training time: 0.72 seconds.
Norm: 128.02, NNZs: 57480, Bias: -1.388868, T: 360000, Avg. loss: 0.000511
Total training time: 0.83 seconds.
-- Epoch 18
-- Epoch 19
Norm: 143.27, NNZs: 58430, Bias: -1.257287, T: 260000, Avg. loss: 0.005744
Total training time: 0.66 seconds.
-- Epoch 14
Norm: 124.75, NNZs: 57194, Bias: -1.379964, T: 180000, Avg. loss: 0.014408
Total training time: 0.53 seconds.
-- Epoch 10
Norm: 135.86, NNZs: 53809, Bias: -0.741181, T: 160000, Avg. loss: 0.025039
Total training time: 0.54 seconds.
Norm: 119.83, NNZs: 55788, Bias: -1.285784, T: 160000, Avg. loss: 0.020619
Total training time: 0.52 seconds.
-- Epoch 9
-- Epoch 9
Norm: 153.08, NNZs: 56065, Bias: -0.694839, T: 280000, Avg. loss: 0.006111
Total training time: 0.69 seconds.
-- Epoch 15
Norm: 145.14, NNZs: 53692, Bias: -0.758827, T: 340000, Avg. loss: 0.002185
Total training time: 0.72 seconds.
-- Epoch 18
Norm: 153.60, NNZs: 55625, Bias: -0.799125, T: 380000, Avg. loss: 0.001497
Total training time: 0.82 seconds.
-- Epoch 20
Norm: 137.68, NNZs: 58350, Bias: -1.233568, T: 180000, Avg. loss: 0.020527
Total training time: 0.54 seconds.
-- Epoch 10
Norm: 127.19, NNZs: 55838, Bias: -1.329544, T: 360000, Avg. loss: 0.000616
Total training time: 0.72 seconds.
-- Epoch 19
Norm: 143.68, NNZs: 58778, Bias: -1.229734, T: 300000, Avg. loss: 0.003109
Total training time: 0.84 seconds.
-- Epoch 16
Norm: 128.07, NNZs: 57480, Bias: -1.389208, T: 380000, Avg. loss: 0.000348
Total training time: 0.85 seconds.
Convergence after 19 epochs took 0.85 seconds
Norm: 130.50, NNZs: 56967, Bias: -1.321434, T: 320000, Avg. loss: 0.001823
Total training time: 0.85 seconds.
-- Epoch 17
Norm: 132.23, NNZs: 58515, Bias: -1.144110, T: 340000, Avg. loss: 0.001062
Total training time: 0.82 seconds.
Norm: 131.24, NNZs: 58329, Bias: -1.163918, T: 240000, Avg. loss: 0.005033
-- Epoch 18
Total training time: 0.55 seconds.
-- Epoch 13
Norm: 153.68, NNZs: 56066, Bias: -0.692597, T: 300000, Avg. loss: 0.004796
Total training time: 0.71 seconds.
-- Epoch 16
Norm: 122.46, NNZs: 55119, Bias: -1.170124, T: 180000, Avg. loss: 0.013910
Total training time: 0.55 seconds.
Norm: 133.14, NNZs: 58416, Bias: -1.166251, T: 360000, Avg. loss: 0.000744
Total training time: 0.86 seconds.
-- Epoch 10
Convergence after 18 epochs took 0.86 seconds
Norm: 126.17, NNZs: 57359, Bias: -1.272969, T: 180000, Avg. loss: 0.015746
Total training time: 0.55 seconds.
Norm: 132.24, NNZs: 58030, Bias: -1.156554, T: 280000, Avg. loss: 0.002539
Total training time: 0.69 seconds.
-- Epoch 10
-- Epoch 15
Norm: 129.30, NNZs: 57088, Bias: -1.307369, T: 260000, Avg. loss: 0.003989
Total training time: 0.73 seconds.
-- Epoch 14
Norm: 127.27, NNZs: 55840, Bias: -1.329869, T: 380000, Avg. loss: 0.000473
Total training time: 0.74 seconds.
Convergence after 19 epochs took 0.74 seconds
Norm: 153.43, NNZs: 56080, Bias: -0.776033, T: 380000, Avg. loss: 0.001348
Total training time: 0.84 seconds.
-- Epoch 20
Norm: 126.21, NNZs: 57203, Bias: -1.407147, T: 200000, Avg. loss: 0.009933
Total training time: 0.56 seconds.
-- Epoch 11
Norm: 146.24, NNZs: 54014, Bias: -0.696413, T: 360000, Avg. loss: 0.001657
Total training time: 0.87 seconds.
-- Epoch 19
Norm: 143.83, NNZs: 58430, Bias: -1.259212, T: 280000, Avg. loss: 0.004250
Total training time: 0.70 seconds.
-- Epoch 15
Norm: 122.01, NNZs: 55802, Bias: -1.282246, T: 180000, Avg. loss: 0.014398
Total training time: 0.56 seconds.
-- Epoch 10
Norm: 154.12, NNZs: 56070, Bias: -0.700391, T: 320000, Avg. loss: 0.003630
Total training time: 0.72 seconds.
-- Epoch 17
Norm: 145.36, NNZs: 53692, Bias: -0.760968, T: 360000, Avg. loss: 0.001660
Total training time: 0.76 seconds.
-- Epoch 19
Norm: 132.31, NNZs: 58515, Bias: -1.144583, T: 360000, Avg. loss: 0.000705
Total training time: 0.84 seconds.
Convergence after 18 epochs took 0.85 seconds
Norm: 128.95, NNZs: 57447, Bias: -1.410698, T: 360000, Avg. loss: 0.000498
Total training time: 0.76 seconds.
Convergence after 18 epochs took 0.76 seconds
Norm: 127.22, NNZs: 55951, Bias: -1.298881, T: 320000, Avg. loss: 0.001344
Total training time: 0.87 seconds.
-- Epoch 17
Norm: 145.49, NNZs: 55760, Bias: -0.697756, T: 180000, Avg. loss: 0.024376
Total training time: 0.56 seconds.
-- Epoch 10
Norm: 128.81, NNZs: 55459, Bias: -1.255314, T: 360000, Avg. loss: 0.000926
Total training time: 0.85 seconds.
-- Epoch 19
Norm: 127.77, NNZs: 57359, Bias: -1.273615, T: 200000, Avg. loss: 0.011039
Total training time: 0.57 seconds.
-- Epoch 11
Norm: 138.25, NNZs: 53814, Bias: -0.751300, T: 180000, Avg. loss: 0.018499
Total training time: 0.59 seconds.
-- Epoch 10
Norm: 139.64, NNZs: 58362, Bias: -1.246039, T: 200000, Avg. loss: 0.014926
Total training time: 0.58 seconds.
-- Epoch 11
Norm: 127.24, NNZs: 57213, Bias: -1.404155, T: 220000, Avg. loss: 0.006758
Total training time: 0.58 seconds.
Norm: 143.99, NNZs: 58778, Bias: -1.228597, T: 320000, Avg. loss: 0.002340
-- Epoch 12
Total training time: 0.88 seconds.
-- Epoch 17
Norm: 144.25, NNZs: 58430, Bias: -1.260833, T: 300000, Avg. loss: 0.003221
Total training time: 0.72 seconds.
-- Epoch 16
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.9s remaining:    1.5s
Norm: 154.46, NNZs: 56070, Bias: -0.699271, T: 340000, Avg. loss: 0.002642
Total training time: 0.74 seconds.
Norm: 131.74, NNZs: 58329, Bias: -1.167334, T: 260000, Avg. loss: 0.003536
Total training time: 0.59 seconds.
-- Epoch 18
-- Epoch 14
Norm: 130.68, NNZs: 56967, Bias: -1.321537, T: 340000, Avg. loss: 0.001308
Total training time: 0.90 seconds.
-- Epoch 18
Norm: 153.77, NNZs: 55630, Bias: -0.800220, T: 400000, Avg. loss: 0.001409
Total training time: 0.88 seconds.
-- Epoch 21
Norm: 123.95, NNZs: 55125, Bias: -1.171475, T: 200000, Avg. loss: 0.009822
Total training time: 0.59 seconds.
Norm: 146.41, NNZs: 54015, Bias: -0.697101, T: 380000, Avg. loss: 0.001284
Total training time: 0.90 seconds.
-- Epoch 11
-- Epoch 20
Norm: 128.92, NNZs: 57361, Bias: -1.277205, T: 220000, Avg. loss: 0.007898
Total training time: 0.59 seconds.
-- Epoch 12
Norm: 127.95, NNZs: 57213, Bias: -1.409132, T: 240000, Avg. loss: 0.004739
Total training time: 0.60 seconds.
-- Epoch 13
Norm: 129.74, NNZs: 57088, Bias: -1.309205, T: 280000, Avg. loss: 0.003059
Total training time: 0.78 seconds.
Norm: 153.59, NNZs: 56080, Bias: -0.776179, T: 400000, Avg. loss: 0.001305
Total training time: 0.89 seconds.
-- Epoch 15
Norm: 144.23, NNZs: 58778, Bias: -1.231739, T: 340000, Avg. loss: 0.001804
Total training time: 0.90 seconds.
Convergence after 20 epochs took 0.89 seconds
Norm: 127.37, NNZs: 55951, Bias: -1.301549, T: 340000, Avg. loss: 0.000963
Total training time: 0.90 seconds.
-- Epoch 18
-- Epoch 18
Norm: 128.93, NNZs: 55459, Bias: -1.255646, T: 380000, Avg. loss: 0.000818
Total training time: 0.88 seconds.
Norm: 145.53, NNZs: 53692, Bias: -0.761719, T: 380000, Avg. loss: 0.001309
Norm: 154.74, NNZs: 56070, Bias: -0.700511, T: 360000, Avg. loss: 0.002280
Total training time: 0.76 seconds.
-- Epoch 19
Norm: 144.57, NNZs: 58430, Bias: -1.260846, T: 320000, Avg. loss: 0.002430
Total training time: 0.74 seconds.
-- Epoch 17
Total training time: 0.80 seconds.
-- Epoch 20
Norm: 132.09, NNZs: 58329, Bias: -1.174465, T: 280000, Avg. loss: 0.002561
Total training time: 0.61 seconds.
-- Epoch 15
Norm: 132.52, NNZs: 58030, Bias: -1.155300, T: 300000, Avg. loss: 0.001950
Total training time: 0.75 seconds.
-- Epoch 16
Norm: 129.71, NNZs: 57361, Bias: -1.285574, T: 240000, Avg. loss: 0.005505
Total training time: 0.61 seconds.
-- Epoch 13
Convergence after 19 epochs took 0.89 seconds
Norm: 123.51, NNZs: 55803, Bias: -1.300608, T: 200000, Avg. loss: 0.009973
Total training time: 0.61 seconds.
-- Epoch 11
Norm: 140.08, NNZs: 53831, Bias: -0.740504, T: 200000, Avg. loss: 0.013918
Total training time: 0.63 seconds.
-- Epoch 11
Norm: 128.44, NNZs: 57213, Bias: -1.413213, T: 260000, Avg. loss: 0.003234
Total training time: 0.62 seconds.
-- Epoch 14
Norm: 124.94, NNZs: 55129, Bias: -1.183871, T: 220000, Avg. loss: 0.006523
Total training time: 0.62 seconds.
-- Epoch 12
Norm: 141.03, NNZs: 58369, Bias: -1.257404, T: 220000, Avg. loss: 0.010605
Total training time: 0.62 seconds.
-- Epoch 12
Norm: 144.42, NNZs: 58778, Bias: -1.228127, T: 360000, Avg. loss: 0.001466
Total training time: 0.92 seconds.
-- Epoch 19
Norm: 154.94, NNZs: 56070, Bias: -0.702254, T: 380000, Avg. loss: 0.001640
Total training time: 0.78 seconds.
-- Epoch 20
Norm: 144.81, NNZs: 58430, Bias: -1.263458, T: 340000, Avg. loss: 0.001891
Total training time: 0.76 seconds.
-- Epoch 18
Norm: 130.82, NNZs: 56967, Bias: -1.321721, T: 360000, Avg. loss: 0.001058
Total training time: 0.93 seconds.
Norm: 132.37, NNZs: 58329, Bias: -1.171890, T: 300000, Avg. loss: 0.001980
Total training time: 0.63 seconds.
-- Epoch 19
-- Epoch 16
Norm: 147.77, NNZs: 55776, Bias: -0.694863, T: 200000, Avg. loss: 0.018722
Total training time: 0.62 seconds.
-- Epoch 11
Norm: 146.55, NNZs: 54015, Bias: -0.699613, T: 400000, Avg. loss: 0.001012
Total training time: 0.94 seconds.
Convergence after 20 epochs took 0.94 seconds
Norm: 130.27, NNZs: 57361, Bias: -1.289039, T: 260000, Avg. loss: 0.003926
Total training time: 0.62 seconds.
-- Epoch 14
Norm: 128.79, NNZs: 57216, Bias: -1.415556, T: 280000, Avg. loss: 0.002303
Total training time: 0.64 seconds.
-- Epoch 15
Norm: 125.66, NNZs: 55129, Bias: -1.183992, T: 240000, Avg. loss: 0.004644
Total training time: 0.64 seconds.
-- Epoch 13
Norm: 141.38, NNZs: 53832, Bias: -0.748806, T: 220000, Avg. loss: 0.009835
Total training time: 0.65 seconds.
-- Epoch 12
Norm: 130.05, NNZs: 57088, Bias: -1.310550, T: 300000, Avg. loss: 0.002230
Total training time: 0.81 seconds.
-- Epoch 16
Norm: 142.13, NNZs: 58371, Bias: -1.260476, T: 240000, Avg. loss: 0.008238
Total training time: 0.64 seconds.
-- Epoch 13
Norm: 155.12, NNZs: 56077, Bias: -0.703693, T: 400000, Avg. loss: 0.001518
Total training time: 0.80 seconds.
-- Epoch 21
Norm: 127.49, NNZs: 55951, Bias: -1.301399, T: 360000, Avg. loss: 0.000756
Total training time: 0.94 seconds.
-- Epoch 19
Norm: 144.58, NNZs: 58781, Bias: -1.230795, T: 380000, Avg. loss: 0.001229
Total training time: 0.94 seconds.
-- Epoch 20
Norm: 124.58, NNZs: 55806, Bias: -1.302543, T: 220000, Avg. loss: 0.007046
Total training time: 0.64 seconds.
Norm: 145.67, NNZs: 53692, Bias: -0.764155, T: 400000, Avg. loss: 0.001006
Total training time: 0.84 seconds.
Norm: 144.99, NNZs: 58430, Bias: -1.264383, T: 360000, Avg. loss: 0.001430
Total training time: 0.78 seconds.
Norm: 132.57, NNZs: 58329, Bias: -1.178827, T: 320000, Avg. loss: 0.001472
Total training time: 0.65 seconds.
-- Epoch 12
-- Epoch 21
-- Epoch 19
Norm: 153.89, NNZs: 55630, Bias: -0.801426, T: 420000, Avg. loss: 0.000964
Total training time: 0.93 seconds.
Convergence after 21 epochs took 0.93 seconds
-- Epoch 17
Norm: 130.69, NNZs: 57363, Bias: -1.286912, T: 280000, Avg. loss: 0.002917
Total training time: 0.64 seconds.
-- Epoch 15
Norm: 130.92, NNZs: 56967, Bias: -1.323202, T: 380000, Avg. loss: 0.000805
Total training time: 0.96 seconds.
Convergence after 19 epochs took 0.96 seconds
Norm: 149.55, NNZs: 55791, Bias: -0.691841, T: 220000, Avg. loss: 0.014340
Total training time: 0.64 seconds.
-- Epoch 12
Norm: 126.17, NNZs: 55130, Bias: -1.188129, T: 260000, Avg. loss: 0.003341
Total training time: 0.65 seconds.
Norm: 142.35, NNZs: 53834, Bias: -0.754006, T: 240000, Avg. loss: 0.007262
Total training time: 0.66 seconds.
-- Epoch 14
-- Epoch 13
Norm: 155.25, NNZs: 56077, Bias: -0.704798, T: 420000, Avg. loss: 0.001070
Total training time: 0.81 seconds.
Convergence after 21 epochs took 0.81 seconds
Norm: 132.71, NNZs: 58030, Bias: -1.163275, T: 320000, Avg. loss: 0.001475
Total training time: 0.80 seconds.
-- Epoch 17
Norm: 145.16, NNZs: 58433, Bias: -1.262645, T: 380000, Avg. loss: 0.001339
Total training time: 0.80 seconds.
Norm: 129.02, NNZs: 57216, Bias: -1.416214, T: 300000, Avg. loss: 0.001511
Total training time: 0.66 seconds.
-- Epoch 20
-- Epoch 16
Norm: 130.30, NNZs: 57088, Bias: -1.310388, T: 320000, Avg. loss: 0.001792
Total training time: 0.84 seconds.
-- Epoch 17
Norm: 132.72, NNZs: 58329, Bias: -1.174518, T: 340000, Avg. loss: 0.001157
Total training time: 0.67 seconds.
-- Epoch 18
Norm: 130.98, NNZs: 57363, Bias: -1.291824, T: 300000, Avg. loss: 0.002106
Total training time: 0.66 seconds.
Norm: 144.70, NNZs: 58781, Bias: -1.230694, T: 400000, Avg. loss: 0.000924
Total training time: 0.97 seconds.
-- Epoch 16
Convergence after 20 epochs took 0.97 seconds
Norm: 150.85, NNZs: 55819, Bias: -0.703407, T: 240000, Avg. loss: 0.010388
Total training time: 0.66 seconds.
-- Epoch 13
Norm: 126.51, NNZs: 55131, Bias: -1.191124, T: 280000, Avg. loss: 0.002240
Total training time: 0.67 seconds.
Norm: 143.08, NNZs: 53842, Bias: -0.755868, T: 260000, Avg. loss: 0.005381
Total training time: 0.68 seconds.
-- Epoch 15
-- Epoch 14
Norm: 142.92, NNZs: 58371, Bias: -1.266573, T: 260000, Avg. loss: 0.005933
Total training time: 0.67 seconds.
-- Epoch 14
Norm: 127.57, NNZs: 55951, Bias: -1.301955, T: 380000, Avg. loss: 0.000575
Total training time: 0.97 seconds.
Convergence after 19 epochs took 0.97 seconds
Norm: 145.77, NNZs: 53692, Bias: -0.763108, T: 420000, Avg. loss: 0.000798
Total training time: 0.87 seconds.
Convergence after 21 epochs took 0.87 seconds
Norm: 125.36, NNZs: 55806, Bias: -1.316215, T: 240000, Avg. loss: 0.005144
Total training time: 0.67 seconds.
-- Epoch 13
Norm: 132.87, NNZs: 58030, Bias: -1.157535, T: 340000, Avg. loss: 0.001191
Total training time: 0.81 seconds.
Norm: 129.18, NNZs: 57216, Bias: -1.417905, T: 320000, Avg. loss: 0.001085
Total training time: 0.68 seconds.
-- Epoch 18
-- Epoch 17
Norm: 145.28, NNZs: 58433, Bias: -1.263698, T: 400000, Avg. loss: 0.000958
Total training time: 0.81 seconds.
Convergence after 20 epochs took 0.81 seconds
Norm: 132.81, NNZs: 58329, Bias: -1.176119, T: 360000, Avg. loss: 0.000766
Total training time: 0.68 seconds.
Convergence after 18 epochs took 0.68 seconds
Norm: 130.49, NNZs: 57088, Bias: -1.310751, T: 340000, Avg. loss: 0.001370
Total training time: 0.86 seconds.
-- Epoch 18
Norm: 131.21, NNZs: 57363, Bias: -1.293773, T: 320000, Avg. loss: 0.001679
Total training time: 0.67 seconds.
-- Epoch 17
Norm: 151.86, NNZs: 55826, Bias: -0.704087, T: 260000, Avg. loss: 0.008017
Total training time: 0.67 seconds.
-- Epoch 14
Norm: 143.60, NNZs: 53843, Bias: -0.757927, T: 280000, Avg. loss: 0.003849
Total training time: 0.69 seconds.
Norm: 126.77, NNZs: 55131, Bias: -1.191035, T: 300000, Avg. loss: 0.001651
Total training time: 0.69 seconds.
-- Epoch 15
-- Epoch 16
Norm: 143.48, NNZs: 58371, Bias: -1.268235, T: 280000, Avg. loss: 0.004177
Total training time: 0.69 seconds.
-- Epoch 15
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    1.1s finished
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    1.1s finished
Norm: 125.91, NNZs: 55809, Bias: -1.314292, T: 260000, Avg. loss: 0.003578
Total training time: 0.69 seconds.
-- Epoch 14
Norm: 129.30, NNZs: 57216, Bias: -1.419352, T: 340000, Avg. loss: 0.000730
Total training time: 0.69 seconds.
-- Epoch 18
Norm: 132.96, NNZs: 58030, Bias: -1.158646, T: 360000, Avg. loss: 0.000763
Total training time: 0.83 seconds.
Norm: 130.64, NNZs: 57088, Bias: -1.311513, T: 360000, Avg. loss: 0.001138
Total training time: 0.87 seconds.
-- Epoch 19
Convergence after 18 epochs took 0.87 seconds
Norm: 131.38, NNZs: 57364, Bias: -1.295490, T: 340000, Avg. loss: 0.001299
Total training time: 0.69 seconds.
-- Epoch 18
Norm: 152.56, NNZs: 55839, Bias: -0.702501, T: 280000, Avg. loss: 0.005579
Total training time: 0.69 seconds.
-- Epoch 15
Norm: 143.99, NNZs: 53843, Bias: -0.761014, T: 300000, Avg. loss: 0.002881
Total training time: 0.71 seconds.
-- Epoch 16
Norm: 126.94, NNZs: 55131, Bias: -1.194106, T: 320000, Avg. loss: 0.001116
Total training time: 0.70 seconds.
-- Epoch 17
Norm: 143.92, NNZs: 58372, Bias: -1.265916, T: 300000, Avg. loss: 0.003202
Total training time: 0.70 seconds.
-- Epoch 16
Norm: 126.31, NNZs: 55809, Bias: -1.317353, T: 280000, Avg. loss: 0.002604
Total training time: 0.70 seconds.
-- Epoch 15
Norm: 129.38, NNZs: 57216, Bias: -1.420420, T: 360000, Avg. loss: 0.000550
Total training time: 0.71 seconds.
Convergence after 18 epochs took 0.71 seconds
Norm: 131.53, NNZs: 57364, Bias: -1.293147, T: 360000, Avg. loss: 0.001074
Total training time: 0.70 seconds.
-- Epoch 19
Norm: 133.05, NNZs: 58030, Bias: -1.158563, T: 380000, Avg. loss: 0.000759
Total training time: 0.84 seconds.
Convergence after 19 epochs took 0.84 seconds
Norm: 153.11, NNZs: 55840, Bias: -0.704243, T: 300000, Avg. loss: 0.004272
Total training time: 0.70 seconds.
-- Epoch 16
Norm: 144.26, NNZs: 53843, Bias: -0.761781, T: 320000, Avg. loss: 0.002000
Total training time: 0.72 seconds.
-- Epoch 17
Norm: 127.07, NNZs: 55131, Bias: -1.194502, T: 340000, Avg. loss: 0.000804
Total training time: 0.71 seconds.
-- Epoch 18
Norm: 144.23, NNZs: 58377, Bias: -1.270415, T: 320000, Avg. loss: 0.002333
Total training time: 0.72 seconds.
-- Epoch 17
Norm: 131.62, NNZs: 57364, Bias: -1.295834, T: 380000, Avg. loss: 0.000780
Total training time: 0.71 seconds.
Norm: 126.57, NNZs: 55809, Bias: -1.317848, T: 300000, Avg. loss: 0.001709
Total training time: 0.71 seconds.
Convergence after 19 epochs took 0.71 seconds
-- Epoch 16
Norm: 153.51, NNZs: 55842, Bias: -0.706646, T: 320000, Avg. loss: 0.003157
Total training time: 0.71 seconds.
-- Epoch 17
Norm: 144.47, NNZs: 53843, Bias: -0.760399, T: 340000, Avg. loss: 0.001526
Total training time: 0.73 seconds.
-- Epoch 18
Norm: 127.15, NNZs: 55131, Bias: -1.195125, T: 360000, Avg. loss: 0.000561
Total training time: 0.72 seconds.
-- Epoch 19
Norm: 144.48, NNZs: 58379, Bias: -1.270544, T: 340000, Avg. loss: 0.001860
Total training time: 0.73 seconds.
-- Epoch 18
Norm: 153.81, NNZs: 55842, Bias: -0.707433, T: 340000, Avg. loss: 0.002322
Total training time: 0.71 seconds.
-- Epoch 18
Norm: 126.79, NNZs: 55810, Bias: -1.320643, T: 320000, Avg. loss: 0.001402
Total training time: 0.72 seconds.
-- Epoch 17
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    1.0s finished
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    1.0s remaining:    1.6s
Norm: 144.62, NNZs: 53843, Bias: -0.762478, T: 360000, Avg. loss: 0.001086
Total training time: 0.74 seconds.
-- Epoch 19
Norm: 127.22, NNZs: 55131, Bias: -1.195357, T: 380000, Avg. loss: 0.000392
Total training time: 0.73 seconds.
Norm: 154.03, NNZs: 55842, Bias: -0.707041, T: 360000, Avg. loss: 0.001778
Total training time: 0.72 seconds.
Convergence after 19 epochs took 0.73 seconds
Norm: 144.66, NNZs: 58379, Bias: -1.270290, T: 360000, Avg. loss: 0.001336
Total training time: 0.74 seconds.
-- Epoch 19
-- Epoch 19
Norm: 126.94, NNZs: 55810, Bias: -1.323778, T: 340000, Avg. loss: 0.001016
Total training time: 0.73 seconds.
-- Epoch 18
Norm: 154.20, NNZs: 55842, Bias: -0.708561, T: 380000, Avg. loss: 0.001319
Total training time: 0.73 seconds.
-- Epoch 20
Norm: 144.73, NNZs: 53843, Bias: -0.761577, T: 380000, Avg. loss: 0.000807
Total training time: 0.75 seconds.
Convergence after 19 epochs took 0.75 seconds
Norm: 144.83, NNZs: 58380, Bias: -1.271359, T: 380000, Avg. loss: 0.001278
Total training time: 0.75 seconds.
Convergence after 19 epochs took 0.75 seconds
Norm: 127.06, NNZs: 55810, Bias: -1.324765, T: 360000, Avg. loss: 0.000755
Total training time: 0.74 seconds.
Convergence after 18 epochs took 0.74 seconds
Norm: 154.33, NNZs: 55842, Bias: -0.707972, T: 400000, Avg. loss: 0.000959
Total training time: 0.74 seconds.
-- Epoch 21
Norm: 154.42, NNZs: 55842, Bias: -0.708254, T: 420000, Avg. loss: 0.000744
Total training time: 0.74 seconds.
Convergence after 21 epochs took 0.74 seconds
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    1.0s finished
	accuracy: 5-fold cross validation: [0.362  0.3618 0.3604 0.357  0.347 ]
	test accuracy: 5-fold cross validation accuracy: 0.36 (+/- 0.01)
dimensionality: 74535
density: 0.824866



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.33112
	accuracy score (normalize=False):  8278

compute the precision
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.6s finished
	precision score (average=macro):  0.27282124734155244
	precision score (average=micro):  0.33112
	precision score (average=weighted):  0.31956646020638246
	precision score (average=None):  [0.52866242 0.16090364 0.18806355 0.23289183 0.20803096 0.21543408
 0.17472853 0.47385496]
	precision score (average=None, zero_division=1):  [0.52866242 0.16090364 0.18806355 0.23289183 0.20803096 0.21543408
 0.17472853 0.47385496]

compute the precision
	recall score (average=macro):  0.27561019880416726
	recall score (average=micro):  0.33112
	recall score (average=weighted):  0.33112
	recall score (average=None):  [0.59498208 0.1516073  0.17237308 0.2402277  0.18638925 0.21157895
 0.15102389 0.49669934]
	recall score (average=None, zero_division=1):  [0.59498208 0.1516073  0.17237308 0.2402277  0.18638925 0.21157895
 0.15102389 0.49669934]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.2736861873637256
	f1 score (average=micro):  0.33112
	f1 score (average=weighted):  0.3247052391718205
	f1 score (average=None):  [0.55986509 0.1561172  0.1798768  0.2365029  0.19661637 0.21348911
 0.16201373 0.4850083 ]

compute the F-beta score
	f beta score (average=macro):  0.27304012288461643
	f beta score (average=micro):  0.3311200000000001
	f beta score (average=weighted):  0.3214736669420114
	f beta score (average=None):  [0.54071661 0.15895427 0.18470102 0.23432294 0.20330969 0.21465186
 0.16941041 0.47825417]

compute the average Hamming loss
	hamming loss:  0.66888

jaccard similarity coefficient score
	jaccard score (average=macro):  0.1678971155819445
	jaccard score (average=None):  [0.38875878 0.08466764 0.09882671 0.13411017 0.10902637 0.11950059
 0.08814741 0.32013925]


================================================================================
Classifier.PERCEPTRON
________________________________________________________________________________
Training: 
Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,
           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=-1,
           penalty=None, random_state=0, shuffle=True, tol=0.001,
           validation_fraction=0.1, verbose=True, warm_start=False)
-- Epoch 1
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 49.94, NNZs: 35277, Bias: -0.220000, T: 25000, Avg. loss: 0.048511
Total training time: 0.03 seconds.
Norm: 48.01, NNZs: 37649, Bias: -0.450000, T: 25000, Avg. loss: 0.048343
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 45.45, NNZs: 34185, Bias: -0.430000, T: 25000, Avg. loss: 0.040543
Total training time: 0.04 seconds.
-- Epoch 1
-- Epoch 1
-- Epoch 2
Norm: 44.05, NNZs: 31997, Bias: -0.420000, T: 25000, Avg. loss: 0.038278
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 44.97, NNZs: 35540, Bias: -0.440000, T: 25000, Avg. loss: 0.038791
Total training time: 0.05 seconds.
-- Epoch 2
-- Epoch 2
Norm: 60.41, NNZs: 42788, Bias: -0.490000, T: 50000, Avg. loss: 0.025419
Total training time: 0.07 seconds.
-- Epoch 3
Norm: 55.34, NNZs: 35755, Bias: -0.470000, T: 50000, Avg. loss: 0.019360
Total training time: 0.06 seconds.
-- Epoch 3
Norm: 43.54, NNZs: 33732, Bias: -0.410000, T: 25000, Avg. loss: 0.038814
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 47.02, NNZs: 36445, Bias: -0.340000, T: 25000, Avg. loss: 0.042001
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 62.81, NNZs: 37785, Bias: -0.540000, T: 75000, Avg. loss: 0.010493
Total training time: 0.08 seconds.
-- Epoch 4
Norm: 56.54, NNZs: 39984, Bias: -0.520000, T: 50000, Avg. loss: 0.020487
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 50.73, NNZs: 37352, Bias: -0.230000, T: 25000, Avg. loss: 0.052154
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 57.09, NNZs: 38368, Bias: -0.570000, T: 50000, Avg. loss: 0.019603
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 67.70, NNZs: 38741, Bias: -0.570000, T: 100000, Avg. loss: 0.005412
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 61.37, NNZs: 40079, Bias: -0.290000, T: 50000, Avg. loss: 0.024697
Total training time: 0.12 seconds.
-- Epoch 3
Norm: 68.73, NNZs: 44762, Bias: -0.540000, T: 75000, Avg. loss: 0.013614
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 55.42, NNZs: 37736, Bias: -0.510000, T: 50000, Avg. loss: 0.019377
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 71.12, NNZs: 39382, Bias: -0.630000, T: 125000, Avg. loss: 0.003405
Total training time: 0.10 seconds.
-- Epoch 6
Norm: 68.94, NNZs: 41976, Bias: -0.290000, T: 75000, Avg. loss: 0.013021
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 64.49, NNZs: 40317, Bias: -0.620000, T: 75000, Avg. loss: 0.010719
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 73.23, NNZs: 39686, Bias: -0.650000, T: 150000, Avg. loss: 0.001732
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 58.74, NNZs: 41093, Bias: -0.500000, T: 50000, Avg. loss: 0.020389
Total training time: 0.10 seconds.
Norm: 63.45, NNZs: 41979, Bias: -0.620000, T: 75000, Avg. loss: 0.010577
Total training time: 0.13 seconds.
-- Epoch 3
-- Epoch 4
Norm: 74.26, NNZs: 45888, Bias: -0.540000, T: 100000, Avg. loss: 0.007461
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 63.05, NNZs: 42238, Bias: -0.330000, T: 50000, Avg. loss: 0.029690
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 62.75, NNZs: 39523, Bias: -0.630000, T: 75000, Avg. loss: 0.009860
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 75.01, NNZs: 39969, Bias: -0.660000, T: 175000, Avg. loss: 0.001218
Total training time: 0.12 seconds.
-- Epoch 8
Norm: 69.10, NNZs: 41322, Bias: -0.670000, T: 100000, Avg. loss: 0.005387
Total training time: 0.15 seconds.
-- Epoch 5
Norm: 68.00, NNZs: 42900, Bias: -0.640000, T: 100000, Avg. loss: 0.005159
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 78.01, NNZs: 46636, Bias: -0.630000, T: 125000, Avg. loss: 0.004188
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 71.54, NNZs: 44332, Bias: -0.340000, T: 75000, Avg. loss: 0.015780
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 73.93, NNZs: 42991, Bias: -0.290000, T: 100000, Avg. loss: 0.007164
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 65.70, NNZs: 42855, Bias: -0.540000, T: 75000, Avg. loss: 0.009921
Total training time: 0.11 seconds.
-- Epoch 4
Norm: 76.21, NNZs: 40095, Bias: -0.640000, T: 200000, Avg. loss: 0.000627
Total training time: 0.13 seconds.
-- Epoch 9
Norm: 67.20, NNZs: 40461, Bias: -0.610000, T: 100000, Avg. loss: 0.005380
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 70.92, NNZs: 43456, Bias: -0.710000, T: 125000, Avg. loss: 0.002823
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 80.74, NNZs: 47022, Bias: -0.580000, T: 150000, Avg. loss: 0.002574
Total training time: 0.16 seconds.
-- Epoch 7
Norm: 77.04, NNZs: 45379, Bias: -0.290000, T: 100000, Avg. loss: 0.008503
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 72.22, NNZs: 41840, Bias: -0.610000, T: 125000, Avg. loss: 0.003166
Total training time: 0.16 seconds.
-- Epoch 6
Norm: 77.29, NNZs: 40256, Bias: -0.620000, T: 225000, Avg. loss: 0.000533
Total training time: 0.14 seconds.
-- Epoch 10
Norm: 77.11, NNZs: 43588, Bias: -0.370000, T: 125000, Avg. loss: 0.003832
Total training time: 0.17 seconds.
-- Epoch 6
Norm: 70.47, NNZs: 43734, Bias: -0.500000, T: 100000, Avg. loss: 0.005069
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 73.03, NNZs: 43866, Bias: -0.710000, T: 150000, Avg. loss: 0.001642
Total training time: 0.16 seconds.
-- Epoch 7
Norm: 82.53, NNZs: 47281, Bias: -0.610000, T: 175000, Avg. loss: 0.001698
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 70.16, NNZs: 41117, Bias: -0.650000, T: 125000, Avg. loss: 0.002676
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 80.90, NNZs: 46006, Bias: -0.360000, T: 125000, Avg. loss: 0.005030
Total training time: 0.12 seconds.
-- Epoch 6
Norm: 77.94, NNZs: 40385, Bias: -0.670000, T: 250000, Avg. loss: 0.000317
Total training time: 0.15 seconds.
-- Epoch 11
Norm: 74.46, NNZs: 42170, Bias: -0.650000, T: 150000, Avg. loss: 0.001790
Total training time: 0.17 seconds.
-- Epoch 7
Norm: 79.49, NNZs: 43830, Bias: -0.390000, T: 150000, Avg. loss: 0.002403
Total training time: 0.19 seconds.
-- Epoch 7
Norm: 74.44, NNZs: 44088, Bias: -0.740000, T: 175000, Avg. loss: 0.000911
Total training time: 0.17 seconds.
Norm: 83.97, NNZs: 47457, Bias: -0.650000, T: 200000, Avg. loss: 0.001088
Total training time: 0.18 seconds.
-- Epoch 8
-- Epoch 9
Norm: 73.46, NNZs: 44240, Bias: -0.590000, T: 125000, Avg. loss: 0.002559
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 83.67, NNZs: 46408, Bias: -0.390000, T: 150000, Avg. loss: 0.002919
Total training time: 0.13 seconds.
-- Epoch 7
Norm: 78.63, NNZs: 40488, Bias: -0.650000, T: 275000, Avg. loss: 0.000303
Total training time: 0.16 seconds.
Convergence after 11 epochs took 0.16 seconds
Norm: 72.29, NNZs: 41480, Bias: -0.660000, T: 150000, Avg. loss: 0.001835
Total training time: 0.16 seconds.
-- Epoch 7
Norm: 76.08, NNZs: 42429, Bias: -0.690000, T: 175000, Avg. loss: 0.001230
Total training time: 0.19 seconds.
-- Epoch 8
Norm: 75.59, NNZs: 44222, Bias: -0.740000, T: 200000, Avg. loss: 0.000620
Total training time: 0.18 seconds.
Norm: 85.17, NNZs: 47607, Bias: -0.630000, T: 225000, Avg. loss: 0.000798
Total training time: 0.19 seconds.
-- Epoch 9
-- Epoch 10
Norm: 81.54, NNZs: 44093, Bias: -0.380000, T: 175000, Avg. loss: 0.001704
Total training time: 0.20 seconds.
Norm: 85.86, NNZs: 46630, Bias: -0.380000, T: 175000, Avg. loss: 0.002064
Total training time: 0.14 seconds.
-- Epoch 8
-- Epoch 8
Norm: 75.82, NNZs: 44556, Bias: -0.610000, T: 150000, Avg. loss: 0.001885
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 73.70, NNZs: 41739, Bias: -0.680000, T: 175000, Avg. loss: 0.000874
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 86.03, NNZs: 47676, Bias: -0.630000, T: 250000, Avg. loss: 0.000516
Total training time: 0.20 seconds.
Norm: 76.42, NNZs: 44370, Bias: -0.780000, T: 225000, Avg. loss: 0.000432
Total training time: 0.19 seconds.
-- Epoch 11
-- Epoch 10
Norm: 87.44, NNZs: 46801, Bias: -0.390000, T: 200000, Avg. loss: 0.001291
Total training time: 0.14 seconds.
-- Epoch 9
Norm: 77.24, NNZs: 42548, Bias: -0.730000, T: 200000, Avg. loss: 0.000761
Total training time: 0.20 seconds.
-- Epoch 9
Norm: 83.07, NNZs: 44301, Bias: -0.380000, T: 200000, Avg. loss: 0.001160
Total training time: 0.21 seconds.
-- Epoch 9
Norm: 77.38, NNZs: 44773, Bias: -0.610000, T: 175000, Avg. loss: 0.001113
Total training time: 0.16 seconds.
-- Epoch 8
Norm: 86.83, NNZs: 47773, Bias: -0.660000, T: 275000, Avg. loss: 0.000488
Total training time: 0.21 seconds.
Norm: 77.11, NNZs: 44491, Bias: -0.770000, T: 250000, Avg. loss: 0.000348
Total training time: 0.20 seconds.
Convergence after 11 epochs took 0.21 seconds
-- Epoch 11
Norm: 74.72, NNZs: 41928, Bias: -0.710000, T: 200000, Avg. loss: 0.000546
Total training time: 0.18 seconds.
-- Epoch 9
Norm: 88.57, NNZs: 46920, Bias: -0.410000, T: 225000, Avg. loss: 0.000925
Total training time: 0.15 seconds.
-- Epoch 10
Norm: 78.30, NNZs: 42677, Bias: -0.720000, T: 225000, Avg. loss: 0.000559
Total training time: 0.21 seconds.
-- Epoch 10
Norm: 84.18, NNZs: 44463, Bias: -0.410000, T: 225000, Avg. loss: 0.000735
Total training time: 0.22 seconds.
-- Epoch 10
Norm: 77.68, NNZs: 44578, Bias: -0.740000, T: 275000, Avg. loss: 0.000245
Total training time: 0.20 seconds.
Convergence after 11 epochs took 0.20 seconds
Norm: 89.73, NNZs: 47058, Bias: -0.420000, T: 250000, Avg. loss: 0.000909
Total training time: 0.16 seconds.
-- Epoch 11
Norm: 78.48, NNZs: 44896, Bias: -0.620000, T: 200000, Avg. loss: 0.000607
Total training time: 0.18 seconds.
-- Epoch 9
Norm: 75.60, NNZs: 42020, Bias: -0.680000, T: 225000, Avg. loss: 0.000447
Total training time: 0.20 seconds.
-- Epoch 10
Norm: 78.96, NNZs: 42809, Bias: -0.730000, T: 250000, Avg. loss: 0.000308
Total training time: 0.22 seconds.
-- Epoch 11
Norm: 90.55, NNZs: 47146, Bias: -0.440000, T: 275000, Avg. loss: 0.000549
Total training time: 0.17 seconds.
Convergence after 11 epochs took 0.17 seconds
Norm: 84.98, NNZs: 44546, Bias: -0.390000, T: 250000, Avg. loss: 0.000511
Total training time: 0.24 seconds.
-- Epoch 11
Norm: 79.41, NNZs: 45032, Bias: -0.630000, T: 225000, Avg. loss: 0.000565
Total training time: 0.19 seconds.
-- Epoch 10
Norm: 76.29, NNZs: 42088, Bias: -0.730000, T: 250000, Avg. loss: 0.000387
Total training time: 0.21 seconds.
Convergence after 10 epochs took 0.21 seconds
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.2s remaining:    0.4s
Norm: 79.49, NNZs: 42923, Bias: -0.740000, T: 275000, Avg. loss: 0.000254
Total training time: 0.23 seconds.
Convergence after 11 epochs took 0.23 seconds
Norm: 85.56, NNZs: 44595, Bias: -0.400000, T: 275000, Avg. loss: 0.000297
Total training time: 0.24 seconds.
Norm: 79.98, NNZs: 45076, Bias: -0.640000, T: 250000, Avg. loss: 0.000245
Total training time: 0.19 seconds.
Convergence after 11 epochs took 0.24 seconds
Convergence after 10 epochs took 0.19 seconds
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.3s finished
train time: 0.310s
test time:  0.026s
accuracy:   0.327


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 45.26, NNZs: 31912, Bias: -0.260000, T: 20000, Avg. loss: 0.045741
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 43.22, NNZs: 32709, Bias: -0.320000, T: 20000, Avg. loss: 0.039145
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 41.96, NNZs: 30969, Bias: -0.420000, T: 20000, Avg. loss: 0.038511
Total training time: 0.01 seconds.
Norm: 44.68, NNZs: 34562, Bias: -0.360000, T: 20000, Avg. loss: 0.046021
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 2
Norm: 40.52, NNZs: 28883, Bias: -0.370000, T: 20000, Avg. loss: 0.036548
Total training time: 0.01 seconds.
Norm: 41.27, NNZs: 32028, Bias: -0.460000, T: 20000, Avg. loss: 0.036786
Total training time: 0.01 seconds.
-- Epoch 2
-- Epoch 2
Norm: 47.39, NNZs: 34566, Bias: -0.260000, T: 20000, Avg. loss: 0.050388
Total training time: 0.01 seconds.
Norm: 40.26, NNZs: 30337, Bias: -0.410000, T: 20000, Avg. loss: 0.035028
Total training time: 0.02 seconds.
-- Epoch 2
-- Epoch 2
Norm: 52.99, NNZs: 36466, Bias: -0.380000, T: 40000, Avg. loss: 0.017210
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 51.70, NNZs: 34619, Bias: -0.500000, T: 40000, Avg. loss: 0.017259
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 55.70, NNZs: 38862, Bias: -0.490000, T: 40000, Avg. loss: 0.022695
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 50.44, NNZs: 32160, Bias: -0.450000, T: 40000, Avg. loss: 0.016588
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 57.86, NNZs: 38908, Bias: -0.340000, T: 40000, Avg. loss: 0.026169
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 49.88, NNZs: 34106, Bias: -0.520000, T: 40000, Avg. loss: 0.017294
Total training time: 0.03 seconds.
-- Epoch 3
Norm: 56.28, NNZs: 36430, Bias: -0.320000, T: 40000, Avg. loss: 0.022185
Total training time: 0.04 seconds.
Norm: 50.90, NNZs: 36219, Bias: -0.540000, T: 40000, Avg. loss: 0.016852
Total training time: 0.03 seconds.
-- Epoch 3
-- Epoch 3
Norm: 58.83, NNZs: 38106, Bias: -0.500000, T: 60000, Avg. loss: 0.008061
Total training time: 0.04 seconds.
-- Epoch 4
Norm: 58.04, NNZs: 36196, Bias: -0.570000, T: 60000, Avg. loss: 0.008305
Total training time: 0.04 seconds.
-- Epoch 4
Norm: 56.18, NNZs: 33582, Bias: -0.500000, T: 60000, Avg. loss: 0.007917
Total training time: 0.04 seconds.
Norm: 62.05, NNZs: 40506, Bias: -0.540000, T: 60000, Avg. loss: 0.010034
Total training time: 0.04 seconds.
-- Epoch 4
-- Epoch 4
Norm: 64.85, NNZs: 40786, Bias: -0.330000, T: 60000, Avg. loss: 0.012344
Total training time: 0.04 seconds.
-- Epoch 4
Norm: 55.66, NNZs: 35703, Bias: -0.570000, T: 60000, Avg. loss: 0.007856
Total training time: 0.04 seconds.
-- Epoch 4
-- Epoch 1
Norm: 62.15, NNZs: 37859, Bias: -0.360000, T: 60000, Avg. loss: 0.010124
Total training time: 0.05 seconds.
Norm: 56.62, NNZs: 37789, Bias: -0.540000, T: 60000, Avg. loss: 0.007708
Total training time: 0.04 seconds.
-- Epoch 4
-- Epoch 4
Norm: 62.18, NNZs: 38933, Bias: -0.510000, T: 80000, Avg. loss: 0.003588
Total training time: 0.05 seconds.
-- Epoch 1
-- Epoch 5
Norm: 61.82, NNZs: 37042, Bias: -0.580000, T: 80000, Avg. loss: 0.004164
Total training time: 0.05 seconds.
-- Epoch 5
Norm: 60.08, NNZs: 34502, Bias: -0.570000, T: 80000, Avg. loss: 0.004130
Total training time: 0.05 seconds.
-- Epoch 5
-- Epoch 1
Norm: 66.35, NNZs: 41346, Bias: -0.530000, T: 80000, Avg. loss: 0.005350
Total training time: 0.05 seconds.
-- Epoch 5
Norm: 69.31, NNZs: 41763, Bias: -0.380000, T: 80000, Avg. loss: 0.006470
Total training time: 0.05 seconds.
-- Epoch 5
-- Epoch 1
Norm: 59.35, NNZs: 36589, Bias: -0.620000, T: 80000, Avg. loss: 0.004030
Total training time: 0.06 seconds.
-- Epoch 5
Norm: 66.07, NNZs: 38603, Bias: -0.350000, T: 80000, Avg. loss: 0.004683
Total training time: 0.06 seconds.
-- Epoch 5
Norm: 60.41, NNZs: 38624, Bias: -0.680000, T: 80000, Avg. loss: 0.004225
Total training time: 0.06 seconds.
-- Epoch 5
-- Epoch 1
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
-- Epoch 1
Norm: 64.12, NNZs: 37552, Bias: -0.590000, T: 100000, Avg. loss: 0.001852
Total training time: 0.07 seconds.
-- Epoch 6
Norm: 64.40, NNZs: 39379, Bias: -0.490000, T: 100000, Avg. loss: 0.001958
Total training time: 0.07 seconds.
Norm: 62.33, NNZs: 34865, Bias: -0.560000, T: 100000, Avg. loss: 0.002087
Total training time: 0.07 seconds.
-- Epoch 6
-- Epoch 6
Norm: 68.92, NNZs: 41761, Bias: -0.560000, T: 100000, Avg. loss: 0.002747
Total training time: 0.07 seconds.
-- Epoch 1
-- Epoch 6
Norm: 72.15, NNZs: 42213, Bias: -0.360000, T: 100000, Avg. loss: 0.003448
Total training time: 0.07 seconds.
-- Epoch 6
-- Epoch 1
Norm: 68.84, NNZs: 39038, Bias: -0.320000, T: 100000, Avg. loss: 0.002763
Total training time: 0.08 seconds.
-- Epoch 6
Norm: 63.08, NNZs: 39127, Bias: -0.640000, T: 100000, Avg. loss: 0.002257
Total training time: 0.07 seconds.
-- Epoch 6
Norm: 61.74, NNZs: 37103, Bias: -0.640000, T: 100000, Avg. loss: 0.002034
Total training time: 0.08 seconds.
-- Epoch 6
Norm: 65.79, NNZs: 37833, Bias: -0.610000, T: 120000, Avg. loss: 0.001235
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 63.95, NNZs: 35109, Bias: -0.550000, T: 120000, Avg. loss: 0.001109
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 65.76, NNZs: 39649, Bias: -0.530000, T: 120000, Avg. loss: 0.000940
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 70.77, NNZs: 42118, Bias: -0.610000, T: 120000, Avg. loss: 0.001722
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 74.29, NNZs: 42608, Bias: -0.380000, T: 120000, Avg. loss: 0.002234
Total training time: 0.08 seconds.
-- Epoch 7
Norm: 70.80, NNZs: 39417, Bias: -0.390000, T: 120000, Avg. loss: 0.001727
Total training time: 0.09 seconds.
-- Epoch 7
Norm: 64.64, NNZs: 39491, Bias: -0.670000, T: 120000, Avg. loss: 0.001200
Total training time: 0.09 seconds.
-- Epoch 7
Norm: 63.25, NNZs: 37410, Bias: -0.680000, T: 120000, Avg. loss: 0.001106
Total training time: 0.09 seconds.
-- Epoch 7
Norm: 66.75, NNZs: 39801, Bias: -0.560000, T: 140000, Avg. loss: 0.000609
Total training time: 0.10 seconds.
-- Epoch 8
Norm: 65.07, NNZs: 35315, Bias: -0.570000, T: 140000, Avg. loss: 0.000765
Total training time: 0.09 seconds.
Norm: 66.80, NNZs: 38033, Bias: -0.650000, T: 140000, Avg. loss: 0.000671
Total training time: 0.10 seconds.
-- Epoch 8
-- Epoch 8
Norm: 72.08, NNZs: 42315, Bias: -0.560000, T: 140000, Avg. loss: 0.000943
Total training time: 0.10 seconds.
-- Epoch 8
Norm: 75.78, NNZs: 42775, Bias: -0.400000, T: 140000, Avg. loss: 0.001126
Total training time: 0.10 seconds.
-- Epoch 8
Norm: 71.98, NNZs: 39645, Bias: -0.370000, T: 140000, Avg. loss: 0.000875
Total training time: 0.11 seconds.
-- Epoch 8
Norm: 65.66, NNZs: 39656, Bias: -0.710000, T: 140000, Avg. loss: 0.000529
Total training time: 0.10 seconds.
-- Epoch 8
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
Norm: 64.35, NNZs: 37576, Bias: -0.670000, T: 140000, Avg. loss: 0.000578
Total training time: 0.10 seconds.
-- Epoch 8
Norm: 67.47, NNZs: 39925, Bias: -0.540000, T: 160000, Avg. loss: 0.000305
Total training time: 0.11 seconds.
-- Epoch 9
Norm: 67.44, NNZs: 38135, Bias: -0.650000, T: 160000, Avg. loss: 0.000315
Total training time: 0.11 seconds.
-- Epoch 9
Norm: 65.74, NNZs: 35431, Bias: -0.540000, T: 160000, Avg. loss: 0.000340
Total training time: 0.11 seconds.
-- Epoch 9
Norm: 72.97, NNZs: 42492, Bias: -0.600000, T: 160000, Avg. loss: 0.000588
Total training time: 0.11 seconds.
-- Epoch 9
Norm: 76.91, NNZs: 42896, Bias: -0.470000, T: 160000, Avg. loss: 0.000793
Total training time: 0.11 seconds.
-- Epoch 9
Norm: 40.39, NNZs: 28623, Bias: -0.360000, T: 20000, Avg. loss: 0.036605
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 72.80, NNZs: 39750, Bias: -0.370000, T: 160000, Avg. loss: 0.000486
Total training time: 0.12 seconds.
-- Epoch 9
Norm: 66.42, NNZs: 39754, Bias: -0.680000, T: 160000, Avg. loss: 0.000379
Total training time: 0.12 seconds.
-- Epoch 9
Norm: 65.16, NNZs: 37725, Bias: -0.710000, T: 160000, Avg. loss: 0.000437
Total training time: 0.12 seconds.
-- Epoch 9
Norm: 68.14, NNZs: 40012, Bias: -0.530000, T: 180000, Avg. loss: 0.000264
Total training time: 0.13 seconds.
-- Epoch 10
Norm: 67.87, NNZs: 38259, Bias: -0.650000, T: 180000, Avg. loss: 0.000182
Total training time: 0.13 seconds.
-- Epoch 10
Norm: 73.58, NNZs: 42580, Bias: -0.610000, T: 180000, Avg. loss: 0.000377
Total training time: 0.13 seconds.
-- Epoch 10
Norm: 77.97, NNZs: 43006, Bias: -0.460000, T: 180000, Avg. loss: 0.000590
Total training time: 0.13 seconds.
-- Epoch 10
Norm: 66.34, NNZs: 35532, Bias: -0.570000, T: 180000, Avg. loss: 0.000232
Total training time: 0.13 seconds.
-- Epoch 10
Norm: 44.33, NNZs: 34652, Bias: -0.360000, T: 20000, Avg. loss: 0.045541
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 43.58, NNZs: 33308, Bias: -0.350000, T: 20000, Avg. loss: 0.039805
Total training time: 0.08 seconds.
-- Epoch 2
Norm: 73.48, NNZs: 39850, Bias: -0.350000, T: 180000, Avg. loss: 0.000335
Total training time: 0.14 seconds.
-- Epoch 10
Norm: 49.96, NNZs: 32060, Bias: -0.460000, T: 40000, Avg. loss: 0.016155
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 41.37, NNZs: 31082, Bias: -0.450000, T: 20000, Avg. loss: 0.038189
Total training time: 0.09 seconds.
Norm: 65.66, NNZs: 37801, Bias: -0.690000, T: 180000, Avg. loss: 0.000187
Total training time: 0.14 seconds.
-- Epoch 2
-- Epoch 10
Norm: 68.19, NNZs: 38295, Bias: -0.660000, T: 200000, Avg. loss: 0.000099
Total training time: 0.14 seconds.
Convergence after 10 epochs took 0.14 seconds
Norm: 40.29, NNZs: 30486, Bias: -0.420000, T: 20000, Avg. loss: 0.036636
Total training time: 0.07 seconds.
-- Epoch 2
Norm: 74.13, NNZs: 42641, Bias: -0.600000, T: 200000, Avg. loss: 0.000272
Total training time: 0.15 seconds.
Norm: 45.74, NNZs: 32216, Bias: -0.190000, T: 20000, Avg. loss: 0.046662
Total training time: 0.10 seconds.
-- Epoch 11
-- Epoch 2
Norm: 78.74, NNZs: 43083, Bias: -0.420000, T: 200000, Avg. loss: 0.000455
Total training time: 0.14 seconds.
-- Epoch 11
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
Norm: 67.04, NNZs: 35674, Bias: -0.580000, T: 200000, Avg. loss: 0.000302
Total training time: 0.15 seconds.
Norm: 41.39, NNZs: 31828, Bias: -0.400000, T: 20000, Avg. loss: 0.036713
Total training time: 0.09 seconds.
Convergence after 10 epochs took 0.15 seconds
-- Epoch 2
Norm: 47.35, NNZs: 34024, Bias: -0.150000, T: 20000, Avg. loss: 0.049360
Total training time: 0.08 seconds.
Norm: 74.04, NNZs: 39909, Bias: -0.400000, T: 200000, Avg. loss: 0.000285
Total training time: 0.15 seconds.
-- Epoch 2
-- Epoch 11
Norm: 52.81, NNZs: 36926, Bias: -0.430000, T: 40000, Avg. loss: 0.017441
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 68.48, NNZs: 40077, Bias: -0.570000, T: 200000, Avg. loss: 0.000144
Total training time: 0.16 seconds.
Norm: 66.02, NNZs: 37880, Bias: -0.710000, T: 200000, Avg. loss: 0.000134
Total training time: 0.15 seconds.
-- Epoch 11
Convergence after 10 epochs took 0.15 seconds
Norm: 66.78, NNZs: 39807, Bias: -0.670000, T: 180000, Avg. loss: 0.000136
Total training time: 0.16 seconds.
-- Epoch 10
Norm: 74.45, NNZs: 42661, Bias: -0.610000, T: 220000, Avg. loss: 0.000124
Total training time: 0.16 seconds.
Convergence after 11 epochs took 0.16 seconds
Norm: 51.34, NNZs: 34893, Bias: -0.500000, T: 40000, Avg. loss: 0.017287
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 56.24, NNZs: 36542, Bias: -0.280000, T: 40000, Avg. loss: 0.022098
Total training time: 0.12 seconds.
-- Epoch 3
Norm: 50.84, NNZs: 35966, Bias: -0.550000, T: 40000, Avg. loss: 0.016813
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 79.26, NNZs: 43140, Bias: -0.400000, T: 220000, Avg. loss: 0.000315
Total training time: 0.16 seconds.
-- Epoch 12
Norm: 56.14, NNZs: 33495, Bias: -0.490000, T: 60000, Avg. loss: 0.007967
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 55.18, NNZs: 38956, Bias: -0.500000, T: 40000, Avg. loss: 0.022202
Total training time: 0.10 seconds.
Norm: 58.78, NNZs: 38399, Bias: -0.540000, T: 60000, Avg. loss: 0.008352
Total training time: 0.11 seconds.
-- Epoch 3
-- Epoch 4
Norm: 74.61, NNZs: 39986, Bias: -0.360000, T: 220000, Avg. loss: 0.000212
Total training time: 0.17 seconds.
Convergence after 11 epochs took 0.17 seconds
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.2s remaining:    0.3s
Norm: 67.13, NNZs: 39871, Bias: -0.700000, T: 200000, Avg. loss: 0.000107
Total training time: 0.17 seconds.
-- Epoch 11
Norm: 57.25, NNZs: 36278, Bias: -0.550000, T: 60000, Avg. loss: 0.008061
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 50.06, NNZs: 34445, Bias: -0.480000, T: 40000, Avg. loss: 0.017002
Total training time: 0.11 seconds.
Norm: 62.30, NNZs: 38076, Bias: -0.270000, T: 60000, Avg. loss: 0.010453
Total training time: 0.13 seconds.
-- Epoch 3
-- Epoch 4
Norm: 58.29, NNZs: 38770, Bias: -0.260000, T: 40000, Avg. loss: 0.025720
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 68.81, NNZs: 40141, Bias: -0.580000, T: 220000, Avg. loss: 0.000110
Total training time: 0.18 seconds.
Norm: 56.61, NNZs: 37641, Bias: -0.560000, T: 60000, Avg. loss: 0.008122
Total training time: 0.12 seconds.
Convergence after 11 epochs took 0.18 seconds
-- Epoch 4
Norm: 62.52, NNZs: 39194, Bias: -0.550000, T: 80000, Avg. loss: 0.003975
Total training time: 0.13 seconds.
Norm: 60.02, NNZs: 34419, Bias: -0.520000, T: 80000, Avg. loss: 0.004239
Total training time: 0.13 seconds.
-- Epoch 5
-- Epoch 5
Norm: 79.70, NNZs: 43204, Bias: -0.410000, T: 240000, Avg. loss: 0.000237
Total training time: 0.18 seconds.
Convergence after 12 epochs took 0.18 seconds
Norm: 61.64, NNZs: 40698, Bias: -0.570000, T: 60000, Avg. loss: 0.010542
Total training time: 0.12 seconds.
-- Epoch 4
-- Epoch 1
Norm: 61.05, NNZs: 37189, Bias: -0.570000, T: 80000, Avg. loss: 0.004045
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 67.59, NNZs: 39980, Bias: -0.740000, T: 220000, Avg. loss: 0.000186
Total training time: 0.19 seconds.
Convergence after 11 epochs took 0.19 seconds
-- Epoch 1
Norm: 66.17, NNZs: 38961, Bias: -0.300000, T: 80000, Avg. loss: 0.005513
Total training time: 0.15 seconds.
-- Epoch 1
-- Epoch 5
-- Epoch 1
Norm: 64.76, NNZs: 40576, Bias: -0.350000, T: 60000, Avg. loss: 0.012504
Total training time: 0.12 seconds.
Norm: 55.80, NNZs: 35806, Bias: -0.610000, T: 60000, Avg. loss: 0.007463
Total training time: 0.12 seconds.
-- Epoch 4
-- Epoch 4
-- Epoch 1
Norm: 59.96, NNZs: 38414, Bias: -0.630000, T: 80000, Avg. loss: 0.003192
Total training time: 0.13 seconds.
-- Epoch 5
Norm: 65.03, NNZs: 39697, Bias: -0.480000, T: 100000, Avg. loss: 0.002045
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 62.47, NNZs: 34936, Bias: -0.520000, T: 100000, Avg. loss: 0.002349
Total training time: 0.15 seconds.
Norm: 65.86, NNZs: 41642, Bias: -0.540000, T: 80000, Avg. loss: 0.005159
Total training time: 0.13 seconds.
-- Epoch 6
-- Epoch 5
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 63.61, NNZs: 37718, Bias: -0.600000, T: 100000, Avg. loss: 0.002363
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 40.61, NNZs: 30845, Bias: -0.370000, T: 20000, Avg. loss: 0.036178
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.2s finished
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 66.52, NNZs: 40003, Bias: -0.540000, T: 120000, Avg. loss: 0.001157
Total training time: 0.15 seconds.
-- Epoch 7
Norm: 68.86, NNZs: 39491, Bias: -0.300000, T: 100000, Avg. loss: 0.002789
Total training time: 0.16 seconds.
-- Epoch 6
Norm: 69.03, NNZs: 41436, Bias: -0.380000, T: 80000, Avg. loss: 0.005771
Total training time: 0.14 seconds.
-- Epoch 5
Norm: 62.22, NNZs: 38933, Bias: -0.650000, T: 100000, Avg. loss: 0.001875
Total training time: 0.15 seconds.
Norm: 68.49, NNZs: 42064, Bias: -0.560000, T: 100000, Avg. loss: 0.002936
Total training time: 0.15 seconds.
-- Epoch 6
-- Epoch 6
Norm: 65.20, NNZs: 38109, Bias: -0.610000, T: 120000, Avg. loss: 0.001243
Total training time: 0.17 seconds.
-- Epoch 7
Norm: 67.47, NNZs: 40182, Bias: -0.570000, T: 140000, Avg. loss: 0.000537
Total training time: 0.17 seconds.
-- Epoch 8
Norm: 70.73, NNZs: 39863, Bias: -0.330000, T: 120000, Avg. loss: 0.001652
Total training time: 0.18 seconds.
-- Epoch 7
Norm: 64.19, NNZs: 35212, Bias: -0.550000, T: 120000, Avg. loss: 0.001260
Total training time: 0.18 seconds.
-- Epoch 7
Norm: 72.12, NNZs: 41949, Bias: -0.360000, T: 100000, Avg. loss: 0.003573
Total training time: 0.15 seconds.
-- Epoch 6
Norm: 41.48, NNZs: 30936, Bias: -0.360000, T: 20000, Avg. loss: 0.037916
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 63.62, NNZs: 39195, Bias: -0.640000, T: 120000, Avg. loss: 0.000916
Total training time: 0.17 seconds.
Norm: 70.35, NNZs: 42458, Bias: -0.610000, T: 120000, Avg. loss: 0.001556
Total training time: 0.16 seconds.
-- Epoch 7
-- Epoch 7
Norm: 49.98, NNZs: 34237, Bias: -0.520000, T: 40000, Avg. loss: 0.016588
Total training time: 0.04 seconds.
Norm: 46.00, NNZs: 32135, Bias: -0.200000, T: 20000, Avg. loss: 0.045825
Total training time: 0.03 seconds.
Norm: 47.15, NNZs: 34110, Bias: -0.230000, T: 20000, Avg. loss: 0.049558
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 2
Norm: 59.42, NNZs: 36581, Bias: -0.620000, T: 80000, Avg. loss: 0.003939
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 41.25, NNZs: 32389, Bias: -0.370000, T: 20000, Avg. loss: 0.037244
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 40.48, NNZs: 29050, Bias: -0.350000, T: 20000, Avg. loss: 0.036522
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 1
Norm: 68.17, NNZs: 40275, Bias: -0.580000, T: 160000, Avg. loss: 0.000292
Total training time: 0.18 seconds.
-- Epoch 9
Norm: 72.13, NNZs: 40012, Bias: -0.320000, T: 140000, Avg. loss: 0.001159
Total training time: 0.19 seconds.
-- Epoch 8
Norm: 65.25, NNZs: 35386, Bias: -0.550000, T: 140000, Avg. loss: 0.000753
Total training time: 0.19 seconds.
Norm: 64.85, NNZs: 39458, Bias: -0.690000, T: 140000, Avg. loss: 0.000784
Total training time: 0.18 seconds.
-- Epoch 8
-- Epoch 8
Norm: 71.78, NNZs: 42712, Bias: -0.640000, T: 140000, Avg. loss: 0.001051
Total training time: 0.18 seconds.
-- Epoch 8
Norm: 43.64, NNZs: 33332, Bias: -0.290000, T: 20000, Avg. loss: 0.039932
Total training time: 0.05 seconds.
-- Epoch 2
-- Epoch 1
Norm: 66.30, NNZs: 38275, Bias: -0.620000, T: 140000, Avg. loss: 0.000776
Total training time: 0.20 seconds.
-- Epoch 8
Norm: 44.48, NNZs: 34299, Bias: -0.350000, T: 20000, Avg. loss: 0.046050
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 68.75, NNZs: 40380, Bias: -0.580000, T: 180000, Avg. loss: 0.000338
Total training time: 0.20 seconds.
-- Epoch 10
Norm: 73.01, NNZs: 40177, Bias: -0.330000, T: 160000, Avg. loss: 0.000591
Total training time: 0.21 seconds.
Norm: 65.74, NNZs: 39606, Bias: -0.700000, T: 160000, Avg. loss: 0.000420
Total training time: 0.19 seconds.
-- Epoch 9
-- Epoch 1
-- Epoch 1
-- Epoch 9
-- Epoch 1
Norm: 65.90, NNZs: 35511, Bias: -0.590000, T: 160000, Avg. loss: 0.000340
Total training time: 0.21 seconds.
-- Epoch 1
-- Epoch 9
Norm: 72.86, NNZs: 42843, Bias: -0.640000, T: 160000, Avg. loss: 0.000702
Total training time: 0.19 seconds.
-- Epoch 9
-- Epoch 1
Norm: 56.21, NNZs: 36767, Bias: -0.310000, T: 40000, Avg. loss: 0.022388
Total training time: 0.06 seconds.
Norm: 67.09, NNZs: 38398, Bias: -0.630000, T: 160000, Avg. loss: 0.000473
Total training time: 0.21 seconds.
-- Epoch 9
Norm: 50.21, NNZs: 32599, Bias: -0.460000, T: 40000, Avg. loss: 0.016774
Total training time: 0.07 seconds.
-- Epoch 3
-- Epoch 3
Norm: 66.35, NNZs: 39703, Bias: -0.710000, T: 180000, Avg. loss: 0.000250
Total training time: 0.21 seconds.
-- Epoch 10
Norm: 69.24, NNZs: 40441, Bias: -0.590000, T: 200000, Avg. loss: 0.000239
Total training time: 0.21 seconds.
Convergence after 10 epochs took 0.21 seconds
Norm: 73.76, NNZs: 40237, Bias: -0.360000, T: 180000, Avg. loss: 0.000398
Total training time: 0.22 seconds.
-- Epoch 1
-- Epoch 10
Norm: 66.52, NNZs: 35612, Bias: -0.580000, T: 180000, Avg. loss: 0.000256
Total training time: 0.22 seconds.
-- Epoch 10
Norm: 43.08, NNZs: 32624, Bias: -0.350000, T: 20000, Avg. loss: 0.039719
Total training time: 0.03 seconds.
Norm: 55.50, NNZs: 38790, Bias: -0.490000, T: 40000, Avg. loss: 0.022311
Total training time: 0.07 seconds.
-- Epoch 2
-- Epoch 3
Norm: 52.84, NNZs: 37241, Bias: -0.390000, T: 40000, Avg. loss: 0.016969
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 55.89, NNZs: 34049, Bias: -0.520000, T: 60000, Avg. loss: 0.008134
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 41.43, NNZs: 32382, Bias: -0.400000, T: 20000, Avg. loss: 0.037280
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 47.42, NNZs: 34532, Bias: -0.170000, T: 20000, Avg. loss: 0.051146
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 74.05, NNZs: 42296, Bias: -0.400000, T: 120000, Avg. loss: 0.001938
Total training time: 0.22 seconds.
Norm: 61.94, NNZs: 40513, Bias: -0.480000, T: 60000, Avg. loss: 0.010802
-- Epoch 7
Total training time: 0.09 seconds.
-- Epoch 4
Norm: 44.58, NNZs: 35094, Bias: -0.370000, T: 20000, Avg. loss: 0.046919
Total training time: 0.04 seconds.
-- Epoch 2
Norm: 73.62, NNZs: 42918, Bias: -0.640000, T: 180000, Avg. loss: 0.000524
Total training time: 0.23 seconds.
Norm: 51.27, NNZs: 34750, Bias: -0.470000, T: 40000, Avg. loss: 0.017436
Total training time: 0.10 seconds.
Norm: 66.88, NNZs: 39784, Bias: -0.690000, T: 200000, Avg. loss: 0.000195
Total training time: 0.23 seconds.
Norm: 66.94, NNZs: 35654, Bias: -0.580000, T: 200000, Avg. loss: 0.000183
Total training time: 0.24 seconds.
-- Epoch 10
Convergence after 10 epochs took 0.23 seconds
-- Epoch 3
-- Epoch 11
Norm: 40.42, NNZs: 30818, Bias: -0.390000, T: 20000, Avg. loss: 0.036111
Total training time: 0.04 seconds.
-- Epoch 2
-- Epoch 1
-- Epoch 1
Norm: 55.53, NNZs: 35675, Bias: -0.560000, T: 60000, Avg. loss: 0.007074
Total training time: 0.11 seconds.
Norm: 41.78, NNZs: 31217, Bias: -0.380000, T: 20000, Avg. loss: 0.038520
Total training time: 0.04 seconds.
-- Epoch 4
-- Epoch 2
Norm: 67.65, NNZs: 38465, Bias: -0.640000, T: 180000, Avg. loss: 0.000326
Total training time: 0.24 seconds.
-- Epoch 10
Norm: 59.34, NNZs: 34761, Bias: -0.530000, T: 80000, Avg. loss: 0.003832
Total training time: 0.11 seconds.
-- Epoch 5
Norm: 58.07, NNZs: 38803, Bias: -0.310000, T: 40000, Avg. loss: 0.026207
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 61.77, NNZs: 37076, Bias: -0.580000, T: 100000, Avg. loss: 0.002005
Total training time: 0.24 seconds.
-- Epoch 6
Norm: 50.70, NNZs: 35926, Bias: -0.540000, T: 40000, Avg. loss: 0.016090
Total training time: 0.11 seconds.
-- Epoch 3
-- Epoch 1
-- Epoch 1
Norm: 40.67, NNZs: 28513, Bias: -0.370000, T: 20000, Avg. loss: 0.035943
Total training time: 0.06 seconds.
-- Epoch 2
Norm: 58.73, NNZs: 38868, Bias: -0.510000, T: 60000, Avg. loss: 0.007643
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 74.33, NNZs: 40362, Bias: -0.350000, T: 200000, Avg. loss: 0.000310
Total training time: 0.27 seconds.
-- Epoch 11
-- Epoch 1
Norm: 62.08, NNZs: 38283, Bias: -0.290000, T: 60000, Avg. loss: 0.010290
Total training time: 0.12 seconds.
Norm: 45.62, NNZs: 31831, Bias: -0.210000, T: 20000, Avg. loss: 0.046454
Total training time: 0.08 seconds.
-- Epoch 4
Norm: 55.63, NNZs: 39562, Bias: -0.510000, T: 40000, Avg. loss: 0.022537
Total training time: 0.06 seconds.
-- Epoch 2
-- Epoch 3
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 75.67, NNZs: 42555, Bias: -0.420000, T: 140000, Avg. loss: 0.001397
Total training time: 0.25 seconds.
-- Epoch 8
Norm: 66.00, NNZs: 41307, Bias: -0.530000, T: 80000, Avg. loss: 0.004940
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 45.52, NNZs: 32020, Bias: -0.230000, T: 20000, Avg. loss: 0.045656
Total training time: 0.03 seconds.
-- Epoch 2
Norm: 67.28, NNZs: 35709, Bias: -0.570000, T: 220000, Avg. loss: 0.000098
Total training time: 0.28 seconds.
Norm: 52.45, NNZs: 36511, Bias: -0.420000, T: 40000, Avg. loss: 0.017173
Total training time: 0.09 seconds.
Convergence after 11 epochs took 0.28 seconds
-- Epoch 3
Norm: 51.47, NNZs: 34937, Bias: -0.470000, T: 40000, Avg. loss: 0.016471
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 56.95, NNZs: 36092, Bias: -0.540000, T: 60000, Avg. loss: 0.007922
Total training time: 0.15 seconds.
-- Epoch 4
Norm: 57.77, NNZs: 39034, Bias: -0.270000, T: 40000, Avg. loss: 0.025806
Total training time: 0.07 seconds.
Norm: 56.25, NNZs: 37478, Bias: -0.620000, T: 60000, Avg. loss: 0.007604
Total training time: 0.14 seconds.
-- Epoch 3
-- Epoch 4
Norm: 47.71, NNZs: 34029, Bias: -0.150000, T: 20000, Avg. loss: 0.049817
Total training time: 0.02 seconds.
-- Epoch 2
Norm: 62.37, NNZs: 41342, Bias: -0.550000, T: 60000, Avg. loss: 0.011092
Total training time: 0.09 seconds.
Norm: 51.13, NNZs: 36343, Bias: -0.560000, T: 40000, Avg. loss: 0.017270
Total training time: 0.09 seconds.
Norm: 41.57, NNZs: 32205, Bias: -0.450000, T: 20000, Avg. loss: 0.036452
Total training time: 0.04 seconds.
-- Epoch 4
-- Epoch 3
Norm: 55.85, NNZs: 36554, Bias: -0.360000, T: 40000, Avg. loss: 0.022191
Total training time: 0.04 seconds.
Norm: 44.66, NNZs: 34553, Bias: -0.390000, T: 20000, Avg. loss: 0.046648
Total training time: 0.05 seconds.
-- Epoch 3
-- Epoch 2
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.3s remaining:    0.5s
Norm: 43.24, NNZs: 33031, Bias: -0.320000, T: 20000, Avg. loss: 0.039761
Total training time: 0.03 seconds.
Norm: 74.18, NNZs: 42998, Bias: -0.660000, T: 200000, Avg. loss: 0.000260
Total training time: 0.28 seconds.
-- Epoch 11
Norm: 59.29, NNZs: 36530, Bias: -0.560000, T: 80000, Avg. loss: 0.003860
Total training time: 0.16 seconds.
-- Epoch 5
-- Epoch 2
Norm: 40.65, NNZs: 28789, Bias: -0.360000, T: 20000, Avg. loss: 0.036286
Total training time: 0.04 seconds.
Norm: 63.07, NNZs: 37297, Bias: -0.610000, T: 120000, Avg. loss: 0.000871
Total training time: 0.29 seconds.
-- Epoch 7
Norm: 74.96, NNZs: 40453, Bias: -0.360000, T: 220000, Avg. loss: 0.000296
Total training time: 0.31 seconds.
Convergence after 11 epochs took 0.31 seconds
Norm: 66.47, NNZs: 42097, Bias: -0.600000, T: 80000, Avg. loss: 0.005237
Total training time: 0.10 seconds.
-- Epoch 5
Norm: 59.95, NNZs: 38366, Bias: -0.610000, T: 80000, Avg. loss: 0.003635
Total training time: 0.16 seconds.
-- Epoch 5
Norm: 40.41, NNZs: 30408, Bias: -0.420000, T: 20000, Avg. loss: 0.035542
Total training time: 0.05 seconds.
-- Epoch 2
Norm: 62.41, NNZs: 39614, Bias: -0.550000, T: 80000, Avg. loss: 0.003962
Total training time: 0.17 seconds.
-- Epoch 5
Norm: 76.63, NNZs: 42678, Bias: -0.420000, T: 160000, Avg. loss: 0.000745
Total training time: 0.29 seconds.
-- Epoch 9
Norm: 57.09, NNZs: 36396, Bias: -0.560000, T: 60000, Avg. loss: 0.007874
Total training time: 0.10 seconds.
-- Epoch 4
Norm: 41.85, NNZs: 31183, Bias: -0.410000, T: 20000, Avg. loss: 0.039135
Total training time: 0.05 seconds.
Norm: 56.32, NNZs: 36251, Bias: -0.310000, T: 40000, Avg. loss: 0.021755
Total training time: 0.13 seconds.
-- Epoch 2
-- Epoch 3
Norm: 64.41, NNZs: 40312, Bias: -0.340000, T: 60000, Avg. loss: 0.011732
Total training time: 0.17 seconds.
Norm: 49.95, NNZs: 34496, Bias: -0.470000, T: 40000, Avg. loss: 0.016731
Total training time: 0.11 seconds.
-- Epoch 4
-- Epoch 3
-- Epoch 2
Norm: 68.79, NNZs: 41784, Bias: -0.540000, T: 100000, Avg. loss: 0.002638
Total training time: 0.17 seconds.
Norm: 61.72, NNZs: 35171, Bias: -0.550000, T: 100000, Avg. loss: 0.002105
Total training time: 0.18 seconds.
-- Epoch 6
Norm: 66.32, NNZs: 39131, Bias: -0.310000, T: 80000, Avg. loss: 0.005334
Total training time: 0.17 seconds.
-- Epoch 5
Norm: 62.13, NNZs: 38231, Bias: -0.320000, T: 60000, Avg. loss: 0.009855
Total training time: 0.07 seconds.
-- Epoch 4
Norm: 68.24, NNZs: 38558, Bias: -0.650000, T: 200000, Avg. loss: 0.000316
Total training time: 0.32 seconds.
-- Epoch 11
Norm: 50.53, NNZs: 32195, Bias: -0.400000, T: 40000, Avg. loss: 0.016996
Total training time: 0.12 seconds.
-- Epoch 3
-- Epoch 6
Norm: 60.60, NNZs: 36829, Bias: -0.580000, T: 80000, Avg. loss: 0.003641
Total training time: 0.18 seconds.
-- Epoch 5
-- Epoch 2
Norm: 57.04, NNZs: 37798, Bias: -0.600000, T: 60000, Avg. loss: 0.007540
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 51.39, NNZs: 36006, Bias: -0.520000, T: 40000, Avg. loss: 0.017552
Total training time: 0.08 seconds.
-- Epoch 3
Norm: 77.38, NNZs: 42758, Bias: -0.410000, T: 180000, Avg. loss: 0.000455
Total training time: 0.31 seconds.
-- Epoch 10
Norm: 58.36, NNZs: 38062, Bias: -0.510000, T: 60000, Avg. loss: 0.008543
Total training time: 0.14 seconds.
-- Epoch 4
Norm: 60.70, NNZs: 37200, Bias: -0.520000, T: 80000, Avg. loss: 0.003874
Total training time: 0.12 seconds.
-- Epoch 5
Norm: 54.82, NNZs: 38627, Bias: -0.490000, T: 40000, Avg. loss: 0.021007
Total training time: 0.09 seconds.
-- Epoch 3
Norm: 69.21, NNZs: 42614, Bias: -0.590000, T: 100000, Avg. loss: 0.002644
Total training time: 0.14 seconds.
-- Epoch 6
Norm: 66.20, NNZs: 39195, Bias: -0.340000, T: 80000, Avg. loss: 0.005344
Total training time: 0.09 seconds.
-- Epoch 5
Norm: 62.51, NNZs: 37968, Bias: -0.330000, T: 60000, Avg. loss: 0.010012
Total training time: 0.15 seconds.
-- Epoch 4
Norm: 77.90, NNZs: 42848, Bias: -0.410000, T: 200000, Avg. loss: 0.000281
Total training time: 0.33 seconds.
-- Epoch 11
Norm: 63.15, NNZs: 37694, Bias: -0.590000, T: 100000, Avg. loss: 0.002436
Total training time: 0.14 seconds.
Norm: 64.78, NNZs: 40051, Bias: -0.550000, T: 100000, Avg. loss: 0.002094
Total training time: 0.21 seconds.
-- Epoch 6
-- Epoch 6
Norm: 50.00, NNZs: 32271, Bias: -0.420000, T: 40000, Avg. loss: 0.014801
Total training time: 0.09 seconds.
Norm: 74.64, NNZs: 43071, Bias: -0.650000, T: 220000, Avg. loss: 0.000193
Total training time: 0.34 seconds.
-- Epoch 3
Convergence after 11 epochs took 0.34 seconds
Norm: 56.22, NNZs: 33648, Bias: -0.500000, T: 60000, Avg. loss: 0.007504
Total training time: 0.15 seconds.
Norm: 64.78, NNZs: 40851, Bias: -0.330000, T: 60000, Avg. loss: 0.012476
Total training time: 0.14 seconds.
Norm: 68.70, NNZs: 39705, Bias: -0.390000, T: 100000, Avg. loss: 0.002677
Total training time: 0.10 seconds.
-- Epoch 4
-- Epoch 6
Norm: 61.82, NNZs: 37036, Bias: -0.600000, T: 100000, Avg. loss: 0.002377
Total training time: 0.22 seconds.
-- Epoch 6
Norm: 62.63, NNZs: 38929, Bias: -0.590000, T: 100000, Avg. loss: 0.002299
Total training time: 0.21 seconds.
-- Epoch 6
Norm: 58.29, NNZs: 38635, Bias: -0.230000, T: 40000, Avg. loss: 0.025722
Total training time: 0.10 seconds.
-- Epoch 3
Norm: 68.60, NNZs: 38591, Bias: -0.660000, T: 220000, Avg. loss: 0.000184
Total training time: 0.36 seconds.
Convergence after 11 epochs took 0.36 seconds
Norm: 68.72, NNZs: 41183, Bias: -0.300000, T: 80000, Avg. loss: 0.006332
Total training time: 0.22 seconds.
-- Epoch 5
Norm: 63.19, NNZs: 35443, Bias: -0.640000, T: 120000, Avg. loss: 0.001207
Total training time: 0.23 seconds.
-- Epoch 7
Norm: 57.39, NNZs: 37627, Bias: -0.600000, T: 60000, Avg. loss: 0.008289
Total training time: 0.12 seconds.
Norm: 61.96, NNZs: 38872, Bias: -0.520000, T: 80000, Avg. loss: 0.003898
Total training time: 0.18 seconds.
-- Epoch 4
-- Epoch 5
Norm: 70.97, NNZs: 42846, Bias: -0.590000, T: 120000, Avg. loss: 0.001491
Total training time: 0.17 seconds.
Norm: 63.09, NNZs: 37326, Bias: -0.610000, T: 100000, Avg. loss: 0.001910
Total training time: 0.23 seconds.
-- Epoch 7
-- Epoch 6
Norm: 70.36, NNZs: 39928, Bias: -0.330000, T: 120000, Avg. loss: 0.001370
Total training time: 0.12 seconds.
Norm: 49.68, NNZs: 33935, Bias: -0.500000, T: 40000, Avg. loss: 0.016451
Total training time: 0.11 seconds.
-- Epoch 7
Norm: 66.35, NNZs: 40287, Bias: -0.590000, T: 120000, Avg. loss: 0.001102
Total training time: 0.23 seconds.
-- Epoch 3
-- Epoch 7
Norm: 70.65, NNZs: 42099, Bias: -0.590000, T: 120000, Avg. loss: 0.001582
Total training time: 0.22 seconds.
-- Epoch 7
Norm: 66.59, NNZs: 38993, Bias: -0.340000, T: 80000, Avg. loss: 0.005024
Total training time: 0.19 seconds.
-- Epoch 5
Norm: 64.69, NNZs: 37979, Bias: -0.630000, T: 120000, Avg. loss: 0.001174
Total training time: 0.17 seconds.
Norm: 51.70, NNZs: 34726, Bias: -0.460000, T: 40000, Avg. loss: 0.017036
Total training time: 0.11 seconds.
-- Epoch 7
-- Epoch 3
Norm: 69.02, NNZs: 41658, Bias: -0.320000, T: 80000, Avg. loss: 0.006185
Total training time: 0.16 seconds.
Norm: 55.44, NNZs: 35952, Bias: -0.550000, T: 60000, Avg. loss: 0.006945
Total training time: 0.17 seconds.
-- Epoch 4
-- Epoch 5
-- Epoch 4
Norm: 52.79, NNZs: 36681, Bias: -0.470000, T: 40000, Avg. loss: 0.017565
Total training time: 0.11 seconds.
-- Epoch 3
Norm: 60.66, NNZs: 38588, Bias: -0.630000, T: 80000, Avg. loss: 0.003667
Total training time: 0.18 seconds.
-- Epoch 5
Norm: 55.89, NNZs: 33854, Bias: -0.530000, T: 60000, Avg. loss: 0.008162
Total training time: 0.12 seconds.
-- Epoch 4
Norm: 78.18, NNZs: 42874, Bias: -0.420000, T: 220000, Avg. loss: 0.000150
Total training time: 0.36 seconds.
Convergence after 11 epochs took 0.36 seconds
Norm: 71.52, NNZs: 40124, Bias: -0.330000, T: 140000, Avg. loss: 0.000919
Total training time: 0.14 seconds.
-- Epoch 8
Norm: 68.87, NNZs: 39586, Bias: -0.350000, T: 100000, Avg. loss: 0.002895
Total training time: 0.24 seconds.
Norm: 61.18, NNZs: 40262, Bias: -0.540000, T: 60000, Avg. loss: 0.010090
Total training time: 0.14 seconds.
-- Epoch 6
-- Epoch 4
Norm: 71.95, NNZs: 42201, Bias: -0.310000, T: 100000, Avg. loss: 0.003471
Total training time: 0.18 seconds.
-- Epoch 6
Norm: 64.12, NNZs: 37457, Bias: -0.640000, T: 140000, Avg. loss: 0.000668
Total training time: 0.38 seconds.
-- Epoch 8
Norm: 71.96, NNZs: 42274, Bias: -0.560000, T: 140000, Avg. loss: 0.000906
Total training time: 0.25 seconds.
-- Epoch 8
Norm: 64.03, NNZs: 39189, Bias: -0.660000, T: 120000, Avg. loss: 0.001042
Total training time: 0.25 seconds.
-- Epoch 7
Norm: 63.53, NNZs: 37293, Bias: -0.610000, T: 120000, Avg. loss: 0.001187
Total training time: 0.27 seconds.
-- Epoch 7
Norm: 64.88, NNZs: 40235, Bias: -0.310000, T: 60000, Avg. loss: 0.011715
Total training time: 0.13 seconds.
-- Epoch 4
Norm: 59.36, NNZs: 34648, Bias: -0.510000, T: 80000, Avg. loss: 0.003498
Total training time: 0.14 seconds.
Norm: 72.25, NNZs: 43028, Bias: -0.610000, T: 140000, Avg. loss: 0.001038
Total training time: 0.20 seconds.
-- Epoch 5
-- Epoch 8
Norm: 69.07, NNZs: 39402, Bias: -0.380000, T: 100000, Avg. loss: 0.002696
Total training time: 0.22 seconds.
-- Epoch 6
Norm: 72.34, NNZs: 40224, Bias: -0.380000, T: 160000, Avg. loss: 0.000433
Total training time: 0.15 seconds.
-- Epoch 9
Norm: 74.09, NNZs: 42474, Bias: -0.370000, T: 120000, Avg. loss: 0.002029
Total training time: 0.19 seconds.
Norm: 64.66, NNZs: 37587, Bias: -0.630000, T: 120000, Avg. loss: 0.001369
Total training time: 0.27 seconds.
Norm: 65.94, NNZs: 38139, Bias: -0.640000, T: 140000, Avg. loss: 0.000727
Total training time: 0.20 seconds.
-- Epoch 7
-- Epoch 7
-- Epoch 8
Norm: 61.10, NNZs: 38425, Bias: -0.630000, T: 80000, Avg. loss: 0.004013
Total training time: 0.17 seconds.
-- Epoch 5
Norm: 59.87, NNZs: 34432, Bias: -0.560000, T: 80000, Avg. loss: 0.004087
Total training time: 0.21 seconds.
-- Epoch 5
Norm: 64.95, NNZs: 37601, Bias: -0.690000, T: 160000, Avg. loss: 0.000486
Total training time: 0.40 seconds.
-- Epoch 9
Norm: 55.61, NNZs: 35374, Bias: -0.560000, T: 60000, Avg. loss: 0.007866
Total training time: 0.16 seconds.
Norm: 71.46, NNZs: 41691, Bias: -0.400000, T: 100000, Avg. loss: 0.003124
Total training time: 0.28 seconds.
-- Epoch 4
-- Epoch 6
Norm: 73.19, NNZs: 40340, Bias: -0.410000, T: 180000, Avg. loss: 0.000405
Total training time: 0.17 seconds.
Norm: 70.97, NNZs: 39680, Bias: -0.360000, T: 120000, Avg. loss: 0.001649
Total training time: 0.24 seconds.
-- Epoch 10
-- Epoch 7
Norm: 57.91, NNZs: 36339, Bias: -0.540000, T: 60000, Avg. loss: 0.007840
Total training time: 0.16 seconds.
Norm: 63.04, NNZs: 39022, Bias: -0.660000, T: 100000, Avg. loss: 0.001957
Total training time: 0.22 seconds.
Norm: 64.95, NNZs: 39358, Bias: -0.680000, T: 140000, Avg. loss: 0.000509
Total training time: 0.28 seconds.
-- Epoch 4
-- Epoch 6
Norm: 64.53, NNZs: 35771, Bias: -0.570000, T: 140000, Avg. loss: 0.000800
Total training time: 0.29 seconds.
-- Epoch 8
Norm: 65.59, NNZs: 41150, Bias: -0.570000, T: 80000, Avg. loss: 0.005806
Total training time: 0.18 seconds.
-- Epoch 5
Norm: 58.50, NNZs: 38346, Bias: -0.540000, T: 60000, Avg. loss: 0.008383
Total training time: 0.16 seconds.
-- Epoch 4
Norm: 63.19, NNZs: 38880, Bias: -0.690000, T: 100000, Avg. loss: 0.001688
Total training time: 0.19 seconds.
-- Epoch 6
Norm: 75.63, NNZs: 42641, Bias: -0.360000, T: 140000, Avg. loss: 0.001280
Total training time: 0.21 seconds.
Norm: 64.43, NNZs: 39288, Bias: -0.520000, T: 100000, Avg. loss: 0.002000
Total training time: 0.24 seconds.
-- Epoch 6
-- Epoch 8
Norm: 73.22, NNZs: 43137, Bias: -0.620000, T: 160000, Avg. loss: 0.000658
Total training time: 0.23 seconds.
-- Epoch 9
Norm: 65.63, NNZs: 37709, Bias: -0.660000, T: 180000, Avg. loss: 0.000317
Total training time: 0.42 seconds.
-- Epoch 10
Norm: 72.50, NNZs: 39933, Bias: -0.320000, T: 140000, Avg. loss: 0.001187
Total training time: 0.25 seconds.
-- Epoch 8
Norm: 66.68, NNZs: 38252, Bias: -0.620000, T: 160000, Avg. loss: 0.000425
Total training time: 0.23 seconds.
-- Epoch 9
Norm: 72.82, NNZs: 42401, Bias: -0.570000, T: 160000, Avg. loss: 0.000539
Total training time: 0.29 seconds.
-- Epoch 9
Norm: 59.15, NNZs: 36713, Bias: -0.580000, T: 80000, Avg. loss: 0.003915
Total training time: 0.24 seconds.
-- Epoch 5
-- Epoch 8
Norm: 61.58, NNZs: 35173, Bias: -0.570000, T: 100000, Avg. loss: 0.001966
Total training time: 0.18 seconds.
Norm: 70.65, NNZs: 39924, Bias: -0.340000, T: 120000, Avg. loss: 0.001598
Total training time: 0.30 seconds.
-- Epoch 7
Norm: 64.69, NNZs: 39148, Bias: -0.720000, T: 120000, Avg. loss: 0.001165
Total training time: 0.20 seconds.
-- Epoch 7
Norm: 76.62, NNZs: 42807, Bias: -0.290000, T: 160000, Avg. loss: 0.000642
Total training time: 0.23 seconds.
-- Epoch 9
Norm: 73.82, NNZs: 40450, Bias: -0.370000, T: 200000, Avg. loss: 0.000320
Total training time: 0.20 seconds.
-- Epoch 11
Norm: 65.79, NNZs: 37805, Bias: -0.660000, T: 140000, Avg. loss: 0.000695
Total training time: 0.31 seconds.
-- Epoch 8
Norm: 67.36, NNZs: 40408, Bias: -0.610000, T: 140000, Avg. loss: 0.000658
Total training time: 0.31 seconds.
-- Epoch 8
-- Epoch 6
Norm: 66.21, NNZs: 37794, Bias: -0.660000, T: 200000, Avg. loss: 0.000212
Total training time: 0.44 seconds.
-- Epoch 11
Norm: 69.24, NNZs: 41166, Bias: -0.340000, T: 80000, Avg. loss: 0.006228
Total training time: 0.19 seconds.
Norm: 73.40, NNZs: 40057, Bias: -0.360000, T: 160000, Avg. loss: 0.000577
Total training time: 0.27 seconds.
-- Epoch 5
-- Epoch 9
Norm: 65.76, NNZs: 39306, Bias: -0.700000, T: 140000, Avg. loss: 0.000565
Total training time: 0.22 seconds.
-- Epoch 8
Norm: 62.25, NNZs: 34829, Bias: -0.560000, T: 100000, Avg. loss: 0.002137
Total training time: 0.26 seconds.
-- Epoch 6
Norm: 73.89, NNZs: 43189, Bias: -0.610000, T: 180000, Avg. loss: 0.000402
Norm: 68.44, NNZs: 41715, Bias: -0.590000, T: 100000, Avg. loss: 0.003079
Total training time: 0.27 seconds.
Total training time: 0.22 seconds.
Norm: 64.69, NNZs: 37538, Bias: -0.620000, T: 140000, Avg. loss: 0.000680
Total training time: 0.33 seconds.
-- Epoch 10
-- Epoch 6
-- Epoch 8
Norm: 61.57, NNZs: 37173, Bias: -0.590000, T: 80000, Avg. loss: 0.004598
Total training time: 0.21 seconds.
-- Epoch 5
Norm: 77.66, NNZs: 42950, Bias: -0.330000, T: 180000, Avg. loss: 0.000632
Total training time: 0.26 seconds.
-- Epoch 10
Norm: 66.64, NNZs: 37857, Bias: -0.660000, T: 220000, Avg. loss: 0.000193
Total training time: 0.46 seconds.
Norm: 74.22, NNZs: 40177, Bias: -0.380000, T: 180000, Avg. loss: 0.000540
Total training time: 0.29 seconds.
Convergence after 11 epochs took 0.46 seconds
-- Epoch 10
Norm: 59.18, NNZs: 36178, Bias: -0.540000, T: 80000, Avg. loss: 0.003759
Total training time: 0.22 seconds.
-- Epoch 5
Norm: 65.24, NNZs: 35927, Bias: -0.600000, T: 160000, Avg. loss: 0.000379
Total training time: 0.34 seconds.
-- Epoch 9
Norm: 66.03, NNZs: 39541, Bias: -0.580000, T: 120000, Avg. loss: 0.001145
Total training time: 0.28 seconds.
-- Epoch 7
Norm: 65.74, NNZs: 39505, Bias: -0.670000, T: 160000, Avg. loss: 0.000352
Total training time: 0.33 seconds.
-- Epoch 9
Norm: 62.09, NNZs: 39038, Bias: -0.520000, T: 80000, Avg. loss: 0.004084
Total training time: 0.21 seconds.
-- Epoch 5
Norm: 72.04, NNZs: 40161, Bias: -0.340000, T: 140000, Avg. loss: 0.001104
Total training time: 0.33 seconds.
Norm: 66.58, NNZs: 39423, Bias: -0.690000, T: 160000, Avg. loss: 0.000440
Total training time: 0.24 seconds.
-- Epoch 8
-- Epoch 9
Norm: 73.25, NNZs: 41911, Bias: -0.410000, T: 120000, Avg. loss: 0.001786
Total training time: 0.34 seconds.
-- Epoch 7
Norm: 74.42, NNZs: 40522, Bias: -0.420000, T: 220000, Avg. loss: 0.000308
Total training time: 0.23 seconds.
Convergence after 11 epochs took 0.23 seconds
Norm: 67.19, NNZs: 38388, Bias: -0.660000, T: 180000, Avg. loss: 0.000283
Total training time: 0.28 seconds.
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.5s finished
-- Epoch 10
Norm: 61.44, NNZs: 37113, Bias: -0.600000, T: 100000, Avg. loss: 0.001961
Total training time: 0.28 seconds.
Norm: 64.81, NNZs: 39389, Bias: -0.680000, T: 120000, Avg. loss: 0.001310
Total training time: 0.28 seconds.
-- Epoch 6
-- Epoch 7
Norm: 66.51, NNZs: 37946, Bias: -0.670000, T: 160000, Avg. loss: 0.000369
Total training time: 0.35 seconds.
-- Epoch 9
Norm: 61.83, NNZs: 36647, Bias: -0.570000, T: 100000, Avg. loss: 0.002392
Total training time: 0.24 seconds.
Norm: 73.53, NNZs: 42502, Bias: -0.560000, T: 180000, Avg. loss: 0.000381
Total training time: 0.34 seconds.
-- Epoch 6
-- Epoch 10
Norm: 72.08, NNZs: 41741, Bias: -0.320000, T: 100000, Avg. loss: 0.003207
Total training time: 0.23 seconds.
-- Epoch 6
Norm: 63.26, NNZs: 35479, Bias: -0.560000, T: 120000, Avg. loss: 0.001116
Total training time: 0.23 seconds.
-- Epoch 7
Norm: 74.73, NNZs: 40234, Bias: -0.400000, T: 200000, Avg. loss: 0.000232
Total training time: 0.31 seconds.
-- Epoch 11
Norm: 67.16, NNZs: 39560, Bias: -0.680000, T: 180000, Avg. loss: 0.000260
Total training time: 0.25 seconds.
-- Epoch 10
Norm: 68.10, NNZs: 40560, Bias: -0.580000, T: 160000, Avg. loss: 0.000443
Total training time: 0.36 seconds.
-- Epoch 9
Norm: 70.30, NNZs: 41996, Bias: -0.620000, T: 120000, Avg. loss: 0.001518
Total training time: 0.26 seconds.
-- Epoch 7
Norm: 67.77, NNZs: 38461, Bias: -0.640000, T: 200000, Avg. loss: 0.000238
Total training time: 0.30 seconds.
Norm: 64.06, NNZs: 35163, Bias: -0.600000, T: 120000, Avg. loss: 0.001293
Total training time: 0.30 seconds.
-- Epoch 11
Norm: 65.58, NNZs: 37708, Bias: -0.600000, T: 160000, Avg. loss: 0.000474
Total training time: 0.37 seconds.
-- Epoch 9
Norm: 63.34, NNZs: 37022, Bias: -0.610000, T: 120000, Avg. loss: 0.001150
Total training time: 0.25 seconds.
-- Epoch 7
Norm: 65.78, NNZs: 36011, Bias: -0.600000, T: 180000, Avg. loss: 0.000293
Total training time: 0.37 seconds.
Norm: 74.44, NNZs: 43258, Bias: -0.650000, T: 200000, Avg. loss: 0.000271
Total training time: 0.31 seconds.
-- Epoch 11
-- Epoch 10
Norm: 66.30, NNZs: 39615, Bias: -0.650000, T: 180000, Avg. loss: 0.000238
Total training time: 0.37 seconds.
-- Epoch 10
Norm: 74.71, NNZs: 42154, Bias: -0.400000, T: 140000, Avg. loss: 0.001157
Total training time: 0.37 seconds.
Norm: 63.80, NNZs: 37587, Bias: -0.560000, T: 100000, Avg. loss: 0.001954
Total training time: 0.25 seconds.
-- Epoch 8
-- Epoch 6
Norm: 78.38, NNZs: 43071, Bias: -0.350000, T: 200000, Avg. loss: 0.000528
Total training time: 0.30 seconds.
Norm: 64.41, NNZs: 39524, Bias: -0.560000, T: 100000, Avg. loss: 0.002109
Total training time: 0.25 seconds.
-- Epoch 6
Norm: 67.26, NNZs: 39744, Bias: -0.580000, T: 140000, Avg. loss: 0.000881
Total training time: 0.33 seconds.
-- Epoch 8
Norm: 68.10, NNZs: 38513, Bias: -0.660000, T: 220000, Avg. loss: 0.000171
Total training time: 0.31 seconds.
Norm: 65.80, NNZs: 39524, Bias: -0.720000, T: 140000, Avg. loss: 0.000577
Total training time: 0.32 seconds.
Convergence after 11 epochs took 0.31 seconds
-- Epoch 7
Norm: 67.59, NNZs: 39622, Bias: -0.720000, T: 200000, Avg. loss: 0.000224
Total training time: 0.28 seconds.
Convergence after 10 epochs took 0.28 seconds
-- Epoch 8
Norm: 75.11, NNZs: 40315, Bias: -0.410000, T: 220000, Avg. loss: 0.000173
Total training time: 0.34 seconds.
Convergence after 11 epochs took 0.34 seconds
Norm: 74.99, NNZs: 43313, Bias: -0.630000, T: 220000, Avg. loss: 0.000258
Total training time: 0.33 seconds.
Convergence after 11 epochs took 0.33 seconds
Norm: 74.14, NNZs: 42596, Bias: -0.580000, T: 200000, Avg. loss: 0.000269
Total training time: 0.38 seconds.
-- Epoch 11
Norm: 63.09, NNZs: 37441, Bias: -0.630000, T: 120000, Avg. loss: 0.001216
Total training time: 0.32 seconds.
-- Epoch 7
Norm: 64.36, NNZs: 37218, Bias: -0.630000, T: 140000, Avg. loss: 0.000659
Total training time: 0.27 seconds.
-- Epoch 8
Norm: 64.52, NNZs: 35719, Bias: -0.590000, T: 140000, Avg. loss: 0.000694
Total training time: 0.27 seconds.
-- Epoch 8
Norm: 68.68, NNZs: 40631, Bias: -0.590000, T: 180000, Avg. loss: 0.000340
Total training time: 0.39 seconds.
-- Epoch 10
-- Epoch 11
Norm: 71.59, NNZs: 42178, Bias: -0.660000, T: 140000, Avg. loss: 0.000983
Total training time: 0.29 seconds.
-- Epoch 8
Norm: 66.62, NNZs: 39673, Bias: -0.670000, T: 200000, Avg. loss: 0.000122
Total training time: 0.39 seconds.
-- Epoch 11
Norm: 65.08, NNZs: 37848, Bias: -0.600000, T: 120000, Avg. loss: 0.001054
Total training time: 0.27 seconds.
-- Epoch 7
Norm: 67.09, NNZs: 37997, Bias: -0.660000, T: 180000, Avg. loss: 0.000303
Total training time: 0.40 seconds.
-- Epoch 10
Norm: 65.30, NNZs: 35362, Bias: -0.580000, T: 140000, Avg. loss: 0.000803
Total training time: 0.34 seconds.
-- Epoch 8
Norm: 66.61, NNZs: 39647, Bias: -0.710000, T: 160000, Avg. loss: 0.000394
Norm: 66.09, NNZs: 37795, Bias: -0.620000, T: 180000, Avg. loss: 0.000239
Total training time: 0.41 seconds.
-- Epoch 10
Total training time: 0.34 seconds.
-- Epoch 9
Norm: 74.09, NNZs: 42051, Bias: -0.360000, T: 120000, Avg. loss: 0.002204
Total training time: 0.28 seconds.
-- Epoch 7
Norm: 66.14, NNZs: 39789, Bias: -0.580000, T: 120000, Avg. loss: 0.001164
Total training time: 0.28 seconds.
Norm: 72.90, NNZs: 40286, Bias: -0.380000, T: 160000, Avg. loss: 0.000556
Total training time: 0.40 seconds.
-- Epoch 7
-- Epoch 9
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.4s remaining:    0.7s
Norm: 66.27, NNZs: 36080, Bias: -0.630000, T: 200000, Avg. loss: 0.000242
Total training time: 0.41 seconds.
Convergence after 10 epochs took 0.41 seconds
Norm: 65.03, NNZs: 37347, Bias: -0.640000, T: 160000, Avg. loss: 0.000356
Total training time: 0.30 seconds.
-- Epoch 9
Norm: 66.94, NNZs: 39756, Bias: -0.660000, T: 220000, Avg. loss: 0.000105
Total training time: 0.41 seconds.
Convergence after 11 epochs took 0.41 seconds
Norm: 72.66, NNZs: 42261, Bias: -0.660000, T: 160000, Avg. loss: 0.000640
Total training time: 0.31 seconds.
Norm: 66.15, NNZs: 38062, Bias: -0.650000, T: 140000, Avg. loss: 0.000665
Total training time: 0.29 seconds.
-- Epoch 9
-- Epoch 8
Norm: 74.66, NNZs: 42652, Bias: -0.610000, T: 220000, Avg. loss: 0.000304
Total training time: 0.41 seconds.
Convergence after 11 epochs took 0.41 seconds
Norm: 68.13, NNZs: 39898, Bias: -0.590000, T: 160000, Avg. loss: 0.000455
Total training time: 0.37 seconds.
-- Epoch 9
Norm: 75.94, NNZs: 42345, Bias: -0.410000, T: 160000, Avg. loss: 0.000929
Total training time: 0.42 seconds.
-- Epoch 9
Norm: 64.32, NNZs: 37675, Bias: -0.650000, T: 140000, Avg. loss: 0.000727
Total training time: 0.36 seconds.
Norm: 67.33, NNZs: 40013, Bias: -0.550000, T: 140000, Avg. loss: 0.000764
Total training time: 0.29 seconds.
-- Epoch 8
Norm: 78.95, NNZs: 43134, Bias: -0.350000, T: 220000, Avg. loss: 0.000294
Total training time: 0.35 seconds.
-- Epoch 8
Convergence after 11 epochs took 0.35 seconds
Norm: 68.95, NNZs: 40704, Bias: -0.590000, T: 200000, Avg. loss: 0.000130
Total training time: 0.43 seconds.
Convergence after 10 epochs took 0.43 seconds
Norm: 65.37, NNZs: 35869, Bias: -0.610000, T: 160000, Avg. loss: 0.000487
Total training time: 0.30 seconds.
-- Epoch 9
Norm: 65.62, NNZs: 37453, Bias: -0.610000, T: 180000, Avg. loss: 0.000222
Total training time: 0.31 seconds.
-- Epoch 10
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.5s remaining:    0.9s
Norm: 67.74, NNZs: 38116, Bias: -0.670000, T: 200000, Avg. loss: 0.000280
Total training time: 0.43 seconds.
Convergence after 10 epochs took 0.43 seconds
Norm: 75.58, NNZs: 42252, Bias: -0.320000, T: 140000, Avg. loss: 0.001317
Total training time: 0.30 seconds.
-- Epoch 8
Norm: 73.53, NNZs: 42404, Bias: -0.670000, T: 180000, Avg. loss: 0.000551
Total training time: 0.32 seconds.
Norm: 67.04, NNZs: 38199, Bias: -0.650000, T: 160000, Avg. loss: 0.000528
Total training time: 0.31 seconds.
-- Epoch 10
-- Epoch 9
Norm: 66.54, NNZs: 37912, Bias: -0.680000, T: 200000, Avg. loss: 0.000195
Total training time: 0.44 seconds.
-- Epoch 11
Norm: 68.79, NNZs: 40012, Bias: -0.590000, T: 180000, Avg. loss: 0.000349
Total training time: 0.38 seconds.
-- Epoch 10
Norm: 66.13, NNZs: 35546, Bias: -0.600000, T: 160000, Avg. loss: 0.000486
Total training time: 0.38 seconds.
-- Epoch 9
Norm: 68.10, NNZs: 40134, Bias: -0.590000, T: 160000, Avg. loss: 0.000481
Total training time: 0.31 seconds.
-- Epoch 9
Norm: 73.48, NNZs: 40335, Bias: -0.370000, T: 180000, Avg. loss: 0.000301
Total training time: 0.43 seconds.
-- Epoch 10
Norm: 65.05, NNZs: 37778, Bias: -0.650000, T: 160000, Avg. loss: 0.000397
Total training time: 0.38 seconds.
-- Epoch 9
Norm: 65.95, NNZs: 35928, Bias: -0.580000, T: 180000, Avg. loss: 0.000217
Total training time: 0.32 seconds.
Norm: 67.17, NNZs: 39732, Bias: -0.720000, T: 180000, Avg. loss: 0.000254
Total training time: 0.38 seconds.
-- Epoch 10
-- Epoch 10
Norm: 66.02, NNZs: 37494, Bias: -0.630000, T: 200000, Avg. loss: 0.000190
Total training time: 0.33 seconds.
-- Epoch 11
Norm: 76.75, NNZs: 42395, Bias: -0.330000, T: 160000, Avg. loss: 0.000848
Total training time: 0.32 seconds.
-- Epoch 9
Norm: 67.77, NNZs: 38368, Bias: -0.650000, T: 180000, Avg. loss: 0.000402
Total training time: 0.32 seconds.
Norm: 74.13, NNZs: 42466, Bias: -0.650000, T: 200000, Avg. loss: 0.000342
-- Epoch 10
Total training time: 0.34 seconds.
-- Epoch 11
Norm: 69.22, NNZs: 40052, Bias: -0.600000, T: 200000, Avg. loss: 0.000188
Total training time: 0.40 seconds.
Convergence after 10 epochs took 0.40 seconds
Norm: 76.80, NNZs: 42433, Bias: -0.410000, T: 180000, Avg. loss: 0.000625
Total training time: 0.45 seconds.
Norm: 68.71, NNZs: 40249, Bias: -0.600000, T: 180000, Avg. loss: 0.000337
Total training time: 0.32 seconds.
-- Epoch 10
-- Epoch 10
Norm: 66.91, NNZs: 35688, Bias: -0.620000, T: 180000, Avg. loss: 0.000331
Total training time: 0.39 seconds.
-- Epoch 10
Norm: 66.63, NNZs: 36012, Bias: -0.580000, T: 200000, Avg. loss: 0.000259
Total training time: 0.33 seconds.
Convergence after 10 epochs took 0.33 seconds
Norm: 73.93, NNZs: 40396, Bias: -0.380000, T: 200000, Avg. loss: 0.000222
Total training time: 0.45 seconds.
-- Epoch 11
Norm: 66.37, NNZs: 37554, Bias: -0.630000, T: 220000, Avg. loss: 0.000142
Total training time: 0.34 seconds.
Convergence after 11 epochs took 0.34 seconds
Norm: 68.29, NNZs: 38430, Bias: -0.670000, T: 200000, Avg. loss: 0.000244
Norm: 77.62, NNZs: 42490, Bias: -0.350000, T: 180000, Avg. loss: 0.000629
Total training time: 0.33 seconds.
Total training time: 0.34 seconds.
-- Epoch 10
Convergence after 10 epochs took 0.34 seconds
Norm: 65.62, NNZs: 37849, Bias: -0.680000, T: 180000, Avg. loss: 0.000246
Total training time: 0.40 seconds.
-- Epoch 10
Norm: 67.68, NNZs: 39805, Bias: -0.720000, T: 200000, Avg. loss: 0.000219
Total training time: 0.40 seconds.
Norm: 66.84, NNZs: 37955, Bias: -0.670000, T: 220000, Avg. loss: 0.000150
Total training time: 0.47 seconds.
Convergence after 10 epochs took 0.40 seconds
Convergence after 11 epochs took 0.47 seconds
Norm: 69.21, NNZs: 40342, Bias: -0.580000, T: 200000, Avg. loss: 0.000204
Total training time: 0.34 seconds.
Convergence after 10 epochs took 0.34 seconds
Norm: 74.47, NNZs: 42538, Bias: -0.690000, T: 220000, Avg. loss: 0.000149
Total training time: 0.36 seconds.
Convergence after 11 epochs took 0.36 seconds
Norm: 77.50, NNZs: 42525, Bias: -0.380000, T: 200000, Avg. loss: 0.000348
Total training time: 0.46 seconds.
-- Epoch 11
Norm: 67.45, NNZs: 35728, Bias: -0.610000, T: 200000, Avg. loss: 0.000266
Total training time: 0.41 seconds.
Convergence after 10 epochs took 0.41 seconds
Norm: 74.43, NNZs: 40445, Bias: -0.400000, T: 220000, Avg. loss: 0.000224
Total training time: 0.46 seconds.
Convergence after 11 epochs took 0.46 seconds
Norm: 78.28, NNZs: 42596, Bias: -0.360000, T: 200000, Avg. loss: 0.000343
Total training time: 0.34 seconds.
-- Epoch 11
Norm: 66.17, NNZs: 37988, Bias: -0.690000, T: 200000, Avg. loss: 0.000283
Total training time: 0.41 seconds.
Convergence after 10 epochs took 0.41 seconds
[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    0.5s remaining:    0.8s
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.5s finished
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.6s finished
[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.5s finished
Norm: 78.03, NNZs: 42588, Bias: -0.420000, T: 220000, Avg. loss: 0.000260
Total training time: 0.47 seconds.
Convergence after 11 epochs took 0.47 seconds
Norm: 78.83, NNZs: 42692, Bias: -0.350000, T: 220000, Avg. loss: 0.000295
Total training time: 0.35 seconds.
Convergence after 11 epochs took 0.35 seconds
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.0s finished
	accuracy: 5-fold cross validation: [0.3554 0.3612 0.3528 0.3522 0.34  ]
	test accuracy: 5-fold cross validation accuracy: 0.35 (+/- 0.01)
dimensionality: 74535
density: 0.594799



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.3266
	accuracy score (normalize=False):  8165

compute the precision
	precision score (average=macro):  0.2732209474151851
	precision score (average=micro):  0.3266
	precision score (average=weighted):  0.32053822413852673
	precision score (average=None):  [0.53362734 0.1588551  0.19685812 0.23120946 0.20146362 0.21122706
 0.17714286 0.47538401]
	precision score (average=None, zero_division=1):  [0.53362734 0.1588551  0.19685812 0.23120946 0.20146362 0.21122706
 0.17714286 0.47538401]

compute the precision
	recall score (average=macro):  0.2745155638180785
	recall score (average=micro):  0.3266
	recall score (average=weighted):  0.3266
	recall score (average=None):  [0.57825568 0.19287576 0.15781189 0.22998102 0.20286086 0.20596491
 0.14547782 0.48289658]
	recall score (average=None, zero_division=1):  [0.57825568 0.19287576 0.15781189 0.22998102 0.20286086 0.20596491
 0.14547782 0.48289658]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.2730793919578336
	f1 score (average=micro):  0.3266
	f1 score (average=weighted):  0.32286347335682386
	f1 score (average=None):  [0.55504587 0.17422013 0.17518567 0.23059361 0.20215983 0.2085628
 0.15975638 0.47911085]

compute the F-beta score
	f beta score (average=macro):  0.27296395327399925
	f beta score (average=micro):  0.3266
	f beta score (average=weighted):  0.3212912724755046
	f beta score (average=None):  [0.54199328 0.164664   0.18757601 0.23096273 0.20174153 0.21015323
 0.16975309 0.47686777]

compute the average Hamming loss
	hamming loss:  0.6734

jaccard similarity coefficient score
	jaccard score (average=macro):  0.167071829594915
	jaccard score (average=None):  [0.38412698 0.09542231 0.09600192 0.13032258 0.11244594 0.11642205
 0.08681263 0.31502023]


================================================================================
Classifier.RANDOM_FOREST_CLASSIFIER
________________________________________________________________________________
Training: 
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=-1, oob_score=False, random_state=0, verbose=True,
                       warm_start=False)
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.8s
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   11.5s finished
train time: 11.585s
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.4s finished
test time:  0.407s
accuracy:   0.375


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   20.4s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   20.9s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   21.1s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   21.5s
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   21.4s
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   58.9s finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   58.9s finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   59.1s finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   59.1s finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.2s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.3s finished
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   59.4s finished
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.3s finished
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.3s finished
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.3s finished
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   59.8s finished
	accuracy: 5-fold cross validation: [0.3698 0.3692 0.3728 0.3798 0.3696]
	test accuracy: 5-fold cross validation accuracy: 0.37 (+/- 0.01)


===> Classification Metrics:

accuracy classification score
	accuracy score:  0.3754
	accuracy score (normalize=False):  9385

compute the precision
	precision score (average=macro):  0.36494037916673894
	precision score (average=micro):  0.3754
	precision score (average=weighted):  0.3661644351403273
	precision score (average=None):  [0.38122525 0.56756757 0.41984733 0.30737705 0.30079156 0.26226226
 0.2962963  0.38415573]
	precision score (average=None, zero_division=1):  [0.38122525 0.56756757 0.41984733 0.30737705 0.30079156 0.26226226
 0.2962963  0.38415573]

compute the precision
	recall score (average=macro):  0.24818377271740238
	recall score (average=micro):  0.3754
	recall score (average=weighted):  0.3754
	recall score (average=None):  [0.90083632 0.0091225  0.02164502 0.056926   0.04941482 0.09192982
 0.00682594 0.84876975]
	recall score (average=None, zero_division=1):  [0.90083632 0.0091225  0.02164502 0.056926   0.04941482 0.09192982
 0.00682594 0.84876975]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.1817759524125217
	f1 score (average=micro):  0.37540000000000007
	f1 score (average=weighted):  0.25394783258956666
	f1 score (average=None):  [0.53573332 0.01795639 0.04116766 0.09606148 0.08488459 0.13613926
 0.01334445 0.52892047]

compute the F-beta score
	f beta score (average=macro):  0.19126351249620785
	f beta score (average=micro):  0.37539999999999996
	f beta score (average=weighted):  0.2416285852584176
	f beta score (average=None):  [0.43093923 0.04285714 0.08972268 0.16350556 0.14909757 0.19135261
 0.03125    0.43138331]

compute the average Hamming loss
	hamming loss:  0.6246

jaccard similarity coefficient score
	jaccard score (average=macro):  0.11625366642448452
	jaccard score (average=None):  [0.36587141 0.00905953 0.02101643 0.05045409 0.04432348 0.07304154
 0.00671704 0.3595458 ]


================================================================================
Classifier.RIDGE_CLASSIFIER
________________________________________________________________________________
Training: 
RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, random_state=0, solver='auto',
                tol=0.001)
train time: 3.604s
test time:  0.048s
accuracy:   0.387


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.9s finished
	accuracy: 5-fold cross validation: [0.4108 0.4112 0.4056 0.4084 0.3986]
	test accuracy: 5-fold cross validation accuracy: 0.41 (+/- 0.01)
dimensionality: 74535
density: 1.000000



===> Classification Metrics:

accuracy classification score
	accuracy score:  0.38716
	accuracy score (normalize=False):  9679

compute the precision
	precision score (average=macro):  0.29148144847076757
	precision score (average=micro):  0.38716
	precision score (average=weighted):  0.331319942725665
	precision score (average=None):  [0.50090768 0.17093236 0.21986755 0.27287649 0.25710754 0.24648506
 0.1947644  0.46891052]
	precision score (average=None, zero_division=1):  [0.50090768 0.17093236 0.21986755 0.27287649 0.25710754 0.24648506
 0.1947644  0.46891052]

compute the precision
	recall score (average=macro):  0.29799143645699677
	recall score (average=micro):  0.38716
	recall score (average=weighted):  0.38716
	recall score (average=None):  [0.76921545 0.08123371 0.13065722 0.25237192 0.18032076 0.19684211
 0.07935154 0.69393879]
	recall score (average=None, zero_division=1):  [0.76921545 0.08123371 0.13065722 0.25237192 0.18032076 0.19684211
 0.07935154 0.69393879]

compute the F1 score, also known as balanced F-score or F-measure
	f1 score (average=macro):  0.28078218245809883
	f1 score (average=micro):  0.38716
	f1 score (average=weighted):  0.3433115433020568
	f1 score (average=None):  [0.60672216 0.11012956 0.16391015 0.26222397 0.21197452 0.21888412
 0.11276144 0.55965153]

compute the F-beta score
	f beta score (average=macro):  0.28304171989750515
	f beta score (average=micro):  0.38716
	f beta score (average=weighted):  0.3320503712096404
	f beta score (average=None):  [0.53847226 0.14001198 0.19345065 0.26851328 0.23692904 0.23464949
 0.15087605 0.50143101]

compute the average Hamming loss
	hamming loss:  0.61284

jaccard similarity coefficient score
	jaccard score (average=macro):  0.17795640661829856
	jaccard score (average=None):  [0.43546387 0.05827361 0.08927131 0.1508963  0.11855229 0.12289157
 0.05974944 0.38855287]


================================================================================
Classifier.RIDGE_CLASSIFIERCV
________________________________________________________________________________
Training: 
RidgeClassifierCV(alphas=array([ 0.1,  1. , 10. ]), class_weight=None, cv=None,
                  fit_intercept=True, normalize=False, scoring=None,
                  store_cv_values=False)
train time: 1745.608s
test time:  0.039s
accuracy:   0.415


cross validation:
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
03/03/2020 06:53:40 PM - ERROR - A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}
Accuracy score for the IMDB_REVIEWS dataset (Multi-class classification): Final classification report: 
1) ADA_BOOST_CLASSIFIER
		Accuracy score = 0.3586		Training time = 11.363102436065674		Test time = 0.7093572616577148		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

2) BERNOULLI_NB
		Accuracy score = 0.37132		Training time = 0.03866863250732422		Test time = 0.03857541084289551		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

3) COMPLEMENT_NB
		Accuracy score = 0.37312		Training time = 0.03188347816467285		Test time = 0.017198562622070312		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

4) DECISION_TREE_CLASSIFIER
		Accuracy score = 0.25764		Training time = 35.036760091781616		Test time = 0.014272212982177734		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

5) EXTRA_TREE_CLASSIFIER
		Accuracy score = 0.2212		Training time = 0.9891903400421143		Test time = 0.019483327865600586		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

6) EXTRA_TREES_CLASSIFIER
		Accuracy score = 0.37404		Training time = 16.37880563735962		Test time = 0.5190737247467041		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

7) GRADIENT_BOOSTING_CLASSIFIER
		Accuracy score = 0.37624		Training time = 405.1393692493439		Test time = 0.29312729835510254		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

8) K_NEIGHBORS_CLASSIFIER
		Accuracy score = 0.26352		Training time = 0.0064580440521240234		Test time = 13.393989086151123		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

9) LINEAR_SVC
		Accuracy score = 0.37328		Training time = 1.785952091217041		Test time = 0.01836395263671875		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

10) LOGISTIC_REGRESSION
		Accuracy score = 0.42084		Training time = 9.605107069015503		Test time = 0.01957106590270996		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

11) LOGISTIC_REGRESSION_CV
		Accuracy score = 0.40532		Training time = 153.31329560279846		Test time = 0.03341484069824219		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

12) MLP_CLASSIFIER
		Accuracy score = 0.34468		Training time = 2298.638989686966		Test time = 0.1609208583831787		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

13) MULTINOMIAL_NB
		Accuracy score = 0.34924		Training time = 0.031163930892944336		Test time = 0.016951322555541992		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

14) NEAREST_CENTROID
		Accuracy score = 0.36844		Training time = 0.027098655700683594		Test time = 0.02132129669189453		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

15) NU_SVC
		Accuracy score = 0.4232		Training time = 738.827440738678		Test time = 364.28000140190125		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

16) PASSIVE_AGGRESSIVE_CLASSIFIER
		Accuracy score = 0.33112		Training time = 0.5089375972747803		Test time = 0.05643320083618164		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

17) PERCEPTRON
		Accuracy score = 0.3266		Training time = 0.30956101417541504		Test time = 0.025687694549560547		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

18) RANDOM_FOREST_CLASSIFIER
		Accuracy score = 0.3754		Training time = 11.585491418838501		Test time = 0.40709662437438965		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']

19) RIDGE_CLASSIFIER
		Accuracy score = 0.38716		Training time = 3.6039609909057617		Test time = 0.04779624938964844		Cross validation scores = [array([0.3464, 0.3472, 0.3528, 0.3464, 0.3606]), array([0.3788, 0.376 , 0.3768, 0.374 , 0.3704]), array([0.3906, 0.3834, 0.389 , 0.3826, 0.39  ]), array([0.262 , 0.255 , 0.251 , 0.2444, 0.2582]), array([0.215 , 0.2322, 0.2226, 0.2232, 0.2272]), array([0.3702, 0.371 , 0.3744, 0.3726, 0.3702]), array([0.3796, 0.3688, 0.3648, 0.364 , 0.3734]), array([0.3044, 0.3134, 0.3262, 0.319 , 0.3132]), array([0.3972, 0.4006, 0.3916, 0.395 , 0.3878]), array([0.4314, 0.4318, 0.4226, 0.4322, 0.4258]), array([0.4184, 0.4236, 0.4178, 0.4234, 0.4154]), array([0.3858, 0.3784, 0.3808, 0.3732, 0.371 ]), array([0.3516, 0.3552, 0.3528, 0.3494, 0.3544]), array([0.3718, 0.377 , 0.3638, 0.3736, 0.3756]), array([0.4302, 0.436 , 0.4248, 0.4384, 0.4282]), array([0.362 , 0.3618, 0.3604, 0.357 , 0.347 ]), array([0.3554, 0.3612, 0.3528, 0.3522, 0.34  ]), array([0.3698, 0.3692, 0.3728, 0.3798, 0.3696]), array([0.4108, 0.4112, 0.4056, 0.4084, 0.3986])]		Average accuracy score (Cross Validation) = ['0.35 (+/- 0.01)', '0.38 (+/- 0.01)', '0.39 (+/- 0.01)', '0.25 (+/- 0.01)', '0.22 (+/- 0.01)', '0.37 (+/- 0.00)', '0.37 (+/- 0.01)', '0.32 (+/- 0.01)', '0.39 (+/- 0.01)', '0.43 (+/- 0.01)', '0.42 (+/- 0.01)', '0.38 (+/- 0.01)', '0.35 (+/- 0.00)', '0.37 (+/- 0.01)', '0.43 (+/- 0.01)', '0.36 (+/- 0.01)', '0.35 (+/- 0.01)', '0.37 (+/- 0.01)', '0.41 (+/- 0.01)']



Best algorithm:
===> 15) NU_SVC
		Accuracy score = 0.4232		Training time = 738.827440738678		Test time = 364.28000140190125



DONE!
Program finished. It took 20113.825294733047 seconds

Process finished with exit code 0

```