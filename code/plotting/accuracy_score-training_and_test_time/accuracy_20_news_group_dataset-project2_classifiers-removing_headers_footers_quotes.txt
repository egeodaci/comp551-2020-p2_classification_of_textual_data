##########################################################
# Classification of text documents using sparse features
##########################################################

This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.

The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.

https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py

Usage: classification_of_text_documents.py [options]

Options:
  -h, --help            show this help message and exit
  --report              Print a detailed classification report.
  --chi2_select=SELECT_CHI2
                        Select some number of features using a chi-squared
                        test
  --confusion_matrix    Print the confusion matrix.
  --top10               Print ten most discriminative terms per class for
                        every classifier.
  --all_categories      Whether to use all categories or not.
  --use_hashing         Use a hashing vectorizer.
  --n_features=N_FEATURES
                        n_features when using the hashing vectorizer.
  --filtered            Remove newsgroup information that is easily overfit:
                        headers, signatures, and quoting.

Loading 20 newsgroups dataset for categories:
all
data loaded
11314 documents - 13.782MB (training set)
7532 documents - 8.262MB (test set)
20 categories

Extracting features from the training data using a sparse vectorizer
done in 1.849190s at 7.453MB/s
n_samples: 11314, n_features: 101322

Extracting features from the test data using the same vectorizer
done in 0.965010s at 8.561MB/s
n_samples: 7532, n_features: 101322

================================================================================
Logistic Regression
________________________________________________________________________________
Training: 
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
train time: 43.415s
test time:  0.021s
accuracy:   0.695
dimensionality: 101322
density: 1.000000

classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.48      0.46      0.47       319
           comp.graphics       0.64      0.71      0.67       389
 comp.os.ms-windows.misc       0.66      0.63      0.64       394
comp.sys.ibm.pc.hardware       0.68      0.65      0.66       392
   comp.sys.mac.hardware       0.76      0.69      0.72       385
          comp.windows.x       0.84      0.72      0.78       395
            misc.forsale       0.77      0.78      0.78       390
               rec.autos       0.75      0.73      0.74       396
         rec.motorcycles       0.48      0.81      0.61       398
      rec.sport.baseball       0.81      0.83      0.82       397
        rec.sport.hockey       0.92      0.87      0.89       399
               sci.crypt       0.89      0.68      0.77       396
         sci.electronics       0.57      0.62      0.59       393
                 sci.med       0.78      0.79      0.79       396
               sci.space       0.71      0.75      0.73       394
  soc.religion.christian       0.65      0.82      0.72       398
      talk.politics.guns       0.58      0.68      0.63       364
   talk.politics.mideast       0.85      0.76      0.80       376
      talk.politics.misc       0.61      0.44      0.51       310
      talk.religion.misc       0.57      0.18      0.27       251

                accuracy                           0.69      7532
               macro avg       0.70      0.68      0.68      7532
            weighted avg       0.71      0.69      0.69      7532

confusion matrix:
[[146   2   3   1   1   2   1   5  19   6   1   1   3  10  17  66   6  12
    6  11]
 [  5 278  22   7   6  19   7   2  10   4   0   4  14   1   9   1   0   0
    0   0]
 [  4  21 247  39  14  13   2   3  19   2   1   3   2   8  10   0   2   0
    3   1]
 [  1  14  35 255  26   3  11   2   8   1   1   1  33   0   1   0   0   0
    0   0]
 [  2   7  10  27 266   3  13   2  16   2   1   1  26   4   4   1   0   0
    0   0]
 [  0  45  28   8   3 286   1   2   9   1   0   2   3   2   4   0   0   0
    1   0]
 [  0   2   2  15  15   0 306  10  13   2   1   1  13   2   3   1   3   0
    1   0]
 [  1   0   1   0   2   1  11 288  43   6   0   0  21   1   9   1   4   2
    4   1]
 [  5   2   0   0   1   0   7  19 323   6   0   0  12   4   7   2   3   0
    7   0]
 [  2   4   0   1   0   2   3   3  23 329  16   0   2   3   0   3   0   3
    3   0]
 [  4   2   0   0   1   0   0   3  14  17 346   0   1   2   0   0   6   1
    2   0]
 [  3  11   5   3   5   3   4   2  24   4   1 271  13   6   7   1  20   4
    9   0]
 [  4  19  12  19  10   2  14  12  19   5   0  12 242   8   9   2   1   2
    1   0]
 [  7   7   1   1   0   2   6   6  23   0   2   0   8 314   4   5   3   1
    5   1]
 [  6  11   2   0   1   1   4   7  23   3   2   0  13  11 295   2   5   2
    6   0]
 [ 19   3   3   0   0   0   1   0  20   3   0   0   2   3   5 326   2   3
    2   6]
 [ 10   1   2   0   1   1   2   7  22   3   1   7   3   5   9   8 249   6
   19   8]
 [ 24   2   2   0   0   0   1   3  15   6   0   2   4   2   2   7   8 284
   13   1]
 [ 16   1   0   0   0   1   2   2  13   3   2   1   4   7  12   2  94   9
  137   4]
 [ 43   3   2   0   0   1   1   4  12   3   2   0   3  10   8  76  25   7
    7  44]]

================================================================================
Decision Tree Classifier
________________________________________________________________________________
Training: 
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=None, splitter='best')
train time: 18.691s
test time:  0.014s
accuracy:   0.444
classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.30      0.27      0.29       319
           comp.graphics       0.43      0.44      0.43       389
 comp.os.ms-windows.misc       0.42      0.43      0.42       394
comp.sys.ibm.pc.hardware       0.35      0.37      0.36       392
   comp.sys.mac.hardware       0.44      0.45      0.45       385
          comp.windows.x       0.52      0.45      0.48       395
            misc.forsale       0.59      0.57      0.58       390
               rec.autos       0.27      0.52      0.35       396
         rec.motorcycles       0.54      0.54      0.54       398
      rec.sport.baseball       0.57      0.49      0.53       397
        rec.sport.hockey       0.62      0.64      0.63       399
               sci.crypt       0.58      0.47      0.52       396
         sci.electronics       0.29      0.29      0.29       393
                 sci.med       0.48      0.41      0.44       396
               sci.space       0.52      0.47      0.49       394
  soc.religion.christian       0.49      0.52      0.50       398
      talk.politics.guns       0.40      0.42      0.41       364
   talk.politics.mideast       0.61      0.55      0.58       376
      talk.politics.misc       0.28      0.22      0.25       310
      talk.religion.misc       0.21      0.17      0.19       251

                accuracy                           0.44      7532
               macro avg       0.45      0.43      0.44      7532
            weighted avg       0.45      0.44      0.45      7532

confusion matrix:
[[ 86   9   5   4   5   2   4  23   9   6   5   3   5  11  12  47  14  21
   11  37]
 [  1 171  30  30  22  36   8  19   4   4   5   6  22   8  10   3   2   4
    3   1]
 [  3  24 171  45  28  28   6  23   6   5   5   8   9  13   5   2   3   5
    3   2]
 [  6  24  41 145  39  16  17  17   9   1   4   3  39  12   4   4   2   3
    3   3]
 [  4  18  15  30 174   8  25  29   6   2   4   7  24  11  11   1   2   6
    4   4]
 [  3  31  56  28  16 176   5  32   2   1   1   8  12   7   7   0   2   2
    5   1]
 [  0  14   8  25  22   8 224  34   5   7   6   3   9   3  10   2   2   3
    3   2]
 [  3   7   9   7   9   6  16 205  25   7   8   6  28  10  10   3  17   8
    7   5]
 [  8   4   6   7   7   4  13  38 215  12   5   8  18   7   9   6  11   4
    7   9]
 [  4   3   5   5   3   2   5  38   8 195  70   5  11   8   9   3   6   3
    7   7]
 [  3   3   5   1   3   6   3  28  10  45 257   3   4   4   1   1   9   4
    4   5]
 [  7  18   7   9  14   8   4  34   7   2   3 185  19   6  11   5  32   8
   13   4]
 [  7  19  18  34  27  10  24  40  16   7  10  19 115  11  10   2   8   7
    6   3]
 [ 12  20   8   9  10   8   8  37  21   7   2   8  20 164  14   7   7  11
   12  11]
 [  7   9   8  11   4   4   5  43  12  12   6  11  23  12 186   8   8   5
   16   4]
 [ 37   5   5   6   0   5   3  25   1  12   3   3   5   9   4 205   8  14
   15  33]
 [ 13   5   7   5   4   4   3  27  14   5   6  12   5  15  22  15 152   4
   29  17]
 [ 25   3   1   0   3   4   1  18   9   6   5   6   6   8   9  17  20 205
   22   8]
 [ 21   2   3   6   2   4   1  28  14   3   6  11  17  17  11  20  63   7
   69   5]
 [ 33   9   4   2   0   1   3  23   4   5   5   3   3   8   5  66  15  11
    9  42]]

================================================================================
Linear SVC (penalty = l2)
________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=None, tol=0.001,
          verbose=0)
train time: 5.643s
test time:  0.030s
accuracy:   0.697
dimensionality: 101322
density: 1.000000

classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.54      0.49      0.51       319
           comp.graphics       0.66      0.72      0.69       389
 comp.os.ms-windows.misc       0.62      0.62      0.62       394
comp.sys.ibm.pc.hardware       0.65      0.66      0.66       392
   comp.sys.mac.hardware       0.73      0.70      0.72       385
          comp.windows.x       0.83      0.70      0.76       395
            misc.forsale       0.75      0.79      0.77       390
               rec.autos       0.75      0.71      0.73       396
         rec.motorcycles       0.79      0.76      0.77       398
      rec.sport.baseball       0.55      0.86      0.67       397
        rec.sport.hockey       0.87      0.87      0.87       399
               sci.crypt       0.83      0.72      0.77       396
         sci.electronics       0.63      0.59      0.61       393
                 sci.med       0.79      0.78      0.79       396
               sci.space       0.75      0.75      0.75       394
  soc.religion.christian       0.65      0.79      0.71       398
      talk.politics.guns       0.60      0.66      0.63       364
   talk.politics.mideast       0.84      0.76      0.80       376
      talk.politics.misc       0.55      0.46      0.50       310
      talk.religion.misc       0.45      0.29      0.35       251

                accuracy                           0.70      7532
               macro avg       0.69      0.68      0.68      7532
            weighted avg       0.70      0.70      0.69      7532

confusion matrix:
[[155   2   4   1   1   0   4   3   4  11   4   3   7   5   8  54   7  13
    9  24]
 [  4 279  20   9   6  21   6   2   0   9   0  11   8   2   8   2   1   0
    0   1]
 [  3  22 243  37  16  11   3   4   1  16   2   3   3   6   9   2   1   2
    8   2]
 [  0  10  38 260  26   6  12   1   0   9   2   4  21   0   1   0   0   0
    2   0]
 [  2  10   8  25 271   6  14   5   1  16   1   2  16   2   1   1   3   0
    0   1]
 [  1  41  39   5   4 278   2   1   1   9   0   2   3   1   5   0   2   0
    0   1]
 [  0   3   2  12  15   0 308   8   6  10   2   1   9   2   2   2   2   3
    2   1]
 [  2   1   3   3   3   1  13 283  17  26   3   2  16   3   5   2   4   4
    5   0]
 [  5   3   1   1   2   0   7  18 302  19   2   0   9   4   7   3   4   1
    8   2]
 [  1   2   0   2   0   1   6   3   5 340  17   0   2   4   1   4   1   2
    6   0]
 [  0   1   2   2   1   0   1   2   2  27 347   0   1   3   0   1   5   0
    1   3]
 [  5   6   5   4   4   1   6   2   4  19   1 285  10   3   5   4  12   2
   15   3]
 [  4  12   9  28  13   6  13  11   9  15   4  10 233   8   9   3   1   1
    3   1]
 [  5   6   4   1   1   0   1   9   4  15   5   0   7 309   6   5   5   5
    5   3]
 [  6  11   5   2   3   1   3  11   6  20   3   0   8   6 296   1   4   0
    6   2]
 [ 21   1   3   0   0   1   2   0   1  15   0   3   3   5   3 315   0   1
    5  19]
 [  8   3   3   1   1   0   4   7   7  13   0  10   1   9   7   9 242   8
   20  11]
 [ 25   1   1   2   0   0   2   2   7   9   1   1   2   3   3   8   8 284
   14   3]
 [  9   1   0   1   2   1   0   4   5  11   2   3   4   8  11   1  84   7
  144  12]
 [ 31   6   2   3   2   1   1   2   1  10   1   2   4   7   6  67  19   5
    8  73]]

================================================================================
Linear SVC (penalty = l1)
________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.001,
          verbose=0)
train time: 4.323s
test time:  0.029s
accuracy:   0.666
dimensionality: 101322
density: 0.005268

classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.46      0.45      0.45       319
           comp.graphics       0.66      0.72      0.69       389
 comp.os.ms-windows.misc       0.64      0.63      0.63       394
comp.sys.ibm.pc.hardware       0.60      0.66      0.63       392
   comp.sys.mac.hardware       0.70      0.67      0.68       385
          comp.windows.x       0.82      0.66      0.73       395
            misc.forsale       0.72      0.75      0.74       390
               rec.autos       0.46      0.74      0.57       396
         rec.motorcycles       0.78      0.71      0.74       398
      rec.sport.baseball       0.83      0.74      0.78       397
        rec.sport.hockey       0.83      0.84      0.84       399
               sci.crypt       0.81      0.71      0.75       396
         sci.electronics       0.56      0.54      0.55       393
                 sci.med       0.79      0.72      0.76       396
               sci.space       0.75      0.71      0.73       394
  soc.religion.christian       0.62      0.76      0.68       398
      talk.politics.guns       0.57      0.66      0.61       364
   talk.politics.mideast       0.86      0.70      0.78       376
      talk.politics.misc       0.52      0.45      0.48       310
      talk.religion.misc       0.38      0.28      0.32       251

                accuracy                           0.67      7532
               macro avg       0.67      0.65      0.66      7532
            weighted avg       0.68      0.67      0.67      7532

confusion matrix:
[[142   2   3   2   1   0   6  16   7   2   4   3   6   7  11  61   8   7
    5  26]
 [  3 279  19  12   8  19   5   9   2   2   1   9   6   1   9   1   0   1
    1   2]
 [  6  19 247  40  11  12   2  18   1   1   2   4   3   5   6   4   4   0
    8   1]
 [  0  11  35 258  24   3   9   9   0   2   2   5  29   0   1   2   1   0
    0   1]
 [  2  12   7  27 258   4  10  16   5   2   3   2  22   5   4   3   2   0
    0   1]
 [  1  45  39   5   5 262   5   9   1   1   1   3   6   0   6   1   2   2
    0   1]
 [  0   1   3  22  16   0 294  19   1   3   2   1  11   2   5   1   5   1
    1   2]
 [  4   2   2   6   2   2  15 294  15   2   2   2  19   4   4   4   3   4
    9   1]
 [  5   2   2   2   2   1   6  43 284   6   1   2  12   4   5   4   3   0
    9   5]
 [  3   3   0   4   1   1   9  29   5 293  26   0   3   3   1   5   1   1
    5   4]
 [  1   2   3   2   5   1   0  12   4  18 334   1   2   1   1   3   3   0
    4   2]
 [  4   5   7   4   6   2  11  20   4   0   2 280  11   2   4   2  18   2
    9   3]
 [  5  11   5  34  18   4  15  27   6   6   6   9 213   8   7   5   7   1
    4   2]
 [  9  10   2   2   1   0   6  26   7   1   6   2   6 287   6   7   6   1
    8   3]
 [  6  11   3   4   4   2   3  24   6   3   1   2  16  10 279   4   2   0
   11   3]
 [ 25   2   1   0   2   1   1  15   3   1   0   4   3   4   2 301   3   6
    5  19]
 [  8   4   4   2   2   1   1  19   6   4   0   9   2   3   9   6 239   6
   23  16]
 [ 28   0   2   2   1   1   3   8   2   4   1   4   2   3   2  16  12 265
   16   4]
 [ 10   1   0   0   1   2   2  12   4   1   5   4   7   9   6   3  78   6
  140  19]
 [ 44   3   1   1   1   2   6  15   2   2   1   1   3   4   5  56  20   4
   10  70]]

================================================================================
Ada Boost Classifier
________________________________________________________________________________
Training: 
AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=None)
train time: 9.493s
test time:  0.524s
accuracy:   0.365
classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.00      0.00      0.00       319
           comp.graphics       0.60      0.23      0.33       389
 comp.os.ms-windows.misc       0.64      0.40      0.49       394
comp.sys.ibm.pc.hardware       0.48      0.31      0.38       392
   comp.sys.mac.hardware       0.69      0.37      0.48       385
          comp.windows.x       0.73      0.41      0.52       395
            misc.forsale       0.75      0.52      0.61       390
               rec.autos       0.79      0.38      0.52       396
         rec.motorcycles       0.92      0.33      0.48       398
      rec.sport.baseball       0.74      0.19      0.30       397
        rec.sport.hockey       0.64      0.57      0.60       399
               sci.crypt       0.80      0.42      0.55       396
         sci.electronics       0.08      0.82      0.15       393
                 sci.med       0.88      0.21      0.34       396
               sci.space       0.73      0.34      0.46       394
  soc.religion.christian       0.52      0.65      0.58       398
      talk.politics.guns       0.48      0.24      0.32       364
   talk.politics.mideast       0.96      0.50      0.65       376
      talk.politics.misc       0.29      0.17      0.21       310
      talk.religion.misc       0.14      0.01      0.02       251

                accuracy                           0.37      7532
               macro avg       0.59      0.35      0.40      7532
            weighted avg       0.61      0.37      0.41      7532

confusion matrix:
[[  0   0   0   0   0   3   2   0   0   0   3   4 190   1   5  95   6   1
    6   3]
 [  0  90  16  12   7  23   4   1   0   0   3   2 224   0   7   0   0   0
    0   0]
 [  0  21 157  23  15  20   2   1   0   0   1   0 148   1   3   0   2   0
    0   0]
 [  0  12  27 121   8   3   5   0   0   0   1   2 207   2   4   0   0   0
    0   0]
 [  1   2   1  27 142   0  12   0   0   0   2   5 188   0   4   0   0   0
    1   0]
 [  0   9  32   4   3 162   2   0   0   0   0   4 170   0   4   0   0   1
    3   1]
 [  0   6   5  23   9   1 202  10   2   1   3   2 120   0   3   2   1   0
    0   0]
 [  0   0   2  10   0   0   8 152   4   0   1   1 206   0   1   1   9   0
    1   0]
 [  0   0   0  12   0   1   4   7 131   1   1   2 228   1   1   4   4   0
    1   0]
 [  0   1   0   3   0   0   3   0   0  75  92   1 215   0   1   1   2   0
    1   2]
 [  0   0   0   0   0   1   7   0   1  18 228   0 140   0   0   1   1   0
    1   1]
 [  0   0   0   2   6   1   1   0   1   0   0 168 167   0   3   1  16   2
   28   0]
 [  1   6   2  12   6   2   7  13   1   1   2  13 322   0   4   0   0   0
    1   0]
 [  3   0   0   0   1   0   2   0   0   0   0   0 293  84   0  10   1   0
    2   0]
 [  3   1   1   1   9   3   3   3   1   0   8   2 212   4 132   2   2   0
    7   0]
 [  4   1   0   0   0   0   1   0   0   0   0   0 122   0   1 257   0   2
    4   6]
 [  0   0   2   1   0   3   3   1   2   1   6   3 194   0   4  12  87   1
   43   1]
 [  1   0   0   0   0   0   0   0   0   1   0   1 138   0   1  15   9 187
   20   3]
 [  0   0   0   0   1   0   0   2   0   2   4   0 202   3   1   8  34   0
   52   1]
 [  1   0   1   1   0   0   1   3   0   1   1   1 136   0   1  84   9   1
    7   3]]

================================================================================
Random forest
________________________________________________________________________________
Training: 
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)
train time: 66.423s
test time:  1.104s
accuracy:   0.620
classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.46      0.38      0.42       319
           comp.graphics       0.58      0.60      0.59       389
 comp.os.ms-windows.misc       0.52      0.65      0.58       394
comp.sys.ibm.pc.hardware       0.62      0.55      0.59       392
   comp.sys.mac.hardware       0.63      0.66      0.64       385
          comp.windows.x       0.67      0.65      0.66       395
            misc.forsale       0.68      0.75      0.71       390
               rec.autos       0.39      0.67      0.50       396
         rec.motorcycles       0.66      0.71      0.68       398
      rec.sport.baseball       0.67      0.78      0.72       397
        rec.sport.hockey       0.84      0.82      0.83       399
               sci.crypt       0.82      0.65      0.73       396
         sci.electronics       0.52      0.42      0.47       393
                 sci.med       0.77      0.65      0.71       396
               sci.space       0.68      0.66      0.67       394
  soc.religion.christian       0.58      0.80      0.68       398
      talk.politics.guns       0.53      0.59      0.56       364
   talk.politics.mideast       0.83      0.70      0.76       376
      talk.politics.misc       0.58      0.33      0.42       310
      talk.religion.misc       0.30      0.08      0.13       251

                accuracy                           0.62      7532
               macro avg       0.62      0.61      0.60      7532
            weighted avg       0.63      0.62      0.61      7532

confusion matrix:
[[122   4   2   0   2   3  10  21   7   8   5   4   2   8  10  81   8   7
    3  12]
 [  4 232  33  15  12  36  11  10   4   3   2   4   9   2  10   0   1   0
    1   0]
 [  2  24 257  26  17  19   3  17   3   5   0   4   4   2   7   0   2   1
    1   0]
 [  2  14  56 217  33   8  11  11   2   4   2   1  27   2   1   1   0   0
    0   0]
 [  0   9  12  23 253   7  18  21   7   6   2   0  19   2   5   1   0   0
    0   0]
 [  2  34  49   8  11 257   4  12   0   2   1   3   2   1   6   0   1   1
    1   0]
 [  0  10   4  17  18   1 291  23   2   4   2   1   6   1   5   1   3   0
    1   0]
 [  5   4   7   2   3   9  15 264  36   5   2   2  13   6   7   2   9   1
    2   2]
 [  4   1   5   3   0   1  11  41 282  14   1   0   8   3   6   5   7   1
    5   0]
 [  3   4   3   3   0   2   3  28  11 308  20   1   0   2   2   3   0   2
    2   0]
 [  3   1   3   1   1   2   1  17   6  25 329   0   1   2   2   0   1   0
    2   2]
 [  5   7  14   5   9   6   8  21   5   1   2 259  14   1   9   1  23   3
    2   1]
 [  3  24  16  21  29  13  16  31  12  13   7  13 165   8  10   1   5   2
    3   1]
 [  9  12   5   1   2   7  12  30   6  14   3   0  12 258   8   4   5   3
    2   3]
 [  7   9   5   3   4   3   6  32  11  10   5   2  13   4 260   6   7   1
    5   1]
 [ 14   0   4   0   0   1   2  17   7   3   1   0   3   1   4 319   2   8
    4   8]
 [  3   4   6   2   2   4   1  25  10  12   1  12   5   7   5  10 216   5
   21  13]
 [ 28   3   1   0   1   2   1  11   8  12   1   4   4   3   7  10   8 262
   10   0]
 [ 12   1   3   1   1   2   2  22   4   6   4   3   8  15  10   9  90  10
  101   6]
 [ 40   3   6   0   2   1   1  15   5   4   4   4   1   5   6  93  23   8
    9  21]]

================================================================================

Process finished with exit code 0