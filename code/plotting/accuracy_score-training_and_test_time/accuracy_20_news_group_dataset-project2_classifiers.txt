##########################################################
# Classification of text documents using sparse features
##########################################################

This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.

The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.

https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py

Usage: classification_of_text_documents.py [options]

Options:
  -h, --help            show this help message and exit
  --report              Print a detailed classification report.
  --chi2_select=SELECT_CHI2
                        Select some number of features using a chi-squared
                        test
  --confusion_matrix    Print the confusion matrix.
  --top10               Print ten most discriminative terms per class for
                        every classifier.
  --all_categories      Whether to use all categories or not.
  --use_hashing         Use a hashing vectorizer.
  --n_features=N_FEATURES
                        n_features when using the hashing vectorizer.
  --filtered            Remove newsgroup information that is easily overfit:
                        headers, signatures, and quoting.

Loading 20 newsgroups dataset for categories:
all
data loaded
11314 documents - 22.055MB (training set)
7532 documents - 13.801MB (test set)
20 categories

Extracting features from the training data using a sparse vectorizer
done in 3.366236s at 6.552MB/s
n_samples: 11314, n_features: 129791

Extracting features from the test data using the same vectorizer
done in 1.825788s at 7.559MB/s
n_samples: 7532, n_features: 129791

================================================================================
Logistic Regression
________________________________________________________________________________
Training: 
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
train time: 62.870s
test time:  0.038s
accuracy:   0.845
dimensionality: 129791
density: 1.000000

classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.80      0.74      0.77       319
           comp.graphics       0.71      0.82      0.76       389
 comp.os.ms-windows.misc       0.77      0.79      0.78       394
comp.sys.ibm.pc.hardware       0.72      0.75      0.74       392
   comp.sys.mac.hardware       0.83      0.85      0.84       385
          comp.windows.x       0.86      0.77      0.81       395
            misc.forsale       0.82      0.88      0.85       390
               rec.autos       0.92      0.91      0.91       396
         rec.motorcycles       0.97      0.96      0.97       398
      rec.sport.baseball       0.93      0.94      0.94       397
        rec.sport.hockey       0.95      0.97      0.96       399
               sci.crypt       0.96      0.92      0.94       396
         sci.electronics       0.77      0.77      0.77       393
                 sci.med       0.88      0.87      0.88       396
               sci.space       0.89      0.93      0.91       394
  soc.religion.christian       0.81      0.93      0.87       398
      talk.politics.guns       0.74      0.91      0.82       364
   talk.politics.mideast       0.97      0.88      0.92       376
      talk.politics.misc       0.84      0.60      0.70       310
      talk.religion.misc       0.81      0.53      0.64       251

                accuracy                           0.85      7532
               macro avg       0.85      0.84      0.84      7532
            weighted avg       0.85      0.85      0.84      7532

confusion matrix:
[[236   2   0   1   1   0   1   0   1   2   0   1   3  10   4  36   0   5
    0  16]
 [  3 319  10   7   9  19   3   1   0   1   0   2   8   0   4   1   1   0
    0   1]
 [  0  19 310  31   8   9   1   1   1   3   1   1   1   1   3   0   0   0
    1   3]
 [  0  13  22 294  17   3  12   3   0   0   1   0  24   0   3   0   0   0
    0   0]
 [  0   3   7  20 326   1  13   0   1   3   0   0   9   0   2   0   0   0
    0   0]
 [  1  39  35   5   2 305   3   0   0   0   0   0   0   1   3   0   1   0
    0   0]
 [  0   4   2  14   9   0 344   6   0   0   2   0   7   2   0   0   0   0
    0   0]
 [  0   1   1   2   0   2  10 360   3   1   0   0  12   1   1   0   0   0
    2   0]
 [  0   0   0   1   0   0   5   7 383   1   0   0   1   0   0   0   0   0
    0   0]
 [  0   0   0   0   1   0   4   2   1 375  13   0   1   0   0   0   0   0
    0   0]
 [  0   1   0   0   4   0   1   0   0   5 386   0   0   0   1   1   0   0
    0   0]
 [  1   7   3   0   2   2   2   2   0   1   0 363   7   2   1   0   1   0
    2   0]
 [  0  13   9  27   9   2   8   5   2   2   0   4 304   3   5   0   0   0
    0   0]
 [  2   9   1   2   2   3   7   2   1   1   3   0  10 344   0   4   1   1
    3   0]
 [  0  11   0   0   1   1   2   0   0   0   0   0   1   8 367   1   0   0
    2   0]
 [  4   1   2   0   0   1   0   0   0   1   0   0   4   2   3 372   0   0
    1   7]
 [  0   1   0   2   1   0   3   3   1   2   0   5   1   4   2   0 330   1
    6   2]
 [ 12   2   0   0   0   8   0   1   0   3   1   0   0   2   1   2   1 332
   11   0]
 [  1   2   0   0   3   0   1   0   1   1   0   2   0   5   8   1  96   2
  185   2]
 [ 36   3   1   1   0   0   1   0   0   2   0   0   0   4   5  44  12   2
    8 132]]

================================================================================
Decision Tree Classifier
________________________________________________________________________________
Training: 
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=None, splitter='best')
train time: 20.914s
test time:  0.017s
accuracy:   0.585
classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.52      0.54      0.53       319
           comp.graphics       0.46      0.52      0.49       389
 comp.os.ms-windows.misc       0.49      0.54      0.51       394
comp.sys.ibm.pc.hardware       0.39      0.46      0.42       392
   comp.sys.mac.hardware       0.54      0.58      0.56       385
          comp.windows.x       0.58      0.52      0.55       395
            misc.forsale       0.61      0.73      0.67       390
               rec.autos       0.62      0.62      0.62       396
         rec.motorcycles       0.79      0.76      0.78       398
      rec.sport.baseball       0.61      0.57      0.59       397
        rec.sport.hockey       0.76      0.73      0.75       399
               sci.crypt       0.78      0.73      0.75       396
         sci.electronics       0.38      0.37      0.37       393
                 sci.med       0.50      0.47      0.48       396
               sci.space       0.73      0.65      0.69       394
  soc.religion.christian       0.74      0.76      0.75       398
      talk.politics.guns       0.56      0.66      0.61       364
   talk.politics.mideast       0.86      0.60      0.71       376
      talk.politics.misc       0.43      0.40      0.42       310
      talk.religion.misc       0.38      0.37      0.38       251

                accuracy                           0.59      7532
               macro avg       0.59      0.58      0.58      7532
            weighted avg       0.59      0.59      0.59      7532

confusion matrix:
[[171   4   5   3   2   3   6   2   4   3   4   3   6  11   9  20   6   9
   10  38]
 [  4 204  28  18  23  27  18   3   2   5   0   7  10  13   8   5   6   2
    3   3]
 [  6  31 211  47  19  23   9   8   1   1   2   7  12   7   3   3   0   1
    0   3]
 [  4  25  40 180  39  12  17   5   1   1   2   5  38  12   5   4   0   0
    1   1]
 [  0  12  11  45 222   5  18   6   2   9   0   1  28   7   5   4   4   0
    3   3]
 [  2  48  50  18  14 204   2   9   6   2   1   6  11   5   2   4   1   3
    6   1]
 [  0   9   8  16  11   1 285  10   5   4   3   2  17   5   7   2   3   0
    1   1]
 [  2   8   7  14   6   1  22 247  23   6   1   1  27  13   6   0   7   0
    2   3]
 [  4   3   2   6   5   2  15  11 304   5   0   2   6  15   2   3   7   0
    5   1]
 [  8   6   6   9   9   7  10  10   4 227  54   4   6  10   6   3   5   0
    6   7]
 [  3   2   1   5  14   1   3   0   1  52 293   2   4   2   2   1   5   0
    4   4]
 [  4  10   5   7  10   4   2   5   5   1   1 288  11   5   4   1  13   2
   17   1]
 [  1  26  14  42  18  23  19  28   9  10   6  11 144  17   6   2   3   0
    9   5]
 [ 13  20   9  15   3  19  13  13   5  15   5   5  27 185   5   5   8   4
   11  16]
 [ 10  11   8  11   7   5  11   4   4   9   3   2  13  13 258   3   4   1
   13   4]
 [ 18   8   6   2   0   4   4   2   0   4   2   0   6  10   3 301   1   4
    6  17]
 [ 12   5   5   3   6   0   3  13   5   5   0   9   2   6   7   2 239   8
   25   9]
 [ 19   3   5   6   0   3   3   2   0  10   2   3   6   7   6   7  13 226
   32  23]
 [  8   2   1   7   2   2   2  15   2   1   5   7   7  19   5   2  84   0
  125  14]
 [ 41   2   6   5   3   3   3   5   3   5   0   2   1   7   5  35  16   4
   11  94]]

================================================================================
Linear SVC (penalty = l2)
________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=None, tol=0.001,
          verbose=0)
train time: 8.782s
test time:  0.052s
accuracy:   0.861
dimensionality: 129791
density: 1.000000

classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.83      0.79      0.81       319
           comp.graphics       0.76      0.82      0.79       389
 comp.os.ms-windows.misc       0.80      0.77      0.79       394
comp.sys.ibm.pc.hardware       0.73      0.78      0.75       392
   comp.sys.mac.hardware       0.83      0.86      0.85       385
          comp.windows.x       0.88      0.78      0.83       395
            misc.forsale       0.84      0.91      0.87       390
               rec.autos       0.93      0.91      0.92       396
         rec.motorcycles       0.96      0.96      0.96       398
      rec.sport.baseball       0.92      0.95      0.94       397
        rec.sport.hockey       0.95      0.98      0.97       399
               sci.crypt       0.94      0.95      0.95       396
         sci.electronics       0.83      0.77      0.80       393
                 sci.med       0.92      0.88      0.90       396
               sci.space       0.90      0.94      0.92       394
  soc.religion.christian       0.86      0.94      0.90       398
      talk.politics.guns       0.74      0.92      0.82       364
   talk.politics.mideast       0.98      0.91      0.94       376
      talk.politics.misc       0.85      0.60      0.70       310
      talk.religion.misc       0.76      0.63      0.69       251

                accuracy                           0.86      7532
               macro avg       0.86      0.85      0.85      7532
            weighted avg       0.86      0.86      0.86      7532

confusion matrix:
[[251   1   0   2   0   0   1   0   2   0   0   1   1   7   8  21   0   1
    0  23]
 [  1 318   8   9   7  18   4   1   1   3   1   4   7   0   4   1   0   0
    0   2]
 [  0  19 304  33   9  11   1   0   1   3   1   1   2   2   4   0   0   0
    0   3]
 [  0  12  21 306  19   1  12   2   0   1   0   0  17   0   0   0   0   0
    0   1]
 [  0   5   4  15 333   1  11   1   0   2   0   0   9   1   0   0   2   0
    1   0]
 [  1  37  30   3   3 310   3   0   0   0   1   0   1   1   4   0   1   0
    0   0]
 [  0   1   1   6   8   0 356   6   2   1   0   1   6   1   0   0   0   0
    1   0]
 [  0   1   0   5   1   1  10 359   6   2   0   0   7   1   0   0   1   0
    2   0]
 [  0   0   0   1   0   0   4   6 383   1   0   0   0   0   1   0   1   0
    0   1]
 [  0   0   1   0   0   0   4   2   1 377  12   0   0   0   0   0   0   0
    0   0]
 [  0   0   0   0   1   1   0   0   0   5 391   0   0   0   0   1   0   0
    0   0]
 [  0   2   1   0   4   1   3   2   0   2   0 378   1   0   0   0   1   1
    0   0]
 [  0   6   5  33  11   0   3   4   3   1   0  10 304   6   3   2   0   0
    1   1]
 [  2   5   0   3   2   2   5   2   1   4   2   0   8 349   1   2   2   2
    3   1]
 [  1   6   0   0   1   3   2   0   0   0   1   1   3   4 369   0   1   0
    2   0]
 [  3   0   2   1   0   0   0   0   0   1   0   0   2   1   3 375   0   0
    0  10]
 [  0   2   1   1   1   0   2   1   0   1   0   3   0   1   1   0 336   1
   10   3]
 [  8   1   1   0   0   2   0   1   0   3   1   0   0   1   1   6   2 343
    6   0]
 [  3   1   0   0   2   0   2   0   0   0   0   4   0   4   6   1  94   2
  186   5]
 [ 31   2   1   1   0   0   1   1   0   1   0   0   0   2   3  29  13   1
    7 158]]

================================================================================
Linear SVC (penalty = l1)
________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.001,
          verbose=0)
train time: 6.597s
test time:  0.037s
accuracy:   0.827
dimensionality: 129791
density: 0.002415

classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.78      0.74      0.76       319
           comp.graphics       0.72      0.78      0.75       389
 comp.os.ms-windows.misc       0.75      0.77      0.76       394
comp.sys.ibm.pc.hardware       0.68      0.77      0.72       392
   comp.sys.mac.hardware       0.79      0.84      0.82       385
          comp.windows.x       0.89      0.75      0.81       395
            misc.forsale       0.83      0.87      0.85       390
               rec.autos       0.87      0.87      0.87       396
         rec.motorcycles       0.91      0.94      0.93       398
      rec.sport.baseball       0.91      0.92      0.92       397
        rec.sport.hockey       0.95      0.96      0.95       399
               sci.crypt       0.91      0.92      0.91       396
         sci.electronics       0.75      0.69      0.72       393
                 sci.med       0.89      0.83      0.86       396
               sci.space       0.86      0.92      0.89       394
  soc.religion.christian       0.87      0.92      0.89       398
      talk.politics.guns       0.72      0.90      0.80       364
   talk.politics.mideast       0.96      0.84      0.90       376
      talk.politics.misc       0.77      0.59      0.67       310
      talk.religion.misc       0.68      0.60      0.63       251

                accuracy                           0.83      7532
               macro avg       0.82      0.82      0.82      7532
            weighted avg       0.83      0.83      0.83      7532

confusion matrix:
[[236   1   0   1   1   0   2   0   2   2   1   1   1   8  11  17   0   2
    1  32]
 [  3 302  13  10   9  13   4   1   3   3   1   6   9   1   7   1   1   0
    2   0]
 [  0  20 302  28  11  10   2   1   4   0   2   2   5   1   2   2   1   0
    0   1]
 [  0  11  29 300  17   2  12   2   0   0   0   3  15   0   0   0   0   0
    1   0]
 [  0   5   3  30 323   0   8   0   1   0   0   0   9   2   0   0   3   0
    0   1]
 [  0  40  36   4   5 295   2   2   0   0   1   0   3   0   5   1   0   0
    1   0]
 [  0   1   1  10   9   0 338   6   4   1   1   1   9   2   3   2   1   0
    0   1]
 [  0   4   1   2   3   1   8 345   8   2   0   0  16   1   0   1   2   0
    2   0]
 [  1   1   0   1   2   0   3  10 374   1   0   0   1   2   0   1   0   0
    1   0]
 [  0   2   0   0   0   1   4   3   0 365  11   2   1   3   2   0   1   2
    0   0]
 [  0   0   0   0   4   0   4   0   0   8 382   0   0   0   0   0   0   0
    1   0]
 [  0   4   3   3   1   0   4   4   0   0   0 363   5   0   1   1   3   0
    4   0]
 [  2   5   6  36  16   2   4  13   7   3   3   8 272   5   5   3   0   0
    1   2]
 [  1   9   1   9   2   1   6   1   1   6   0   3   6 329   4   3   1   1
    7   5]
 [  1   8   0   1   2   2   2   1   2   2   0   2   3   4 361   1   1   0
    1   0]
 [  7   0   1   0   0   0   1   1   0   0   0   0   2   1   3 366   0   2
    2  12]
 [  0   2   1   1   0   0   2   4   0   0   0   3   0   3   1   0 326   3
   11   7]
 [ 11   1   0   1   1   3   0   2   3   4   1   1   3   0   6   3   3 317
   11   5]
 [  2   2   0   1   1   0   1   0   1   2   0   3   1   4   7   1  93   3
  182   6]
 [ 38   0   4   0   0   0   1   1   0   1   0   0   3   3   3  20  18   1
    8 150]]

================================================================================
Ada Boost Classifier
________________________________________________________________________________
Training: 
AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=None)
train time: 14.517s
test time:  0.638s
accuracy:   0.508
classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.70      0.29      0.41       319
           comp.graphics       0.58      0.30      0.39       389
 comp.os.ms-windows.misc       0.63      0.52      0.57       394
comp.sys.ibm.pc.hardware       0.63      0.15      0.24       392
   comp.sys.mac.hardware       0.62      0.54      0.57       385
          comp.windows.x       0.75      0.47      0.58       395
            misc.forsale       0.84      0.66      0.74       390
               rec.autos       0.79      0.54      0.64       396
         rec.motorcycles       0.86      0.76      0.80       398
      rec.sport.baseball       0.67      0.54      0.60       397
        rec.sport.hockey       0.91      0.63      0.74       399
               sci.crypt       0.83      0.75      0.79       396
         sci.electronics       0.11      0.73      0.20       393
                 sci.med       0.00      0.00      0.00       396
               sci.space       0.76      0.58      0.66       394
  soc.religion.christian       0.60      0.76      0.67       398
      talk.politics.guns       0.60      0.67      0.64       364
   talk.politics.mideast       0.92      0.56      0.70       376
      talk.politics.misc       0.46      0.35      0.40       310
      talk.religion.misc       0.35      0.21      0.26       251

                accuracy                           0.51      7532
               macro avg       0.63      0.50      0.53      7532
            weighted avg       0.64      0.51      0.54      7532

confusion matrix:
[[ 92   0   0   0  15   2   3   0   1   2   1   0  82   0   6  70   5   0
    5  35]
 [  0 115  14   2  11  23   2   1   0   0   0   4 205   1   8   1   1   1
    0   0]
 [  0  23 203   8  28  19   0   6   0   1   0   6  92   0   4   1   0   1
    1   1]
 [  0  20  44  57  10   1   3   1   0   0   0   8 235   0   9   1   2   0
    1   0]
 [  2   7   1   4 207   3   5   0   0   2   2   4 140   0   4   0   0   0
    1   3]
 [  0  11  45   0   6 184   1   3   0   0   0   4 128   2   6   0   0   0
    5   0]
 [  0   4   3   4   6   0 256   6   3   3   0   2  90   0   5   2   2   0
    4   0]
 [  0   0   1   0   0   0   6 215  16   0   0   1 131   0   2   2  13   2
    6   1]
 [  0   0   0   0   1   1   6  13 301   1   0   2  65   0   2   1   4   0
    1   0]
 [  0   5   0   0   1   0   3   2   2 216  22   2 136   0   1   2   1   0
    2   2]
 [  0   1   0   0   6   0   6   0   0  76 250   0  49   0   2   1   2   0
    2   4]
 [  0   0   2   4   2   0   2   0   3   0   0 296  53   0   4   1  11   0
   18   0]
 [  0   8   6   7  11   3   4  17  15   6   0  18 287   0   6   3   1   0
    1   0]
 [  1   1   0   0   3   5   4   2   0   1   0   0 348   0   0  16   2   5
    6   2]
 [  0   1   0   0  16   0   2   2   3   9   0   3 116   0 229   4   1   0
    8   0]
 [  9   1   0   0   0   1   0   0   0   0   0   0  58   0   2 301   0   3
    3  20]
 [  0   0   0   3   0   2   3   3   7   1   0   4  45   0   4   2 245   1
   36   8]
 [ 21   0   0   0   0   0   0   0   0   4   0   0  88   0   4  11   3 211
   17  17]
 [  1   2   1   0   0   1   0   1   1   0   0   1  80   0   0  11  95   4
  108   4]
 [  5   0   0   1  13   0   0   1   0   2   0   0  73   0   4  72  18   2
    8  52]]

================================================================================
Random forest
________________________________________________________________________________
Training: 
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)
train time: 49.846s
test time:  0.834s
accuracy:   0.788
classification report:
                          precision    recall  f1-score   support

             alt.atheism       0.74      0.63      0.68       319
           comp.graphics       0.63      0.73      0.67       389
 comp.os.ms-windows.misc       0.71      0.80      0.76       394
comp.sys.ibm.pc.hardware       0.66      0.70      0.68       392
   comp.sys.mac.hardware       0.78      0.81      0.79       385
          comp.windows.x       0.75      0.72      0.74       395
            misc.forsale       0.77      0.87      0.82       390
               rec.autos       0.86      0.83      0.85       396
         rec.motorcycles       0.91      0.91      0.91       398
      rec.sport.baseball       0.86      0.92      0.89       397
        rec.sport.hockey       0.91      0.95      0.93       399
               sci.crypt       0.90      0.93      0.91       396
         sci.electronics       0.75      0.56      0.64       393
                 sci.med       0.85      0.73      0.79       396
               sci.space       0.84      0.90      0.87       394
  soc.religion.christian       0.71      0.94      0.81       398
      talk.politics.guns       0.67      0.88      0.76       364
   talk.politics.mideast       0.95      0.81      0.88       376
      talk.politics.misc       0.86      0.50      0.63       310
      talk.religion.misc       0.78      0.38      0.51       251

                accuracy                           0.79      7532
               macro avg       0.79      0.78      0.78      7532
            weighted avg       0.79      0.79      0.78      7532

confusion matrix:
[[200   5   0   1   2   1   4   0   0   3   2   1   2  10   2  54   6   6
    1  19]
 [  2 284  19  19  10  29   3   0   2   2   1   1   6   2   7   0   2   0
    0   0]
 [  0  16 316  21  11   9   3   1   0   1   0   3   4   1   5   0   0   1
    0   2]
 [  1  21  31 276  18   6  14   3   0   2   0   2  16   0   2   0   0   0
    0   0]
 [  0   3   7  29 312   7  12   1   1   4   0   0   7   1   1   0   0   0
    0   0]
 [  0  43  42  10   4 285   3   1   0   0   0   1   1   0   5   0   0   0
    0   0]
 [  0   4   4  14  11   1 340   4   2   0   1   1   4   1   3   0   0   0
    0   0]
 [  0   6   4   4   1   1  13 330  16   1   0   0   7   1   3   0   7   1
    1   0]
 [  0   2   0   2   2   1   7  11 362   1   0   1   1   3   0   3   1   0
    1   0]
 [  0   2   0   0   0   3   4   1   1 366  16   2   0   1   1   0   0   0
    0   0]
 [  0   2   0   0   2   0   3   0   0  11 379   0   0   1   1   0   0   0
    0   0]
 [  0   9   1   2   1   2   1   2   1   3   0 368   2   1   2   0   1   0
    0   0]
 [  1  22  16  28  13  15  13  15   7   8   4  15 219   1  10   3   2   0
    0   1]
 [  5  13   2  10   2   6  13   5   3  10   5   1  17 291   3   4   2   1
    3   0]
 [  0  11   0   0   3   2   2   1   1   3   0   1   2   9 354   0   1   1
    3   0]
 [  5   2   0   1   2   3   1   0   0   2   0   0   1   2   2 376   0   0
    1   0]
 [  0   2   0   1   1   1   3   6   0   1   1   5   2   5   4   3 322   1
    5   1]
 [ 25   1   0   0   1   7   1   1   0   3   4   4   1   3   2   7   7 305
    3   1]
 [  1   1   0   1   3   0   4   1   0   1   2   2   0   8   9   4 111   3
  156   3]
 [ 30   5   1   0   1   1   0   2   1   3   2   1   0   3   5  75  17   1
    8  95]]

================================================================================

Process finished with exit code 0
